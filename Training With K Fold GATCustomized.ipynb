{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import networkx as nx\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import Data_Prep.Graph_Data as gd\n",
    "from Data_Prep.Graph_Data import Molecule_data\n",
    "from math import sqrt\n",
    "# from torch_geometric.nn import GATv2Conv\n",
    "from models.GAT import GAT\n",
    "# from optuna_v1.attenFP_v1 import AttentionConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFoldsData():\n",
    "    iy = 0\n",
    "    folds = 10\n",
    "    for fold in tqdm(range(folds)):\n",
    "        df_train = pd.read_csv('New_fold/fold_'+str(iy)+'_'+'x_train.csv')\n",
    "        df_test  = pd.read_csv('New_fold/fold_'+str(iy)+'_'+'x_test.csv')\n",
    "        smiles = df_train['SMILES']\n",
    "#         codIds = df_train['CODID']\n",
    "        band_gap = df_train['logS']\n",
    "        band_gap = band_gap.to_numpy()\n",
    "\n",
    "        smiles_test = df_test['SMILES']\n",
    "#         codIds_test = df_test['CODID']\n",
    "        band_gap_test = df_test['logS']\n",
    "        band_gap_test = band_gap_test.to_numpy()\n",
    "\n",
    "\n",
    "        smile_graph = {}\n",
    "        band_gap_arr = []\n",
    "        smiles_array = []\n",
    "\n",
    "        for i,smile in enumerate(smiles):\n",
    "            g = gd.smile_to_graph(smile)\n",
    "            if g != None:\n",
    "                smile_graph[smile] = g\n",
    "                band_gap_arr.append(band_gap[i])\n",
    "                smiles_array.append(smile)\n",
    "\n",
    "        smile_graph_test = {}\n",
    "        band_gap_arr_test = []\n",
    "        smiles_array_test = []\n",
    "\n",
    "        for i,smile in enumerate(smiles_test):\n",
    "            g = gd.smile_to_graph(smile)\n",
    "            if g != None:\n",
    "                smile_graph_test[smile] = g\n",
    "                band_gap_arr_test.append(band_gap_test[i])\n",
    "                smiles_array_test.append(smile)\n",
    "\n",
    "        train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(iy),y=band_gap_arr,\n",
    "                                   smile_graph=smile_graph,smiles=smiles_array)\n",
    "\n",
    "        test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(iy),y=band_gap_arr_test,\n",
    "                                   smile_graph=smile_graph_test,smiles=smiles_array_test)\n",
    "\n",
    "        iy+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Prep/solubility_1.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = df['SMILES']\n",
    "# codIds = df['CODID']\n",
    "band_gap = df['logS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_gap = band_gap.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFoldsCsv():\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=10,shuffle = True, random_state = 2) #, random_state = 2\n",
    "    ix = 0\n",
    "    train1 = df\n",
    "    for train_index, test_index in (kf.split(train1)):\n",
    "        print (\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train,X_test=train1.iloc[train_index], train1.iloc[test_index]\n",
    "        X_train.to_csv('New_fold/fold_'+str(ix)+'_'+'x_train.csv',index=False)\n",
    "        X_test.to_csv('New_fold/fold_'+str(ix)+'_'+'x_test.csv',index=False)\n",
    "        ix+=1\n",
    "    createFoldsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_file_train = 'data/processed/' + 'train_data_set_fold_'+str(0)+'.pt'\n",
    "processed_data_file_test = 'data/processed/'  + 'test_data_set_fold_'+str(0)+'.pt'\n",
    "if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n",
    "        print('please run create_data.py to prepare data in pytorch format!')\n",
    "        createFoldsCsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:7\"\n",
    "    print(\"cuda:7\")\n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "    print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer,train_loader):\n",
    "    train_labels = 0\n",
    "    train_predictions = 0\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # out = model(data)\n",
    "        y = data.y.view([-1])\n",
    "        out1 = out.view([-1])\n",
    "        # print(\"train : \", y.shape)\n",
    "        loss = F.mse_loss(out1, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         train_labels += train_labels + y\n",
    "#         train_predictions += train_predictions + out1\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return total_loss,sqrt(total_loss / total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader, model):\n",
    "    # mse = []\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # out = model(data)\n",
    "        # mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "        # return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "        y = data.y.view([-1])\n",
    "        out1 = out.view([-1])\n",
    "        # print(\"test : \", y.shape)\n",
    "        test_loss = F.mse_loss(out1, y)\n",
    "        # print(\"no of graphs: \", data.num_graphs)\n",
    "        total_loss += float(test_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "#         total_preds = torch.cat((total_preds, out1.cpu()), 0)\n",
    "#         total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "        # mse.append(test_loss).cpu()\n",
    "    # return test_loss,float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    return total_loss,sqrt(total_loss / total_examples) #,total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model.\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0021931604377098835,\n",
    "#                                weight_decay=1.2733069489371785e-05)\n",
    "the_last_loss = 100\n",
    "patience = 30\n",
    "trigger_times = 0\n",
    "count_loss_difference = 0\n",
    "#LR = 0.005\n",
    "learning_rate = 0.00688267742977242\n",
    "weight_decay=0.000307616688331247\n",
    "#LR = 0.0028894537419258915\n",
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 200\n",
    "results = []\n",
    "TRAIN_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ret = []\n",
    "best_mse = 0.80\n",
    "best_ci = 0\n",
    "best_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c10b5c334541fa9ddf17f98e91f842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 5.8143 Test: 1.3705 \n",
      "Epoch: 001, Loss: 1.3472 Test: 1.2614 \n",
      "Epoch: 002, Loss: 1.2670 Test: 1.1438 \n",
      "Epoch: 003, Loss: 1.2546 Test: 1.3363 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2435 Test: 1.1090 \n",
      "Epoch: 005, Loss: 1.2269 Test: 1.0638 \n",
      "Epoch: 006, Loss: 1.2317 Test: 1.0521 \n",
      "Epoch: 007, Loss: 1.2502 Test: 1.0939 \n",
      "trigger times: 1\n",
      "Epoch: 008, Loss: 1.2059 Test: 1.0512 \n",
      "Epoch: 009, Loss: 1.1986 Test: 1.1042 \n",
      "trigger times: 1\n",
      "Epoch: 010, Loss: 1.2268 Test: 1.0578 \n",
      "trigger times: 2\n",
      "Epoch: 011, Loss: 1.1726 Test: 1.0819 \n",
      "trigger times: 3\n",
      "Epoch: 012, Loss: 1.1697 Test: 1.0792 \n",
      "trigger times: 4\n",
      "Epoch: 013, Loss: 1.1650 Test: 1.0993 \n",
      "trigger times: 5\n",
      "Epoch: 014, Loss: 1.1785 Test: 1.1824 \n",
      "trigger times: 6\n",
      "Epoch: 015, Loss: 1.2084 Test: 1.0249 \n",
      "Epoch: 016, Loss: 1.1658 Test: 1.1235 \n",
      "trigger times: 1\n",
      "Epoch: 017, Loss: 1.1814 Test: 1.2693 \n",
      "trigger times: 2\n",
      "Epoch: 018, Loss: 1.1722 Test: 1.0979 \n",
      "trigger times: 3\n",
      "Epoch: 019, Loss: 1.1545 Test: 1.0331 \n",
      "trigger times: 4\n",
      "Epoch: 020, Loss: 1.1756 Test: 1.0525 \n",
      "trigger times: 5\n",
      "Epoch: 021, Loss: 1.1781 Test: 1.0904 \n",
      "trigger times: 6\n",
      "Epoch: 022, Loss: 1.1727 Test: 1.0318 \n",
      "trigger times: 7\n",
      "Epoch: 023, Loss: 1.1411 Test: 1.0470 \n",
      "trigger times: 8\n",
      "Epoch: 024, Loss: 1.1815 Test: 1.0542 \n",
      "trigger times: 9\n",
      "Epoch: 025, Loss: 1.1842 Test: 1.1378 \n",
      "trigger times: 10\n",
      "Epoch: 026, Loss: 1.1595 Test: 1.0584 \n",
      "trigger times: 11\n",
      "Epoch: 027, Loss: 1.1704 Test: 1.0643 \n",
      "trigger times: 12\n",
      "Epoch: 028, Loss: 1.1444 Test: 1.1845 \n",
      "trigger times: 13\n",
      "Epoch: 029, Loss: 1.1927 Test: 1.3747 \n",
      "trigger times: 14\n",
      "Epoch: 030, Loss: 1.1749 Test: 1.2572 \n",
      "trigger times: 15\n",
      "Epoch: 031, Loss: 1.1591 Test: 1.0586 \n",
      "trigger times: 16\n",
      "Epoch: 032, Loss: 1.1848 Test: 1.0418 \n",
      "trigger times: 17\n",
      "Epoch: 033, Loss: 1.1639 Test: 1.0885 \n",
      "trigger times: 18\n",
      "Epoch: 034, Loss: 1.1859 Test: 1.2544 \n",
      "trigger times: 19\n",
      "Epoch: 035, Loss: 1.1727 Test: 1.0791 \n",
      "trigger times: 20\n",
      "Epoch: 036, Loss: 1.2015 Test: 1.0798 \n",
      "trigger times: 21\n",
      "Epoch: 037, Loss: 1.2373 Test: 1.0815 \n",
      "trigger times: 22\n",
      "Epoch: 038, Loss: 1.1758 Test: 1.4653 \n",
      "trigger times: 23\n",
      "Epoch: 039, Loss: 1.1702 Test: 1.0102 \n",
      "Epoch: 040, Loss: 1.1336 Test: 1.0812 \n",
      "trigger times: 1\n",
      "Epoch: 041, Loss: 1.1598 Test: 1.0345 \n",
      "trigger times: 2\n",
      "Epoch: 042, Loss: 1.2112 Test: 1.0660 \n",
      "trigger times: 3\n",
      "Epoch: 043, Loss: 1.2133 Test: 1.1342 \n",
      "trigger times: 4\n",
      "Epoch: 044, Loss: 1.2050 Test: 1.0297 \n",
      "trigger times: 5\n",
      "Epoch: 045, Loss: 1.1792 Test: 1.0095 \n",
      "Epoch: 046, Loss: 1.1355 Test: 1.0507 \n",
      "trigger times: 1\n",
      "Epoch: 047, Loss: 1.1451 Test: 1.0144 \n",
      "trigger times: 2\n",
      "Epoch: 048, Loss: 1.1445 Test: 1.0807 \n",
      "trigger times: 3\n",
      "Epoch: 049, Loss: 1.1743 Test: 1.0874 \n",
      "trigger times: 4\n",
      "Epoch: 050, Loss: 1.1919 Test: 1.1668 \n",
      "trigger times: 5\n",
      "Epoch: 051, Loss: 1.2001 Test: 1.1136 \n",
      "trigger times: 6\n",
      "Epoch: 052, Loss: 1.1942 Test: 1.0652 \n",
      "trigger times: 7\n",
      "Epoch: 053, Loss: 1.1770 Test: 1.0933 \n",
      "trigger times: 8\n",
      "Epoch: 054, Loss: 1.1768 Test: 1.1636 \n",
      "trigger times: 9\n",
      "Epoch: 055, Loss: 1.2554 Test: 1.0547 \n",
      "trigger times: 10\n",
      "Epoch: 056, Loss: 1.2036 Test: 1.0462 \n",
      "trigger times: 11\n",
      "Epoch: 057, Loss: 1.1316 Test: 1.1646 \n",
      "trigger times: 12\n",
      "Epoch: 058, Loss: 1.1949 Test: 0.9835 \n",
      "Epoch: 059, Loss: 1.1253 Test: 0.9888 \n",
      "trigger times: 1\n",
      "Epoch: 060, Loss: 1.2256 Test: 1.5102 \n",
      "trigger times: 2\n",
      "Epoch: 061, Loss: 1.1753 Test: 0.9980 \n",
      "trigger times: 3\n",
      "Epoch: 062, Loss: 1.1640 Test: 1.1841 \n",
      "trigger times: 4\n",
      "Epoch: 063, Loss: 1.1756 Test: 1.0568 \n",
      "trigger times: 5\n",
      "Epoch: 064, Loss: 1.2076 Test: 1.2059 \n",
      "trigger times: 6\n",
      "Epoch: 065, Loss: 1.2018 Test: 1.0927 \n",
      "trigger times: 7\n",
      "Epoch: 066, Loss: 1.1097 Test: 1.0215 \n",
      "trigger times: 8\n",
      "Epoch: 067, Loss: 1.1510 Test: 1.1225 \n",
      "trigger times: 9\n",
      "Epoch: 068, Loss: 1.1216 Test: 1.0046 \n",
      "trigger times: 10\n",
      "Epoch: 069, Loss: 1.1585 Test: 1.1644 \n",
      "trigger times: 11\n",
      "Epoch: 070, Loss: 1.1967 Test: 1.0614 \n",
      "trigger times: 12\n",
      "Epoch: 071, Loss: 1.1187 Test: 1.1239 \n",
      "trigger times: 13\n",
      "Epoch: 072, Loss: 1.1453 Test: 1.0474 \n",
      "trigger times: 14\n",
      "Epoch: 073, Loss: 1.1465 Test: 1.1437 \n",
      "trigger times: 15\n",
      "Epoch: 074, Loss: 1.1827 Test: 1.1629 \n",
      "trigger times: 16\n",
      "Epoch: 075, Loss: 1.1725 Test: 1.0233 \n",
      "trigger times: 17\n",
      "Epoch: 076, Loss: 1.3713 Test: 1.0139 \n",
      "trigger times: 18\n",
      "Epoch: 077, Loss: 1.1355 Test: 1.0294 \n",
      "trigger times: 19\n",
      "Epoch: 078, Loss: 1.1523 Test: 1.4842 \n",
      "trigger times: 20\n",
      "Epoch: 079, Loss: 1.3035 Test: 1.0132 \n",
      "trigger times: 21\n",
      "Epoch: 080, Loss: 1.1596 Test: 1.1495 \n",
      "trigger times: 22\n",
      "Epoch: 081, Loss: 1.1420 Test: 1.0737 \n",
      "trigger times: 23\n",
      "Epoch: 082, Loss: 1.1195 Test: 1.0273 \n",
      "trigger times: 24\n",
      "Epoch: 083, Loss: 1.1660 Test: 1.1482 \n",
      "trigger times: 25\n",
      "Epoch: 084, Loss: 1.1483 Test: 1.0524 \n",
      "trigger times: 26\n",
      "Epoch: 085, Loss: 1.1018 Test: 1.0480 \n",
      "trigger times: 27\n",
      "Epoch: 086, Loss: 1.1307 Test: 0.9791 \n",
      "Epoch: 087, Loss: 1.0945 Test: 0.9824 \n",
      "trigger times: 1\n",
      "Epoch: 088, Loss: 1.1034 Test: 1.4423 \n",
      "trigger times: 2\n",
      "Epoch: 089, Loss: 1.1821 Test: 1.1172 \n",
      "trigger times: 3\n",
      "Epoch: 090, Loss: 1.1515 Test: 1.3378 \n",
      "trigger times: 4\n",
      "Epoch: 091, Loss: 1.2528 Test: 1.1014 \n",
      "trigger times: 5\n",
      "Epoch: 092, Loss: 1.1126 Test: 1.1035 \n",
      "trigger times: 6\n",
      "Epoch: 093, Loss: 1.1444 Test: 1.1573 \n",
      "trigger times: 7\n",
      "Epoch: 094, Loss: 1.1449 Test: 0.9764 \n",
      "Epoch: 095, Loss: 1.2026 Test: 1.1661 \n",
      "trigger times: 1\n",
      "Epoch: 096, Loss: 1.1682 Test: 1.3361 \n",
      "trigger times: 2\n",
      "Epoch: 097, Loss: 1.1486 Test: 1.2594 \n",
      "trigger times: 3\n",
      "Epoch: 098, Loss: 5.9436 Test: 1.6121 \n",
      "trigger times: 4\n",
      "Epoch: 099, Loss: 1.3858 Test: 1.0960 \n",
      "trigger times: 5\n",
      "Epoch: 100, Loss: 1.3471 Test: 1.0216 \n",
      "trigger times: 6\n",
      "Epoch: 101, Loss: 1.1523 Test: 1.3489 \n",
      "trigger times: 7\n",
      "Epoch: 102, Loss: 1.2094 Test: 1.2639 \n",
      "trigger times: 8\n",
      "Epoch: 103, Loss: 1.2110 Test: 1.0194 \n",
      "trigger times: 9\n",
      "Epoch: 104, Loss: 1.1316 Test: 1.0820 \n",
      "trigger times: 10\n",
      "Epoch: 105, Loss: 1.1217 Test: 1.0112 \n",
      "trigger times: 11\n",
      "Epoch: 106, Loss: 1.1643 Test: 1.0368 \n",
      "trigger times: 12\n",
      "Epoch: 107, Loss: 1.1416 Test: 0.9898 \n",
      "trigger times: 13\n",
      "Epoch: 108, Loss: 1.1310 Test: 1.1394 \n",
      "trigger times: 14\n",
      "Epoch: 109, Loss: 1.0973 Test: 1.0506 \n",
      "trigger times: 15\n",
      "Epoch: 110, Loss: 1.1453 Test: 0.9813 \n",
      "trigger times: 16\n",
      "Epoch: 111, Loss: 1.1168 Test: 1.0259 \n",
      "trigger times: 17\n",
      "Epoch: 112, Loss: 1.1346 Test: 1.0274 \n",
      "trigger times: 18\n",
      "Epoch: 113, Loss: 1.1176 Test: 0.9938 \n",
      "trigger times: 19\n",
      "Epoch: 114, Loss: 1.1243 Test: 0.9982 \n",
      "trigger times: 20\n",
      "Epoch: 115, Loss: 1.1444 Test: 1.0010 \n",
      "trigger times: 21\n",
      "Epoch: 116, Loss: 1.0873 Test: 0.9904 \n",
      "trigger times: 22\n",
      "Epoch: 117, Loss: 1.1921 Test: 0.9830 \n",
      "trigger times: 23\n",
      "Epoch: 118, Loss: 1.0802 Test: 1.0496 \n",
      "trigger times: 24\n",
      "Epoch: 119, Loss: 1.0687 Test: 0.9876 \n",
      "trigger times: 25\n",
      "Epoch: 120, Loss: 1.0757 Test: 1.0876 \n",
      "trigger times: 26\n",
      "Epoch: 121, Loss: 1.0769 Test: 1.0482 \n",
      "trigger times: 27\n",
      "Epoch: 122, Loss: 1.0775 Test: 0.9889 \n",
      "trigger times: 28\n",
      "Epoch: 123, Loss: 1.0866 Test: 1.0130 \n",
      "trigger times: 29\n",
      "Epoch: 124, Loss: 1.1124 Test: 1.0317 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 5.6797 Test: 1.4158 \n",
      "Epoch: 001, Loss: 1.3376 Test: 1.3844 \n",
      "Epoch: 002, Loss: 1.2617 Test: 1.2395 \n",
      "Epoch: 003, Loss: 1.2383 Test: 1.2443 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2391 Test: 1.2562 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2184 Test: 1.2138 \n",
      "Epoch: 006, Loss: 1.2658 Test: 1.1713 \n",
      "Epoch: 007, Loss: 1.2060 Test: 1.1439 \n",
      "Epoch: 008, Loss: 1.2244 Test: 1.1433 \n",
      "Epoch: 009, Loss: 1.1873 Test: 1.1352 \n",
      "Epoch: 010, Loss: 1.1725 Test: 1.2189 \n",
      "trigger times: 1\n",
      "Epoch: 011, Loss: 1.1807 Test: 1.1430 \n",
      "trigger times: 2\n",
      "Epoch: 012, Loss: 1.1955 Test: 1.2987 \n",
      "trigger times: 3\n",
      "Epoch: 013, Loss: 1.1654 Test: 1.1249 \n",
      "Epoch: 014, Loss: 1.1472 Test: 1.1056 \n",
      "Epoch: 015, Loss: 1.1875 Test: 1.1321 \n",
      "trigger times: 1\n",
      "Epoch: 016, Loss: 1.1659 Test: 1.3048 \n",
      "trigger times: 2\n",
      "Epoch: 017, Loss: 1.1728 Test: 1.1253 \n",
      "trigger times: 3\n",
      "Epoch: 018, Loss: 1.2081 Test: 1.2007 \n",
      "trigger times: 4\n",
      "Epoch: 019, Loss: 1.1601 Test: 1.2686 \n",
      "trigger times: 5\n",
      "Epoch: 020, Loss: 1.1396 Test: 1.1171 \n",
      "trigger times: 6\n",
      "Epoch: 021, Loss: 1.1210 Test: 1.1747 \n",
      "trigger times: 7\n",
      "Epoch: 022, Loss: 1.1315 Test: 1.1615 \n",
      "trigger times: 8\n",
      "Epoch: 023, Loss: 1.1355 Test: 1.2891 \n",
      "trigger times: 9\n",
      "Epoch: 024, Loss: 1.1620 Test: 1.1053 \n",
      "Epoch: 025, Loss: 1.1349 Test: 1.1739 \n",
      "trigger times: 1\n",
      "Epoch: 026, Loss: 1.1354 Test: 1.0598 \n",
      "Epoch: 027, Loss: 1.1472 Test: 1.2020 \n",
      "trigger times: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Loss: 1.1555 Test: 1.1098 \n",
      "trigger times: 2\n",
      "Epoch: 029, Loss: 1.1163 Test: 1.2039 \n",
      "trigger times: 3\n",
      "Epoch: 030, Loss: 1.1307 Test: 1.1765 \n",
      "trigger times: 4\n",
      "Epoch: 031, Loss: 1.1541 Test: 1.1486 \n",
      "trigger times: 5\n",
      "Epoch: 032, Loss: 1.1305 Test: 1.1646 \n",
      "trigger times: 6\n",
      "Epoch: 033, Loss: 1.1779 Test: 1.1286 \n",
      "trigger times: 7\n",
      "Epoch: 034, Loss: 1.1247 Test: 1.1190 \n",
      "trigger times: 8\n",
      "Epoch: 035, Loss: 1.1090 Test: 1.0880 \n",
      "trigger times: 9\n",
      "Epoch: 036, Loss: 1.1492 Test: 1.2071 \n",
      "trigger times: 10\n",
      "Epoch: 037, Loss: 1.1693 Test: 1.1414 \n",
      "trigger times: 11\n",
      "Epoch: 038, Loss: 1.1731 Test: 1.2568 \n",
      "trigger times: 12\n",
      "Epoch: 039, Loss: 1.1211 Test: 1.1195 \n",
      "trigger times: 13\n",
      "Epoch: 040, Loss: 1.1119 Test: 1.1124 \n",
      "trigger times: 14\n",
      "Epoch: 041, Loss: 1.1205 Test: 1.0797 \n",
      "trigger times: 15\n",
      "Epoch: 042, Loss: 1.1340 Test: 1.2920 \n",
      "trigger times: 16\n",
      "Epoch: 043, Loss: 1.1795 Test: 1.0624 \n",
      "trigger times: 17\n",
      "Epoch: 044, Loss: 1.1090 Test: 1.1861 \n",
      "trigger times: 18\n",
      "Epoch: 045, Loss: 1.1343 Test: 1.1801 \n",
      "trigger times: 19\n",
      "Epoch: 046, Loss: 1.1582 Test: 1.2183 \n",
      "trigger times: 20\n",
      "Epoch: 047, Loss: 1.1367 Test: 1.2145 \n",
      "trigger times: 21\n",
      "Epoch: 048, Loss: 1.1644 Test: 1.1782 \n",
      "trigger times: 22\n",
      "Epoch: 049, Loss: 1.1405 Test: 1.0789 \n",
      "trigger times: 23\n",
      "Epoch: 050, Loss: 1.1297 Test: 1.0835 \n",
      "trigger times: 24\n",
      "Epoch: 051, Loss: 1.1437 Test: 1.3632 \n",
      "trigger times: 25\n",
      "Epoch: 052, Loss: 1.1590 Test: 1.2266 \n",
      "trigger times: 26\n",
      "Epoch: 053, Loss: 1.2173 Test: 1.5239 \n",
      "trigger times: 27\n",
      "Epoch: 054, Loss: 1.1373 Test: 1.0575 \n",
      "Epoch: 055, Loss: 1.0985 Test: 1.2414 \n",
      "trigger times: 1\n",
      "Epoch: 056, Loss: 3.8673 Test: 1.4914 \n",
      "trigger times: 2\n",
      "Epoch: 057, Loss: 1.8136 Test: 1.0932 \n",
      "trigger times: 3\n",
      "Epoch: 058, Loss: 1.0940 Test: 1.1349 \n",
      "trigger times: 4\n",
      "Epoch: 059, Loss: 1.0797 Test: 1.0843 \n",
      "trigger times: 5\n",
      "Epoch: 060, Loss: 1.0758 Test: 1.0577 \n",
      "trigger times: 6\n",
      "Epoch: 061, Loss: 1.0612 Test: 1.1063 \n",
      "trigger times: 7\n",
      "Epoch: 062, Loss: 1.0773 Test: 1.1270 \n",
      "trigger times: 8\n",
      "Epoch: 063, Loss: 1.0593 Test: 1.0463 \n",
      "Epoch: 064, Loss: 1.0614 Test: 1.0443 \n",
      "Epoch: 065, Loss: 1.0412 Test: 1.0351 \n",
      "Epoch: 066, Loss: 1.0651 Test: 1.1069 \n",
      "trigger times: 1\n",
      "Epoch: 067, Loss: 1.0505 Test: 1.0581 \n",
      "trigger times: 2\n",
      "Epoch: 068, Loss: 1.0464 Test: 1.0739 \n",
      "trigger times: 3\n",
      "Epoch: 069, Loss: 1.0732 Test: 1.0890 \n",
      "trigger times: 4\n",
      "Epoch: 070, Loss: 1.0433 Test: 1.2345 \n",
      "trigger times: 5\n",
      "Epoch: 071, Loss: 1.0504 Test: 1.0642 \n",
      "trigger times: 6\n",
      "Epoch: 072, Loss: 1.0433 Test: 1.3326 \n",
      "trigger times: 7\n",
      "Epoch: 073, Loss: 1.0669 Test: 1.0639 \n",
      "trigger times: 8\n",
      "Epoch: 074, Loss: 1.0468 Test: 1.1350 \n",
      "trigger times: 9\n",
      "Epoch: 075, Loss: 1.0456 Test: 1.1208 \n",
      "trigger times: 10\n",
      "Epoch: 076, Loss: 1.0469 Test: 1.0523 \n",
      "trigger times: 11\n",
      "Epoch: 077, Loss: 1.0531 Test: 1.1110 \n",
      "trigger times: 12\n",
      "Epoch: 078, Loss: 1.0478 Test: 1.0813 \n",
      "trigger times: 13\n",
      "Epoch: 079, Loss: 1.0596 Test: 1.0752 \n",
      "trigger times: 14\n",
      "Epoch: 080, Loss: 1.0695 Test: 1.1393 \n",
      "trigger times: 15\n",
      "Epoch: 081, Loss: 1.0547 Test: 1.0889 \n",
      "trigger times: 16\n",
      "Epoch: 082, Loss: 1.0456 Test: 1.1333 \n",
      "trigger times: 17\n",
      "Epoch: 083, Loss: 1.0635 Test: 1.0695 \n",
      "trigger times: 18\n",
      "Epoch: 084, Loss: 1.0593 Test: 1.0931 \n",
      "trigger times: 19\n",
      "Epoch: 085, Loss: 1.0628 Test: 1.1075 \n",
      "trigger times: 20\n",
      "Epoch: 086, Loss: 1.0722 Test: 1.0968 \n",
      "trigger times: 21\n",
      "Epoch: 087, Loss: 1.0742 Test: 1.0538 \n",
      "trigger times: 22\n",
      "Epoch: 088, Loss: 1.1106 Test: 1.5098 \n",
      "trigger times: 23\n",
      "Epoch: 089, Loss: 1.0898 Test: 1.0868 \n",
      "trigger times: 24\n",
      "Epoch: 090, Loss: 1.0867 Test: 1.1132 \n",
      "trigger times: 25\n",
      "Epoch: 091, Loss: 1.1277 Test: 1.3651 \n",
      "trigger times: 26\n",
      "Epoch: 092, Loss: 1.1477 Test: 1.7527 \n",
      "trigger times: 27\n",
      "Epoch: 093, Loss: 1.1207 Test: 1.0736 \n",
      "trigger times: 28\n",
      "Epoch: 094, Loss: 1.0841 Test: 1.0981 \n",
      "trigger times: 29\n",
      "Epoch: 095, Loss: 1.0643 Test: 1.1848 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 4.7703 Test: 1.3465 \n",
      "Epoch: 001, Loss: 1.3127 Test: 1.3522 \n",
      "trigger times: 1\n",
      "Epoch: 002, Loss: 1.2602 Test: 1.2790 \n",
      "Epoch: 003, Loss: 1.2761 Test: 1.2670 \n",
      "Epoch: 004, Loss: 1.2783 Test: 1.2395 \n",
      "Epoch: 005, Loss: 1.2115 Test: 1.3017 \n",
      "trigger times: 1\n",
      "Epoch: 006, Loss: 1.2133 Test: 1.1543 \n",
      "Epoch: 007, Loss: 1.1824 Test: 1.1796 \n",
      "trigger times: 1\n",
      "Epoch: 008, Loss: 1.1800 Test: 1.2102 \n",
      "trigger times: 2\n",
      "Epoch: 009, Loss: 1.2099 Test: 1.2106 \n",
      "trigger times: 3\n",
      "Epoch: 010, Loss: 1.2396 Test: 1.1751 \n",
      "trigger times: 4\n",
      "Epoch: 011, Loss: 1.1561 Test: 1.1794 \n",
      "trigger times: 5\n",
      "Epoch: 012, Loss: 1.1726 Test: 1.1954 \n",
      "trigger times: 6\n",
      "Epoch: 013, Loss: 1.1660 Test: 1.2483 \n",
      "trigger times: 7\n",
      "Epoch: 014, Loss: 1.1541 Test: 1.2771 \n",
      "trigger times: 8\n",
      "Epoch: 015, Loss: 1.1804 Test: 1.1670 \n",
      "trigger times: 9\n",
      "Epoch: 016, Loss: 1.1588 Test: 1.3542 \n",
      "trigger times: 10\n",
      "Epoch: 017, Loss: 1.1761 Test: 1.1768 \n",
      "trigger times: 11\n",
      "Epoch: 018, Loss: 1.1501 Test: 1.2166 \n",
      "trigger times: 12\n",
      "Epoch: 019, Loss: 1.1485 Test: 1.1568 \n",
      "trigger times: 13\n",
      "Epoch: 020, Loss: 1.1363 Test: 1.1570 \n",
      "trigger times: 14\n",
      "Epoch: 021, Loss: 1.1384 Test: 1.1795 \n",
      "trigger times: 15\n",
      "Epoch: 022, Loss: 1.1673 Test: 1.2297 \n",
      "trigger times: 16\n",
      "Epoch: 023, Loss: 1.1363 Test: 1.1606 \n",
      "trigger times: 17\n",
      "Epoch: 024, Loss: 1.1576 Test: 1.2181 \n",
      "trigger times: 18\n",
      "Epoch: 025, Loss: 1.1566 Test: 1.3343 \n",
      "trigger times: 19\n",
      "Epoch: 026, Loss: 1.1888 Test: 1.1638 \n",
      "trigger times: 20\n",
      "Epoch: 027, Loss: 1.1281 Test: 1.3775 \n",
      "trigger times: 21\n",
      "Epoch: 028, Loss: 1.1487 Test: 1.1863 \n",
      "trigger times: 22\n",
      "Epoch: 029, Loss: 1.1339 Test: 1.1787 \n",
      "trigger times: 23\n",
      "Epoch: 030, Loss: 1.1697 Test: 1.2016 \n",
      "trigger times: 24\n",
      "Epoch: 031, Loss: 1.1433 Test: 1.2042 \n",
      "trigger times: 25\n",
      "Epoch: 032, Loss: 1.2102 Test: 1.1968 \n",
      "trigger times: 26\n",
      "Epoch: 033, Loss: 1.1634 Test: 1.3339 \n",
      "trigger times: 27\n",
      "Epoch: 034, Loss: 1.1379 Test: 1.1635 \n",
      "trigger times: 28\n",
      "Epoch: 035, Loss: 1.1445 Test: 1.2206 \n",
      "trigger times: 29\n",
      "Epoch: 036, Loss: 1.1463 Test: 1.4659 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 4.4254 Test: 1.4395 \n",
      "Epoch: 001, Loss: 1.3185 Test: 1.2574 \n",
      "Epoch: 002, Loss: 1.3266 Test: 1.2675 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2556 Test: 1.2055 \n",
      "Epoch: 004, Loss: 1.2513 Test: 1.2650 \n",
      "trigger times: 1\n",
      "Epoch: 005, Loss: 1.2415 Test: 1.1740 \n",
      "Epoch: 006, Loss: 1.2661 Test: 1.1995 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.2253 Test: 1.1265 \n",
      "Epoch: 008, Loss: 1.1826 Test: 1.3191 \n",
      "trigger times: 1\n",
      "Epoch: 009, Loss: 1.1970 Test: 1.0889 \n",
      "Epoch: 010, Loss: 1.1803 Test: 1.1760 \n",
      "trigger times: 1\n",
      "Epoch: 011, Loss: 1.1905 Test: 1.1546 \n",
      "trigger times: 2\n",
      "Epoch: 012, Loss: 1.1870 Test: 1.1206 \n",
      "trigger times: 3\n",
      "Epoch: 013, Loss: 1.1521 Test: 1.1571 \n",
      "trigger times: 4\n",
      "Epoch: 014, Loss: 1.1564 Test: 1.1266 \n",
      "trigger times: 5\n",
      "Epoch: 015, Loss: 1.1436 Test: 1.1189 \n",
      "trigger times: 6\n",
      "Epoch: 016, Loss: 1.1468 Test: 1.2155 \n",
      "trigger times: 7\n",
      "Epoch: 017, Loss: 1.1771 Test: 1.0852 \n",
      "Epoch: 018, Loss: 1.1949 Test: 1.0841 \n",
      "Epoch: 019, Loss: 1.1695 Test: 1.1166 \n",
      "trigger times: 1\n",
      "Epoch: 020, Loss: 1.1699 Test: 1.1301 \n",
      "trigger times: 2\n",
      "Epoch: 021, Loss: 1.1737 Test: 1.0902 \n",
      "trigger times: 3\n",
      "Epoch: 022, Loss: 1.1596 Test: 1.1112 \n",
      "trigger times: 4\n",
      "Epoch: 023, Loss: 1.1641 Test: 1.1189 \n",
      "trigger times: 5\n",
      "Epoch: 024, Loss: 1.1813 Test: 1.1429 \n",
      "trigger times: 6\n",
      "Epoch: 025, Loss: 1.1646 Test: 1.0828 \n",
      "Epoch: 026, Loss: 1.1534 Test: 1.1093 \n",
      "trigger times: 1\n",
      "Epoch: 027, Loss: 1.2154 Test: 1.2213 \n",
      "trigger times: 2\n",
      "Epoch: 028, Loss: 1.1730 Test: 1.3049 \n",
      "trigger times: 3\n",
      "Epoch: 029, Loss: 1.1738 Test: 1.2852 \n",
      "trigger times: 4\n",
      "Epoch: 030, Loss: 1.1472 Test: 1.2766 \n",
      "trigger times: 5\n",
      "Epoch: 031, Loss: 1.1843 Test: 1.1145 \n",
      "trigger times: 6\n",
      "Epoch: 032, Loss: 1.1753 Test: 1.1987 \n",
      "trigger times: 7\n",
      "Epoch: 033, Loss: 1.1851 Test: 1.2348 \n",
      "trigger times: 8\n",
      "Epoch: 034, Loss: 1.1678 Test: 1.2263 \n",
      "trigger times: 9\n",
      "Epoch: 035, Loss: 1.1912 Test: 1.1280 \n",
      "trigger times: 10\n",
      "Epoch: 036, Loss: 1.2096 Test: 1.3603 \n",
      "trigger times: 11\n",
      "Epoch: 037, Loss: 1.2407 Test: 1.1470 \n",
      "trigger times: 12\n",
      "Epoch: 038, Loss: 1.1639 Test: 1.0873 \n",
      "trigger times: 13\n",
      "Epoch: 039, Loss: 1.1237 Test: 1.1249 \n",
      "trigger times: 14\n",
      "Epoch: 040, Loss: 1.1383 Test: 1.2405 \n",
      "trigger times: 15\n",
      "Epoch: 041, Loss: 1.1337 Test: 1.1041 \n",
      "trigger times: 16\n",
      "Epoch: 042, Loss: 1.1508 Test: 1.1051 \n",
      "trigger times: 17\n",
      "Epoch: 043, Loss: 1.1882 Test: 1.1279 \n",
      "trigger times: 18\n",
      "Epoch: 044, Loss: 1.1599 Test: 1.0887 \n",
      "trigger times: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045, Loss: 1.2447 Test: 1.0811 \n",
      "Epoch: 046, Loss: 1.1351 Test: 1.1429 \n",
      "trigger times: 1\n",
      "Epoch: 047, Loss: 1.1758 Test: 1.1194 \n",
      "trigger times: 2\n",
      "Epoch: 048, Loss: 1.1357 Test: 1.3296 \n",
      "trigger times: 3\n",
      "Epoch: 049, Loss: 1.1784 Test: 1.3332 \n",
      "trigger times: 4\n",
      "Epoch: 050, Loss: 1.1761 Test: 1.0617 \n",
      "Epoch: 051, Loss: 1.1611 Test: 1.1681 \n",
      "trigger times: 1\n",
      "Epoch: 052, Loss: 1.3058 Test: 1.3725 \n",
      "trigger times: 2\n",
      "Epoch: 053, Loss: 1.2176 Test: 1.0858 \n",
      "trigger times: 3\n",
      "Epoch: 054, Loss: 1.1189 Test: 1.0848 \n",
      "trigger times: 4\n",
      "Epoch: 055, Loss: 1.1552 Test: 1.1647 \n",
      "trigger times: 5\n",
      "Epoch: 056, Loss: 1.1299 Test: 1.0517 \n",
      "Epoch: 057, Loss: 1.1141 Test: 1.0553 \n",
      "trigger times: 1\n",
      "Epoch: 058, Loss: 1.0971 Test: 1.0883 \n",
      "trigger times: 2\n",
      "Epoch: 059, Loss: 1.2250 Test: 2.2361 \n",
      "trigger times: 3\n",
      "Epoch: 060, Loss: 3.7980 Test: 1.1643 \n",
      "trigger times: 4\n",
      "Epoch: 061, Loss: 1.1765 Test: 1.0922 \n",
      "trigger times: 5\n",
      "Epoch: 062, Loss: 1.1083 Test: 1.0760 \n",
      "trigger times: 6\n",
      "Epoch: 063, Loss: 1.1159 Test: 1.1022 \n",
      "trigger times: 7\n",
      "Epoch: 064, Loss: 1.1352 Test: 1.0987 \n",
      "trigger times: 8\n",
      "Epoch: 065, Loss: 1.1210 Test: 1.1016 \n",
      "trigger times: 9\n",
      "Epoch: 066, Loss: 1.1180 Test: 1.0680 \n",
      "trigger times: 10\n",
      "Epoch: 067, Loss: 1.1119 Test: 1.0440 \n",
      "Epoch: 068, Loss: 1.0717 Test: 1.0805 \n",
      "trigger times: 1\n",
      "Epoch: 069, Loss: 1.0898 Test: 1.0631 \n",
      "trigger times: 2\n",
      "Epoch: 070, Loss: 1.0989 Test: 1.0585 \n",
      "trigger times: 3\n",
      "Epoch: 071, Loss: 1.0777 Test: 1.0820 \n",
      "trigger times: 4\n",
      "Epoch: 072, Loss: 1.0637 Test: 1.1070 \n",
      "trigger times: 5\n",
      "Epoch: 073, Loss: 1.1030 Test: 1.1107 \n",
      "trigger times: 6\n",
      "Epoch: 074, Loss: 1.0820 Test: 1.0549 \n",
      "trigger times: 7\n",
      "Epoch: 075, Loss: 1.0723 Test: 1.0542 \n",
      "trigger times: 8\n",
      "Epoch: 076, Loss: 1.0723 Test: 1.0588 \n",
      "trigger times: 9\n",
      "Epoch: 077, Loss: 1.0890 Test: 1.0689 \n",
      "trigger times: 10\n",
      "Epoch: 078, Loss: 1.0735 Test: 1.0505 \n",
      "trigger times: 11\n",
      "Epoch: 079, Loss: 1.0581 Test: 1.0342 \n",
      "Epoch: 080, Loss: 1.0807 Test: 1.0482 \n",
      "trigger times: 1\n",
      "Epoch: 081, Loss: 1.0972 Test: 1.1718 \n",
      "trigger times: 2\n",
      "Epoch: 082, Loss: 1.0976 Test: 1.0479 \n",
      "trigger times: 3\n",
      "Epoch: 083, Loss: 1.0889 Test: 1.0752 \n",
      "trigger times: 4\n",
      "Epoch: 084, Loss: 1.0928 Test: 1.0669 \n",
      "trigger times: 5\n",
      "Epoch: 085, Loss: 1.0703 Test: 1.0286 \n",
      "Epoch: 086, Loss: 1.0730 Test: 1.0910 \n",
      "trigger times: 1\n",
      "Epoch: 087, Loss: 1.0876 Test: 1.0934 \n",
      "trigger times: 2\n",
      "Epoch: 088, Loss: 1.0743 Test: 1.0572 \n",
      "trigger times: 3\n",
      "Epoch: 089, Loss: 1.1094 Test: 1.1201 \n",
      "trigger times: 4\n",
      "Epoch: 090, Loss: 1.1030 Test: 1.0644 \n",
      "trigger times: 5\n",
      "Epoch: 091, Loss: 1.0734 Test: 1.2375 \n",
      "trigger times: 6\n",
      "Epoch: 092, Loss: 1.1613 Test: 1.0698 \n",
      "trigger times: 7\n",
      "Epoch: 093, Loss: 1.0998 Test: 1.0930 \n",
      "trigger times: 8\n",
      "Epoch: 094, Loss: 1.0955 Test: 1.2652 \n",
      "trigger times: 9\n",
      "Epoch: 095, Loss: 1.1189 Test: 1.0658 \n",
      "trigger times: 10\n",
      "Epoch: 096, Loss: 1.1552 Test: 1.1086 \n",
      "trigger times: 11\n",
      "Epoch: 097, Loss: 1.0843 Test: 1.1046 \n",
      "trigger times: 12\n",
      "Epoch: 098, Loss: 1.1066 Test: 1.0852 \n",
      "trigger times: 13\n",
      "Epoch: 099, Loss: 1.1649 Test: 1.0592 \n",
      "trigger times: 14\n",
      "Epoch: 100, Loss: 1.0842 Test: 1.0398 \n",
      "trigger times: 15\n",
      "Epoch: 101, Loss: 1.1546 Test: 1.1939 \n",
      "trigger times: 16\n",
      "Epoch: 102, Loss: 1.1117 Test: 1.0675 \n",
      "trigger times: 17\n",
      "Epoch: 103, Loss: 1.0887 Test: 1.3293 \n",
      "trigger times: 18\n",
      "Epoch: 104, Loss: 1.1405 Test: 1.0572 \n",
      "trigger times: 19\n",
      "Epoch: 105, Loss: 1.1680 Test: 1.0746 \n",
      "trigger times: 20\n",
      "Epoch: 106, Loss: 1.1069 Test: 1.0750 \n",
      "trigger times: 21\n",
      "Epoch: 107, Loss: 1.1195 Test: 1.1287 \n",
      "trigger times: 22\n",
      "Epoch: 108, Loss: 1.1436 Test: 1.0947 \n",
      "trigger times: 23\n",
      "Epoch: 109, Loss: 1.0973 Test: 1.1077 \n",
      "trigger times: 24\n",
      "Epoch: 110, Loss: 1.1921 Test: 1.1830 \n",
      "trigger times: 25\n",
      "Epoch: 111, Loss: 4.6030 Test: 2.6267 \n",
      "trigger times: 26\n",
      "Epoch: 112, Loss: 1.9706 Test: 1.2195 \n",
      "trigger times: 27\n",
      "Epoch: 113, Loss: 1.1861 Test: 1.2468 \n",
      "trigger times: 28\n",
      "Epoch: 114, Loss: 1.2287 Test: 1.1807 \n",
      "trigger times: 29\n",
      "Epoch: 115, Loss: 1.1512 Test: 1.1230 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 4.9639 Test: 1.3605 \n",
      "Epoch: 001, Loss: 1.3517 Test: 1.3901 \n",
      "trigger times: 1\n",
      "Epoch: 002, Loss: 1.2616 Test: 1.1584 \n",
      "Epoch: 003, Loss: 1.2371 Test: 1.2085 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2320 Test: 1.3417 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2367 Test: 1.2547 \n",
      "trigger times: 3\n",
      "Epoch: 006, Loss: 1.1969 Test: 1.2546 \n",
      "trigger times: 4\n",
      "Epoch: 007, Loss: 1.2122 Test: 1.1278 \n",
      "Epoch: 008, Loss: 1.1916 Test: 1.2360 \n",
      "trigger times: 1\n",
      "Epoch: 009, Loss: 1.2100 Test: 1.1070 \n",
      "Epoch: 010, Loss: 1.1670 Test: 1.1308 \n",
      "trigger times: 1\n",
      "Epoch: 011, Loss: 1.1826 Test: 1.1666 \n",
      "trigger times: 2\n",
      "Epoch: 012, Loss: 1.1958 Test: 1.2305 \n",
      "trigger times: 3\n",
      "Epoch: 013, Loss: 1.1791 Test: 1.1684 \n",
      "trigger times: 4\n",
      "Epoch: 014, Loss: 1.1697 Test: 1.2049 \n",
      "trigger times: 5\n",
      "Epoch: 015, Loss: 1.1705 Test: 1.0951 \n",
      "Epoch: 016, Loss: 1.1796 Test: 1.1566 \n",
      "trigger times: 1\n",
      "Epoch: 017, Loss: 1.1643 Test: 1.2262 \n",
      "trigger times: 2\n",
      "Epoch: 018, Loss: 1.1602 Test: 1.1291 \n",
      "trigger times: 3\n",
      "Epoch: 019, Loss: 1.1456 Test: 1.1570 \n",
      "trigger times: 4\n",
      "Epoch: 020, Loss: 1.1645 Test: 1.1117 \n",
      "trigger times: 5\n",
      "Epoch: 021, Loss: 1.1548 Test: 1.1329 \n",
      "trigger times: 6\n",
      "Epoch: 022, Loss: 1.1569 Test: 1.1146 \n",
      "trigger times: 7\n",
      "Epoch: 023, Loss: 1.1548 Test: 1.1192 \n",
      "trigger times: 8\n",
      "Epoch: 024, Loss: 1.1702 Test: 1.0752 \n",
      "Epoch: 025, Loss: 1.1540 Test: 1.1367 \n",
      "trigger times: 1\n",
      "Epoch: 026, Loss: 1.1553 Test: 1.0938 \n",
      "trigger times: 2\n",
      "Epoch: 027, Loss: 1.1675 Test: 1.2138 \n",
      "trigger times: 3\n",
      "Epoch: 028, Loss: 1.1428 Test: 1.1508 \n",
      "trigger times: 4\n",
      "Epoch: 029, Loss: 1.1778 Test: 1.3134 \n",
      "trigger times: 5\n",
      "Epoch: 030, Loss: 1.1520 Test: 1.0931 \n",
      "trigger times: 6\n",
      "Epoch: 031, Loss: 1.2056 Test: 1.1314 \n",
      "trigger times: 7\n",
      "Epoch: 032, Loss: 1.1673 Test: 1.1166 \n",
      "trigger times: 8\n",
      "Epoch: 033, Loss: 1.1900 Test: 1.1236 \n",
      "trigger times: 9\n",
      "Epoch: 034, Loss: 1.1689 Test: 1.1013 \n",
      "trigger times: 10\n",
      "Epoch: 035, Loss: 1.1380 Test: 1.1276 \n",
      "trigger times: 11\n",
      "Epoch: 036, Loss: 1.1785 Test: 1.3025 \n",
      "trigger times: 12\n",
      "Epoch: 037, Loss: 1.1781 Test: 1.3289 \n",
      "trigger times: 13\n",
      "Epoch: 038, Loss: 1.1880 Test: 1.0975 \n",
      "trigger times: 14\n",
      "Epoch: 039, Loss: 1.1519 Test: 1.1925 \n",
      "trigger times: 15\n",
      "Epoch: 040, Loss: 1.1419 Test: 1.1498 \n",
      "trigger times: 16\n",
      "Epoch: 041, Loss: 1.1703 Test: 1.5233 \n",
      "trigger times: 17\n",
      "Epoch: 042, Loss: 1.2374 Test: 1.1150 \n",
      "trigger times: 18\n",
      "Epoch: 043, Loss: 1.1808 Test: 1.0830 \n",
      "trigger times: 19\n",
      "Epoch: 044, Loss: 1.1351 Test: 1.2905 \n",
      "trigger times: 20\n",
      "Epoch: 045, Loss: 1.1989 Test: 1.1313 \n",
      "trigger times: 21\n",
      "Epoch: 046, Loss: 1.1566 Test: 1.1746 \n",
      "trigger times: 22\n",
      "Epoch: 047, Loss: 1.3032 Test: 1.7069 \n",
      "trigger times: 23\n",
      "Epoch: 048, Loss: 1.4674 Test: 1.0945 \n",
      "trigger times: 24\n",
      "Epoch: 049, Loss: 1.1179 Test: 1.0723 \n",
      "Epoch: 050, Loss: 1.0936 Test: 1.1182 \n",
      "trigger times: 1\n",
      "Epoch: 051, Loss: 1.1395 Test: 1.2543 \n",
      "trigger times: 2\n",
      "Epoch: 052, Loss: 1.1479 Test: 1.2733 \n",
      "trigger times: 3\n",
      "Epoch: 053, Loss: 1.1356 Test: 1.1018 \n",
      "trigger times: 4\n",
      "Epoch: 054, Loss: 1.1013 Test: 1.1490 \n",
      "trigger times: 5\n",
      "Epoch: 055, Loss: 1.1220 Test: 1.1875 \n",
      "trigger times: 6\n",
      "Epoch: 056, Loss: 1.0978 Test: 1.0800 \n",
      "trigger times: 7\n",
      "Epoch: 057, Loss: 1.1448 Test: 1.0854 \n",
      "trigger times: 8\n",
      "Epoch: 058, Loss: 1.1425 Test: 1.1448 \n",
      "trigger times: 9\n",
      "Epoch: 059, Loss: 1.0898 Test: 1.0875 \n",
      "trigger times: 10\n",
      "Epoch: 060, Loss: 1.1566 Test: 1.6627 \n",
      "trigger times: 11\n",
      "Epoch: 061, Loss: 1.2039 Test: 1.1640 \n",
      "trigger times: 12\n",
      "Epoch: 062, Loss: 1.1728 Test: 1.1227 \n",
      "trigger times: 13\n",
      "Epoch: 063, Loss: 1.1979 Test: 1.2891 \n",
      "trigger times: 14\n",
      "Epoch: 064, Loss: 1.1794 Test: 1.2275 \n",
      "trigger times: 15\n",
      "Epoch: 065, Loss: 1.1732 Test: 1.4710 \n",
      "trigger times: 16\n",
      "Epoch: 066, Loss: 1.1522 Test: 1.0948 \n",
      "trigger times: 17\n",
      "Epoch: 067, Loss: 1.1206 Test: 1.4284 \n",
      "trigger times: 18\n",
      "Epoch: 068, Loss: 1.1502 Test: 1.0655 \n",
      "Epoch: 069, Loss: 1.1163 Test: 1.0831 \n",
      "trigger times: 1\n",
      "Epoch: 070, Loss: 1.1601 Test: 1.0526 \n",
      "Epoch: 071, Loss: 2.8967 Test: 1.6567 \n",
      "trigger times: 1\n",
      "Epoch: 072, Loss: 1.2111 Test: 1.0977 \n",
      "trigger times: 2\n",
      "Epoch: 073, Loss: 1.1081 Test: 1.0656 \n",
      "trigger times: 3\n",
      "Epoch: 074, Loss: 1.0773 Test: 1.1062 \n",
      "trigger times: 4\n",
      "Epoch: 075, Loss: 1.0711 Test: 1.0663 \n",
      "trigger times: 5\n",
      "Epoch: 076, Loss: 1.0667 Test: 1.0604 \n",
      "trigger times: 6\n",
      "Epoch: 077, Loss: 1.0707 Test: 1.0864 \n",
      "trigger times: 7\n",
      "Epoch: 078, Loss: 1.0616 Test: 1.0567 \n",
      "trigger times: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 079, Loss: 1.0717 Test: 1.0825 \n",
      "trigger times: 9\n",
      "Epoch: 080, Loss: 1.0750 Test: 1.2554 \n",
      "trigger times: 10\n",
      "Epoch: 081, Loss: 1.0779 Test: 1.0683 \n",
      "trigger times: 11\n",
      "Epoch: 082, Loss: 1.0569 Test: 1.0526 \n",
      "Epoch: 083, Loss: 1.0766 Test: 1.0612 \n",
      "trigger times: 1\n",
      "Epoch: 084, Loss: 1.0667 Test: 1.1590 \n",
      "trigger times: 2\n",
      "Epoch: 085, Loss: 1.0725 Test: 1.1424 \n",
      "trigger times: 3\n",
      "Epoch: 086, Loss: 1.0764 Test: 1.0681 \n",
      "trigger times: 4\n",
      "Epoch: 087, Loss: 1.0701 Test: 1.1097 \n",
      "trigger times: 5\n",
      "Epoch: 088, Loss: 1.0930 Test: 1.1321 \n",
      "trigger times: 6\n",
      "Epoch: 089, Loss: 1.1009 Test: 1.0560 \n",
      "trigger times: 7\n",
      "Epoch: 090, Loss: 1.0753 Test: 1.0772 \n",
      "trigger times: 8\n",
      "Epoch: 091, Loss: 1.0807 Test: 1.0662 \n",
      "trigger times: 9\n",
      "Epoch: 092, Loss: 1.0674 Test: 1.0667 \n",
      "trigger times: 10\n",
      "Epoch: 093, Loss: 1.0891 Test: 1.0496 \n",
      "Epoch: 094, Loss: 1.0948 Test: 1.0765 \n",
      "trigger times: 1\n",
      "Epoch: 095, Loss: 1.1760 Test: 1.4299 \n",
      "trigger times: 2\n",
      "Epoch: 096, Loss: 1.1200 Test: 1.0390 \n",
      "Epoch: 097, Loss: 1.1232 Test: 1.0629 \n",
      "trigger times: 1\n",
      "Epoch: 098, Loss: 1.1305 Test: 1.1065 \n",
      "trigger times: 2\n",
      "Epoch: 099, Loss: 1.1109 Test: 1.0699 \n",
      "trigger times: 3\n",
      "Epoch: 100, Loss: 1.1316 Test: 1.0435 \n",
      "trigger times: 4\n",
      "Epoch: 101, Loss: 1.0977 Test: 1.0534 \n",
      "trigger times: 5\n",
      "Epoch: 102, Loss: 1.1422 Test: 1.1239 \n",
      "trigger times: 6\n",
      "Epoch: 103, Loss: 1.1536 Test: 1.1108 \n",
      "trigger times: 7\n",
      "Epoch: 104, Loss: 1.0832 Test: 1.1279 \n",
      "trigger times: 8\n",
      "Epoch: 105, Loss: 1.1376 Test: 1.1602 \n",
      "trigger times: 9\n",
      "Epoch: 106, Loss: 1.1264 Test: 1.0736 \n",
      "trigger times: 10\n",
      "Epoch: 107, Loss: 1.1256 Test: 1.0411 \n",
      "trigger times: 11\n",
      "Epoch: 108, Loss: 1.1026 Test: 1.0704 \n",
      "trigger times: 12\n",
      "Epoch: 109, Loss: 1.2044 Test: 1.1163 \n",
      "trigger times: 13\n",
      "Epoch: 110, Loss: 1.0893 Test: 1.0557 \n",
      "trigger times: 14\n",
      "Epoch: 111, Loss: 1.1080 Test: 1.0669 \n",
      "trigger times: 15\n",
      "Epoch: 112, Loss: 1.1393 Test: 1.0881 \n",
      "trigger times: 16\n",
      "Epoch: 113, Loss: 1.1213 Test: 1.1756 \n",
      "trigger times: 17\n",
      "Epoch: 114, Loss: 1.1430 Test: 1.1095 \n",
      "trigger times: 18\n",
      "Epoch: 115, Loss: 1.2085 Test: 1.5326 \n",
      "trigger times: 19\n",
      "Epoch: 116, Loss: 1.3166 Test: 1.2320 \n",
      "trigger times: 20\n",
      "Epoch: 117, Loss: 1.0856 Test: 1.0512 \n",
      "trigger times: 21\n",
      "Epoch: 118, Loss: 1.0854 Test: 1.0628 \n",
      "trigger times: 22\n",
      "Epoch: 119, Loss: 1.3252 Test: 3.7355 \n",
      "trigger times: 23\n",
      "Epoch: 120, Loss: 6.1069 Test: 1.1844 \n",
      "trigger times: 24\n",
      "Epoch: 121, Loss: 1.2179 Test: 1.0996 \n",
      "trigger times: 25\n",
      "Epoch: 122, Loss: 1.2172 Test: 1.1639 \n",
      "trigger times: 26\n",
      "Epoch: 123, Loss: 1.1918 Test: 1.1281 \n",
      "trigger times: 27\n",
      "Epoch: 124, Loss: 1.2810 Test: 1.3411 \n",
      "trigger times: 28\n",
      "Epoch: 125, Loss: 1.2164 Test: 1.6684 \n",
      "trigger times: 29\n",
      "Epoch: 126, Loss: 1.2119 Test: 1.2452 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 4.7908 Test: 1.5298 \n",
      "Epoch: 001, Loss: 1.2701 Test: 1.4629 \n",
      "Epoch: 002, Loss: 1.2471 Test: 1.4608 \n",
      "Epoch: 003, Loss: 1.2438 Test: 1.5572 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2208 Test: 1.4664 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.1632 Test: 1.4431 \n",
      "Epoch: 006, Loss: 1.2306 Test: 1.5441 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.1839 Test: 1.7315 \n",
      "trigger times: 2\n",
      "Epoch: 008, Loss: 1.1447 Test: 1.4445 \n",
      "trigger times: 3\n",
      "Epoch: 009, Loss: 1.1565 Test: 1.4817 \n",
      "trigger times: 4\n",
      "Epoch: 010, Loss: 1.1751 Test: 1.4839 \n",
      "trigger times: 5\n",
      "Epoch: 011, Loss: 1.1380 Test: 1.4898 \n",
      "trigger times: 6\n",
      "Epoch: 012, Loss: 1.1267 Test: 1.4546 \n",
      "trigger times: 7\n",
      "Epoch: 013, Loss: 1.1623 Test: 1.3974 \n",
      "Epoch: 014, Loss: 1.1385 Test: 1.4510 \n",
      "trigger times: 1\n",
      "Epoch: 015, Loss: 1.1552 Test: 1.5131 \n",
      "trigger times: 2\n",
      "Epoch: 016, Loss: 1.1418 Test: 1.3310 \n",
      "Epoch: 017, Loss: 1.1074 Test: 1.4775 \n",
      "trigger times: 1\n",
      "Epoch: 018, Loss: 1.1096 Test: 1.3387 \n",
      "trigger times: 2\n",
      "Epoch: 019, Loss: 1.1254 Test: 1.3382 \n",
      "trigger times: 3\n",
      "Epoch: 020, Loss: 1.1386 Test: 1.4367 \n",
      "trigger times: 4\n",
      "Epoch: 021, Loss: 1.1101 Test: 1.4904 \n",
      "trigger times: 5\n",
      "Epoch: 022, Loss: 1.1112 Test: 1.3636 \n",
      "trigger times: 6\n",
      "Epoch: 023, Loss: 1.1019 Test: 1.5055 \n",
      "trigger times: 7\n",
      "Epoch: 024, Loss: 1.1423 Test: 1.4555 \n",
      "trigger times: 8\n",
      "Epoch: 025, Loss: 1.1127 Test: 1.3570 \n",
      "trigger times: 9\n",
      "Epoch: 026, Loss: 1.1176 Test: 1.3765 \n",
      "trigger times: 10\n",
      "Epoch: 027, Loss: 1.1186 Test: 1.4154 \n",
      "trigger times: 11\n",
      "Epoch: 028, Loss: 1.0846 Test: 1.3445 \n",
      "trigger times: 12\n",
      "Epoch: 029, Loss: 1.1131 Test: 1.4243 \n",
      "trigger times: 13\n",
      "Epoch: 030, Loss: 1.1433 Test: 1.6851 \n",
      "trigger times: 14\n",
      "Epoch: 031, Loss: 1.1298 Test: 1.3996 \n",
      "trigger times: 15\n",
      "Epoch: 032, Loss: 1.1137 Test: 1.4548 \n",
      "trigger times: 16\n",
      "Epoch: 033, Loss: 1.1492 Test: 1.3853 \n",
      "trigger times: 17\n",
      "Epoch: 034, Loss: 1.1072 Test: 1.4561 \n",
      "trigger times: 18\n",
      "Epoch: 035, Loss: 1.1294 Test: 1.5590 \n",
      "trigger times: 19\n",
      "Epoch: 036, Loss: 1.0983 Test: 1.4232 \n",
      "trigger times: 20\n",
      "Epoch: 037, Loss: 1.1533 Test: 1.4321 \n",
      "trigger times: 21\n",
      "Epoch: 038, Loss: 1.1451 Test: 1.4254 \n",
      "trigger times: 22\n",
      "Epoch: 039, Loss: 1.1526 Test: 1.4945 \n",
      "trigger times: 23\n",
      "Epoch: 040, Loss: 1.1790 Test: 1.3631 \n",
      "trigger times: 24\n",
      "Epoch: 041, Loss: 1.2432 Test: 1.4938 \n",
      "trigger times: 25\n",
      "Epoch: 042, Loss: 1.1736 Test: 1.4029 \n",
      "trigger times: 26\n",
      "Epoch: 043, Loss: 1.1100 Test: 1.3720 \n",
      "trigger times: 27\n",
      "Epoch: 044, Loss: 1.1536 Test: 1.6116 \n",
      "trigger times: 28\n",
      "Epoch: 045, Loss: 1.0910 Test: 1.5089 \n",
      "trigger times: 29\n",
      "Epoch: 046, Loss: 1.1133 Test: 1.4158 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 5.8273 Test: 1.3233 \n",
      "Epoch: 001, Loss: 1.3290 Test: 1.2425 \n",
      "Epoch: 002, Loss: 1.2636 Test: 1.2109 \n",
      "Epoch: 003, Loss: 1.2453 Test: 1.2206 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2309 Test: 1.3291 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2431 Test: 1.1698 \n",
      "Epoch: 006, Loss: 1.2205 Test: 1.1531 \n",
      "Epoch: 007, Loss: 1.2016 Test: 1.1696 \n",
      "trigger times: 1\n",
      "Epoch: 008, Loss: 1.2262 Test: 1.1121 \n",
      "Epoch: 009, Loss: 1.2394 Test: 1.1486 \n",
      "trigger times: 1\n",
      "Epoch: 010, Loss: 1.1876 Test: 1.2182 \n",
      "trigger times: 2\n",
      "Epoch: 011, Loss: 1.1648 Test: 1.1694 \n",
      "trigger times: 3\n",
      "Epoch: 012, Loss: 1.1561 Test: 1.1369 \n",
      "trigger times: 4\n",
      "Epoch: 013, Loss: 1.1588 Test: 1.1193 \n",
      "trigger times: 5\n",
      "Epoch: 014, Loss: 1.1731 Test: 1.1258 \n",
      "trigger times: 6\n",
      "Epoch: 015, Loss: 1.1514 Test: 1.1562 \n",
      "trigger times: 7\n",
      "Epoch: 016, Loss: 1.1708 Test: 1.1423 \n",
      "trigger times: 8\n",
      "Epoch: 017, Loss: 1.1734 Test: 1.2432 \n",
      "trigger times: 9\n",
      "Epoch: 018, Loss: 1.1592 Test: 1.1197 \n",
      "trigger times: 10\n",
      "Epoch: 019, Loss: 1.1445 Test: 1.1325 \n",
      "trigger times: 11\n",
      "Epoch: 020, Loss: 1.1618 Test: 1.1295 \n",
      "trigger times: 12\n",
      "Epoch: 021, Loss: 1.1814 Test: 1.1573 \n",
      "trigger times: 13\n",
      "Epoch: 022, Loss: 1.1808 Test: 1.0777 \n",
      "Epoch: 023, Loss: 1.1859 Test: 1.1027 \n",
      "trigger times: 1\n",
      "Epoch: 024, Loss: 1.1665 Test: 1.0733 \n",
      "Epoch: 025, Loss: 1.1621 Test: 1.1335 \n",
      "trigger times: 1\n",
      "Epoch: 026, Loss: 1.1664 Test: 1.1404 \n",
      "trigger times: 2\n",
      "Epoch: 027, Loss: 1.1931 Test: 1.1765 \n",
      "trigger times: 3\n",
      "Epoch: 028, Loss: 1.1959 Test: 1.0878 \n",
      "trigger times: 4\n",
      "Epoch: 029, Loss: 1.1964 Test: 1.0855 \n",
      "trigger times: 5\n",
      "Epoch: 030, Loss: 1.2109 Test: 1.2130 \n",
      "trigger times: 6\n",
      "Epoch: 031, Loss: 1.1720 Test: 1.1605 \n",
      "trigger times: 7\n",
      "Epoch: 032, Loss: 1.1392 Test: 1.1712 \n",
      "trigger times: 8\n",
      "Epoch: 033, Loss: 1.2035 Test: 1.1454 \n",
      "trigger times: 9\n",
      "Epoch: 034, Loss: 1.1840 Test: 1.2886 \n",
      "trigger times: 10\n",
      "Epoch: 035, Loss: 1.1929 Test: 1.1571 \n",
      "trigger times: 11\n",
      "Epoch: 036, Loss: 1.1854 Test: 1.0847 \n",
      "trigger times: 12\n",
      "Epoch: 037, Loss: 1.1459 Test: 1.0726 \n",
      "Epoch: 038, Loss: 1.1827 Test: 1.0776 \n",
      "trigger times: 1\n",
      "Epoch: 039, Loss: 1.1651 Test: 1.0666 \n",
      "Epoch: 040, Loss: 1.2414 Test: 1.4385 \n",
      "trigger times: 1\n",
      "Epoch: 041, Loss: 1.1665 Test: 1.1056 \n",
      "trigger times: 2\n",
      "Epoch: 042, Loss: 1.1394 Test: 1.0857 \n",
      "trigger times: 3\n",
      "Epoch: 043, Loss: 1.1673 Test: 1.0855 \n",
      "trigger times: 4\n",
      "Epoch: 044, Loss: 1.1660 Test: 1.1099 \n",
      "trigger times: 5\n",
      "Epoch: 045, Loss: 1.1485 Test: 1.0672 \n",
      "trigger times: 6\n",
      "Epoch: 046, Loss: 1.1271 Test: 1.3503 \n",
      "trigger times: 7\n",
      "Epoch: 047, Loss: 1.2141 Test: 1.3549 \n",
      "trigger times: 8\n",
      "Epoch: 048, Loss: 1.1970 Test: 1.0815 \n",
      "trigger times: 9\n",
      "Epoch: 049, Loss: 1.1555 Test: 1.2194 \n",
      "trigger times: 10\n",
      "Epoch: 050, Loss: 1.1998 Test: 1.0664 \n",
      "Epoch: 051, Loss: 1.2286 Test: 1.3348 \n",
      "trigger times: 1\n",
      "Epoch: 052, Loss: 1.1277 Test: 1.1930 \n",
      "trigger times: 2\n",
      "Epoch: 053, Loss: 1.1350 Test: 1.1004 \n",
      "trigger times: 3\n",
      "Epoch: 054, Loss: 1.1951 Test: 1.0944 \n",
      "trigger times: 4\n",
      "Epoch: 055, Loss: 1.1222 Test: 1.0862 \n",
      "trigger times: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 056, Loss: 1.1410 Test: 1.4216 \n",
      "trigger times: 6\n",
      "Epoch: 057, Loss: 1.1167 Test: 1.2145 \n",
      "trigger times: 7\n",
      "Epoch: 058, Loss: 1.1449 Test: 1.3082 \n",
      "trigger times: 8\n",
      "Epoch: 059, Loss: 1.1668 Test: 1.2384 \n",
      "trigger times: 9\n",
      "Epoch: 060, Loss: 1.1141 Test: 1.0538 \n",
      "Epoch: 061, Loss: 1.0893 Test: 1.0981 \n",
      "trigger times: 1\n",
      "Epoch: 062, Loss: 1.2164 Test: 1.3621 \n",
      "trigger times: 2\n",
      "Epoch: 063, Loss: 1.1619 Test: 1.3981 \n",
      "trigger times: 3\n",
      "Epoch: 064, Loss: 1.1792 Test: 1.0532 \n",
      "Epoch: 065, Loss: 1.0907 Test: 1.2955 \n",
      "trigger times: 1\n",
      "Epoch: 066, Loss: 1.2020 Test: 1.0766 \n",
      "trigger times: 2\n",
      "Epoch: 067, Loss: 1.1145 Test: 1.2679 \n",
      "trigger times: 3\n",
      "Epoch: 068, Loss: 1.1725 Test: 1.0674 \n",
      "trigger times: 4\n",
      "Epoch: 069, Loss: 1.1463 Test: 1.2440 \n",
      "trigger times: 5\n",
      "Epoch: 070, Loss: 1.2008 Test: 1.0943 \n",
      "trigger times: 6\n",
      "Epoch: 071, Loss: 1.1184 Test: 1.1068 \n",
      "trigger times: 7\n",
      "Epoch: 072, Loss: 1.2237 Test: 1.1045 \n",
      "trigger times: 8\n",
      "Epoch: 073, Loss: 1.0964 Test: 1.0273 \n",
      "Epoch: 074, Loss: 1.1078 Test: 1.0330 \n",
      "trigger times: 1\n",
      "Epoch: 075, Loss: 1.1686 Test: 1.1443 \n",
      "trigger times: 2\n",
      "Epoch: 076, Loss: 1.1301 Test: 1.1024 \n",
      "trigger times: 3\n",
      "Epoch: 077, Loss: 1.1282 Test: 1.0820 \n",
      "trigger times: 4\n",
      "Epoch: 078, Loss: 1.1780 Test: 1.0586 \n",
      "trigger times: 5\n",
      "Epoch: 079, Loss: 1.0606 Test: 1.1376 \n",
      "trigger times: 6\n",
      "Epoch: 080, Loss: 1.2073 Test: 1.1961 \n",
      "trigger times: 7\n",
      "Epoch: 081, Loss: 1.1295 Test: 1.0519 \n",
      "trigger times: 8\n",
      "Epoch: 082, Loss: 1.1100 Test: 1.0813 \n",
      "trigger times: 9\n",
      "Epoch: 083, Loss: 1.0965 Test: 1.0516 \n",
      "trigger times: 10\n",
      "Epoch: 084, Loss: 1.1516 Test: 1.0329 \n",
      "trigger times: 11\n",
      "Epoch: 085, Loss: 1.1696 Test: 1.1144 \n",
      "trigger times: 12\n",
      "Epoch: 086, Loss: 1.1461 Test: 1.1441 \n",
      "trigger times: 13\n",
      "Epoch: 087, Loss: 1.1781 Test: 1.0363 \n",
      "trigger times: 14\n",
      "Epoch: 088, Loss: 1.1667 Test: 1.1220 \n",
      "trigger times: 15\n",
      "Epoch: 089, Loss: 1.1652 Test: 1.0549 \n",
      "trigger times: 16\n",
      "Epoch: 090, Loss: 1.1720 Test: 1.1354 \n",
      "trigger times: 17\n",
      "Epoch: 091, Loss: 1.2660 Test: 1.1127 \n",
      "trigger times: 18\n",
      "Epoch: 092, Loss: 1.1020 Test: 1.0614 \n",
      "trigger times: 19\n",
      "Epoch: 093, Loss: 1.1269 Test: 1.0842 \n",
      "trigger times: 20\n",
      "Epoch: 094, Loss: 1.2673 Test: 1.6022 \n",
      "trigger times: 21\n",
      "Epoch: 095, Loss: 1.1401 Test: 1.1932 \n",
      "trigger times: 22\n",
      "Epoch: 096, Loss: 1.1095 Test: 1.0564 \n",
      "trigger times: 23\n",
      "Epoch: 097, Loss: 1.1125 Test: 1.2695 \n",
      "trigger times: 24\n",
      "Epoch: 098, Loss: 1.1715 Test: 1.1711 \n",
      "trigger times: 25\n",
      "Epoch: 099, Loss: 1.1496 Test: 1.0461 \n",
      "trigger times: 26\n",
      "Epoch: 100, Loss: 1.1051 Test: 1.0571 \n",
      "trigger times: 27\n",
      "Epoch: 101, Loss: 1.1215 Test: 1.1012 \n",
      "trigger times: 28\n",
      "Epoch: 102, Loss: 1.1259 Test: 1.0530 \n",
      "trigger times: 29\n",
      "Epoch: 103, Loss: 1.0905 Test: 1.0755 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 5.5191 Test: 1.4171 \n",
      "Epoch: 001, Loss: 1.3051 Test: 1.3487 \n",
      "Epoch: 002, Loss: 1.2321 Test: 1.3360 \n",
      "Epoch: 003, Loss: 1.2393 Test: 1.3957 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2230 Test: 1.2611 \n",
      "Epoch: 005, Loss: 1.1872 Test: 1.2277 \n",
      "Epoch: 006, Loss: 1.1895 Test: 1.2439 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.1711 Test: 1.2743 \n",
      "trigger times: 2\n",
      "Epoch: 008, Loss: 1.1968 Test: 1.3242 \n",
      "trigger times: 3\n",
      "Epoch: 009, Loss: 1.2097 Test: 1.2057 \n",
      "Epoch: 010, Loss: 1.1802 Test: 1.2499 \n",
      "trigger times: 1\n",
      "Epoch: 011, Loss: 1.1746 Test: 1.1937 \n",
      "Epoch: 012, Loss: 1.1634 Test: 1.3366 \n",
      "trigger times: 1\n",
      "Epoch: 013, Loss: 1.1405 Test: 1.2704 \n",
      "trigger times: 2\n",
      "Epoch: 014, Loss: 1.1514 Test: 1.2834 \n",
      "trigger times: 3\n",
      "Epoch: 015, Loss: 1.1789 Test: 1.2868 \n",
      "trigger times: 4\n",
      "Epoch: 016, Loss: 1.1621 Test: 1.2383 \n",
      "trigger times: 5\n",
      "Epoch: 017, Loss: 1.1760 Test: 1.3254 \n",
      "trigger times: 6\n",
      "Epoch: 018, Loss: 1.1630 Test: 1.3405 \n",
      "trigger times: 7\n",
      "Epoch: 019, Loss: 1.1578 Test: 1.2223 \n",
      "trigger times: 8\n",
      "Epoch: 020, Loss: 1.1300 Test: 1.2923 \n",
      "trigger times: 9\n",
      "Epoch: 021, Loss: 1.1262 Test: 1.2258 \n",
      "trigger times: 10\n",
      "Epoch: 022, Loss: 1.1714 Test: 1.2085 \n",
      "trigger times: 11\n",
      "Epoch: 023, Loss: 1.1494 Test: 1.2070 \n",
      "trigger times: 12\n",
      "Epoch: 024, Loss: 1.2173 Test: 1.2435 \n",
      "trigger times: 13\n",
      "Epoch: 025, Loss: 1.1502 Test: 1.2330 \n",
      "trigger times: 14\n",
      "Epoch: 026, Loss: 1.1294 Test: 1.2812 \n",
      "trigger times: 15\n",
      "Epoch: 027, Loss: 1.1495 Test: 1.3218 \n",
      "trigger times: 16\n",
      "Epoch: 028, Loss: 1.1366 Test: 1.2739 \n",
      "trigger times: 17\n",
      "Epoch: 029, Loss: 1.1629 Test: 1.3412 \n",
      "trigger times: 18\n",
      "Epoch: 030, Loss: 1.1268 Test: 1.2222 \n",
      "trigger times: 19\n",
      "Epoch: 031, Loss: 1.1805 Test: 1.2340 \n",
      "trigger times: 20\n",
      "Epoch: 032, Loss: 1.1407 Test: 1.2261 \n",
      "trigger times: 21\n",
      "Epoch: 033, Loss: 1.1452 Test: 1.3022 \n",
      "trigger times: 22\n",
      "Epoch: 034, Loss: 1.1513 Test: 1.3292 \n",
      "trigger times: 23\n",
      "Epoch: 035, Loss: 1.1318 Test: 1.2580 \n",
      "trigger times: 24\n",
      "Epoch: 036, Loss: 1.1533 Test: 1.4328 \n",
      "trigger times: 25\n",
      "Epoch: 037, Loss: 1.1541 Test: 1.2089 \n",
      "trigger times: 26\n",
      "Epoch: 038, Loss: 1.1638 Test: 1.3672 \n",
      "trigger times: 27\n",
      "Epoch: 039, Loss: 1.1849 Test: 1.2221 \n",
      "trigger times: 28\n",
      "Epoch: 040, Loss: 1.1571 Test: 1.4419 \n",
      "trigger times: 29\n",
      "Epoch: 041, Loss: 1.2468 Test: 1.4259 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 5.7606 Test: 1.3637 \n",
      "Epoch: 001, Loss: 1.3278 Test: 1.2449 \n",
      "Epoch: 002, Loss: 1.2512 Test: 1.2178 \n",
      "Epoch: 003, Loss: 1.2337 Test: 1.2222 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2270 Test: 1.2668 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2077 Test: 1.1893 \n",
      "Epoch: 006, Loss: 1.2192 Test: 1.1655 \n",
      "Epoch: 007, Loss: 1.1983 Test: 1.1480 \n",
      "Epoch: 008, Loss: 1.2048 Test: 1.1516 \n",
      "trigger times: 1\n",
      "Epoch: 009, Loss: 1.1644 Test: 1.2421 \n",
      "trigger times: 2\n",
      "Epoch: 010, Loss: 1.1857 Test: 1.1744 \n",
      "trigger times: 3\n",
      "Epoch: 011, Loss: 1.1542 Test: 1.1757 \n",
      "trigger times: 4\n",
      "Epoch: 012, Loss: 1.1951 Test: 1.2651 \n",
      "trigger times: 5\n",
      "Epoch: 013, Loss: 1.1757 Test: 1.1213 \n",
      "Epoch: 014, Loss: 1.1672 Test: 1.1597 \n",
      "trigger times: 1\n",
      "Epoch: 015, Loss: 1.1581 Test: 1.3029 \n",
      "trigger times: 2\n",
      "Epoch: 016, Loss: 1.1653 Test: 1.1532 \n",
      "trigger times: 3\n",
      "Epoch: 017, Loss: 1.1718 Test: 1.3150 \n",
      "trigger times: 4\n",
      "Epoch: 018, Loss: 1.2064 Test: 1.1360 \n",
      "trigger times: 5\n",
      "Epoch: 019, Loss: 1.1239 Test: 1.1142 \n",
      "Epoch: 020, Loss: 1.1975 Test: 1.1256 \n",
      "trigger times: 1\n",
      "Epoch: 021, Loss: 1.1687 Test: 1.2004 \n",
      "trigger times: 2\n",
      "Epoch: 022, Loss: 1.1579 Test: 1.1246 \n",
      "trigger times: 3\n",
      "Epoch: 023, Loss: 1.1719 Test: 1.2145 \n",
      "trigger times: 4\n",
      "Epoch: 024, Loss: 1.1748 Test: 1.1084 \n",
      "Epoch: 025, Loss: 1.1721 Test: 1.1143 \n",
      "trigger times: 1\n",
      "Epoch: 026, Loss: 1.1724 Test: 1.1257 \n",
      "trigger times: 2\n",
      "Epoch: 027, Loss: 1.1697 Test: 1.2856 \n",
      "trigger times: 3\n",
      "Epoch: 028, Loss: 1.1386 Test: 1.0980 \n",
      "Epoch: 029, Loss: 1.1413 Test: 1.1309 \n",
      "trigger times: 1\n",
      "Epoch: 030, Loss: 1.1607 Test: 1.1090 \n",
      "trigger times: 2\n",
      "Epoch: 031, Loss: 1.1285 Test: 1.1008 \n",
      "trigger times: 3\n",
      "Epoch: 032, Loss: 1.1626 Test: 1.1057 \n",
      "trigger times: 4\n",
      "Epoch: 033, Loss: 1.1389 Test: 1.1206 \n",
      "trigger times: 5\n",
      "Epoch: 034, Loss: 1.1614 Test: 1.1215 \n",
      "trigger times: 6\n",
      "Epoch: 035, Loss: 1.1351 Test: 1.0830 \n",
      "Epoch: 036, Loss: 1.1579 Test: 1.1426 \n",
      "trigger times: 1\n",
      "Epoch: 037, Loss: 1.1712 Test: 1.1617 \n",
      "trigger times: 2\n",
      "Epoch: 038, Loss: 1.1686 Test: 1.1361 \n",
      "trigger times: 3\n",
      "Epoch: 039, Loss: 1.1702 Test: 1.1514 \n",
      "trigger times: 4\n",
      "Epoch: 040, Loss: 1.1618 Test: 1.1127 \n",
      "trigger times: 5\n",
      "Epoch: 041, Loss: 1.1544 Test: 1.1852 \n",
      "trigger times: 6\n",
      "Epoch: 042, Loss: 1.1257 Test: 1.1018 \n",
      "trigger times: 7\n",
      "Epoch: 043, Loss: 1.1400 Test: 1.1116 \n",
      "trigger times: 8\n",
      "Epoch: 044, Loss: 1.1681 Test: 1.3504 \n",
      "trigger times: 9\n",
      "Epoch: 045, Loss: 1.2563 Test: 1.0777 \n",
      "Epoch: 046, Loss: 1.1407 Test: 1.1933 \n",
      "trigger times: 1\n",
      "Epoch: 047, Loss: 1.1865 Test: 1.2614 \n",
      "trigger times: 2\n",
      "Epoch: 048, Loss: 1.1434 Test: 1.0793 \n",
      "trigger times: 3\n",
      "Epoch: 049, Loss: 1.2207 Test: 1.1263 \n",
      "trigger times: 4\n",
      "Epoch: 050, Loss: 1.1166 Test: 1.2714 \n",
      "trigger times: 5\n",
      "Epoch: 051, Loss: 1.1447 Test: 1.0960 \n",
      "trigger times: 6\n",
      "Epoch: 052, Loss: 1.1699 Test: 1.0897 \n",
      "trigger times: 7\n",
      "Epoch: 053, Loss: 1.1258 Test: 1.1168 \n",
      "trigger times: 8\n",
      "Epoch: 054, Loss: 1.1252 Test: 1.3059 \n",
      "trigger times: 9\n",
      "Epoch: 055, Loss: 1.1041 Test: 1.0985 \n",
      "trigger times: 10\n",
      "Epoch: 056, Loss: 1.1851 Test: 1.1309 \n",
      "trigger times: 11\n",
      "Epoch: 057, Loss: 1.1930 Test: 1.1456 \n",
      "trigger times: 12\n",
      "Epoch: 058, Loss: 1.1511 Test: 1.1250 \n",
      "trigger times: 13\n",
      "Epoch: 059, Loss: 1.2281 Test: 1.1931 \n",
      "trigger times: 14\n",
      "Epoch: 060, Loss: 1.1559 Test: 1.0798 \n",
      "trigger times: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 061, Loss: 1.1274 Test: 1.2263 \n",
      "trigger times: 16\n",
      "Epoch: 062, Loss: 1.1593 Test: 1.2589 \n",
      "trigger times: 17\n",
      "Epoch: 063, Loss: 1.2331 Test: 2.3718 \n",
      "trigger times: 18\n",
      "Epoch: 064, Loss: 1.1977 Test: 1.0902 \n",
      "trigger times: 19\n",
      "Epoch: 065, Loss: 1.1120 Test: 1.0807 \n",
      "trigger times: 20\n",
      "Epoch: 066, Loss: 1.1281 Test: 1.2409 \n",
      "trigger times: 21\n",
      "Epoch: 067, Loss: 1.1151 Test: 1.1773 \n",
      "trigger times: 22\n",
      "Epoch: 068, Loss: 1.3342 Test: 1.0889 \n",
      "trigger times: 23\n",
      "Epoch: 069, Loss: 1.1260 Test: 1.1455 \n",
      "trigger times: 24\n",
      "Epoch: 070, Loss: 1.1073 Test: 1.0720 \n",
      "Epoch: 071, Loss: 1.0960 Test: 1.1745 \n",
      "trigger times: 1\n",
      "Epoch: 072, Loss: 1.1925 Test: 1.2379 \n",
      "trigger times: 2\n",
      "Epoch: 073, Loss: 1.1215 Test: 1.1772 \n",
      "trigger times: 3\n",
      "Epoch: 074, Loss: 1.1390 Test: 1.1490 \n",
      "trigger times: 4\n",
      "Epoch: 075, Loss: 1.1304 Test: 1.0457 \n",
      "Epoch: 076, Loss: 1.1311 Test: 1.3633 \n",
      "trigger times: 1\n",
      "Epoch: 077, Loss: 3.1306 Test: 1.0798 \n",
      "trigger times: 2\n",
      "Epoch: 078, Loss: 1.1297 Test: 1.1711 \n",
      "trigger times: 3\n",
      "Epoch: 079, Loss: 1.0856 Test: 1.0648 \n",
      "trigger times: 4\n",
      "Epoch: 080, Loss: 1.0626 Test: 1.0702 \n",
      "trigger times: 5\n",
      "Epoch: 081, Loss: 1.0510 Test: 1.1057 \n",
      "trigger times: 6\n",
      "Epoch: 082, Loss: 1.0500 Test: 1.0573 \n",
      "trigger times: 7\n",
      "Epoch: 083, Loss: 1.0800 Test: 1.0603 \n",
      "trigger times: 8\n",
      "Epoch: 084, Loss: 1.0473 Test: 1.1067 \n",
      "trigger times: 9\n",
      "Epoch: 085, Loss: 1.0630 Test: 1.0732 \n",
      "trigger times: 10\n",
      "Epoch: 086, Loss: 1.0548 Test: 1.0532 \n",
      "trigger times: 11\n",
      "Epoch: 087, Loss: 1.0513 Test: 1.0730 \n",
      "trigger times: 12\n",
      "Epoch: 088, Loss: 1.0416 Test: 1.0639 \n",
      "trigger times: 13\n",
      "Epoch: 089, Loss: 1.0681 Test: 1.1045 \n",
      "trigger times: 14\n",
      "Epoch: 090, Loss: 1.0479 Test: 1.1090 \n",
      "trigger times: 15\n",
      "Epoch: 091, Loss: 1.0629 Test: 1.0866 \n",
      "trigger times: 16\n",
      "Epoch: 092, Loss: 1.0595 Test: 1.1204 \n",
      "trigger times: 17\n",
      "Epoch: 093, Loss: 1.0529 Test: 1.0557 \n",
      "trigger times: 18\n",
      "Epoch: 094, Loss: 1.0582 Test: 1.0728 \n",
      "trigger times: 19\n",
      "Epoch: 095, Loss: 1.0672 Test: 1.1151 \n",
      "trigger times: 20\n",
      "Epoch: 096, Loss: 1.0797 Test: 1.0641 \n",
      "trigger times: 21\n",
      "Epoch: 097, Loss: 1.1045 Test: 1.1948 \n",
      "trigger times: 22\n",
      "Epoch: 098, Loss: 1.0957 Test: 1.0800 \n",
      "trigger times: 23\n",
      "Epoch: 099, Loss: 1.0667 Test: 1.0973 \n",
      "trigger times: 24\n",
      "Epoch: 100, Loss: 1.0702 Test: 1.1759 \n",
      "trigger times: 25\n",
      "Epoch: 101, Loss: 1.0705 Test: 1.0520 \n",
      "trigger times: 26\n",
      "Epoch: 102, Loss: 1.0666 Test: 1.0232 \n",
      "Epoch: 103, Loss: 1.0652 Test: 1.0353 \n",
      "trigger times: 1\n",
      "Epoch: 104, Loss: 1.0584 Test: 1.1732 \n",
      "trigger times: 2\n",
      "Epoch: 105, Loss: 1.0941 Test: 1.0839 \n",
      "trigger times: 3\n",
      "Epoch: 106, Loss: 1.1022 Test: 1.0887 \n",
      "trigger times: 4\n",
      "Epoch: 107, Loss: 1.0956 Test: 1.0918 \n",
      "trigger times: 5\n",
      "Epoch: 108, Loss: 1.8673 Test: 1.3052 \n",
      "trigger times: 6\n",
      "Epoch: 109, Loss: 1.0922 Test: 1.1707 \n",
      "trigger times: 7\n",
      "Epoch: 110, Loss: 1.0521 Test: 1.0664 \n",
      "trigger times: 8\n",
      "Epoch: 111, Loss: 1.0643 Test: 1.0525 \n",
      "trigger times: 9\n",
      "Epoch: 112, Loss: 1.0521 Test: 1.0426 \n",
      "trigger times: 10\n",
      "Epoch: 113, Loss: 1.0625 Test: 1.0733 \n",
      "trigger times: 11\n",
      "Epoch: 114, Loss: 1.0663 Test: 1.0555 \n",
      "trigger times: 12\n",
      "Epoch: 115, Loss: 1.0719 Test: 1.0958 \n",
      "trigger times: 13\n",
      "Epoch: 116, Loss: 1.0636 Test: 1.1224 \n",
      "trigger times: 14\n",
      "Epoch: 117, Loss: 1.0692 Test: 1.1249 \n",
      "trigger times: 15\n",
      "Epoch: 118, Loss: 1.0834 Test: 1.1214 \n",
      "trigger times: 16\n",
      "Epoch: 119, Loss: 1.0931 Test: 1.1021 \n",
      "trigger times: 17\n",
      "Epoch: 120, Loss: 1.0858 Test: 1.2052 \n",
      "trigger times: 18\n",
      "Epoch: 121, Loss: 1.2674 Test: 1.0731 \n",
      "trigger times: 19\n",
      "Epoch: 122, Loss: 1.0679 Test: 1.1933 \n",
      "trigger times: 20\n",
      "Epoch: 123, Loss: 1.0618 Test: 1.1095 \n",
      "trigger times: 21\n",
      "Epoch: 124, Loss: 1.0998 Test: 1.1422 \n",
      "trigger times: 22\n",
      "Epoch: 125, Loss: 1.0989 Test: 1.0942 \n",
      "trigger times: 23\n",
      "Epoch: 126, Loss: 1.1325 Test: 1.1095 \n",
      "trigger times: 24\n",
      "Epoch: 127, Loss: 1.1083 Test: 1.1523 \n",
      "trigger times: 25\n",
      "Epoch: 128, Loss: 1.1038 Test: 1.1629 \n",
      "trigger times: 26\n",
      "Epoch: 129, Loss: 1.1216 Test: 1.1929 \n",
      "trigger times: 27\n",
      "Epoch: 130, Loss: 1.1238 Test: 1.1207 \n",
      "trigger times: 28\n",
      "Epoch: 131, Loss: 1.2334 Test: 1.1111 \n",
      "trigger times: 29\n",
      "Epoch: 132, Loss: 1.1020 Test: 1.0708 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Epoch: 000, Loss: 4.6083 Test: 1.3423 \n",
      "Epoch: 001, Loss: 1.3035 Test: 1.1610 \n",
      "Epoch: 002, Loss: 1.2470 Test: 1.1695 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2718 Test: 1.1237 \n",
      "Epoch: 004, Loss: 1.2663 Test: 1.1281 \n",
      "trigger times: 1\n",
      "Epoch: 005, Loss: 1.2537 Test: 1.4209 \n",
      "trigger times: 2\n",
      "Epoch: 006, Loss: 1.2102 Test: 1.0741 \n",
      "Epoch: 007, Loss: 1.2558 Test: 1.0896 \n",
      "trigger times: 1\n",
      "Epoch: 008, Loss: 1.1920 Test: 1.1337 \n",
      "trigger times: 2\n",
      "Epoch: 009, Loss: 1.1956 Test: 1.1170 \n",
      "trigger times: 3\n",
      "Epoch: 010, Loss: 1.2016 Test: 1.1691 \n",
      "trigger times: 4\n",
      "Epoch: 011, Loss: 1.1752 Test: 1.2195 \n",
      "trigger times: 5\n",
      "Epoch: 012, Loss: 1.1786 Test: 1.2307 \n",
      "trigger times: 6\n",
      "Epoch: 013, Loss: 1.1797 Test: 1.0197 \n",
      "Epoch: 014, Loss: 1.1574 Test: 1.0634 \n",
      "trigger times: 1\n",
      "Epoch: 015, Loss: 1.1610 Test: 1.0425 \n",
      "trigger times: 2\n",
      "Epoch: 016, Loss: 1.1694 Test: 1.1954 \n",
      "trigger times: 3\n",
      "Epoch: 017, Loss: 1.1697 Test: 1.1055 \n",
      "trigger times: 4\n",
      "Epoch: 018, Loss: 1.1781 Test: 1.0077 \n",
      "Epoch: 019, Loss: 1.1473 Test: 1.1178 \n",
      "trigger times: 1\n",
      "Epoch: 020, Loss: 1.1628 Test: 1.0551 \n",
      "trigger times: 2\n",
      "Epoch: 021, Loss: 1.1651 Test: 1.1169 \n",
      "trigger times: 3\n",
      "Epoch: 022, Loss: 1.1423 Test: 1.0009 \n",
      "Epoch: 023, Loss: 1.1836 Test: 1.2189 \n",
      "trigger times: 1\n",
      "Epoch: 024, Loss: 1.2262 Test: 1.0130 \n",
      "trigger times: 2\n",
      "Epoch: 025, Loss: 1.1481 Test: 1.1427 \n",
      "trigger times: 3\n",
      "Epoch: 026, Loss: 1.1375 Test: 1.0431 \n",
      "trigger times: 4\n",
      "Epoch: 027, Loss: 1.1721 Test: 1.0580 \n",
      "trigger times: 5\n",
      "Epoch: 028, Loss: 1.1761 Test: 1.0258 \n",
      "trigger times: 6\n",
      "Epoch: 029, Loss: 1.1830 Test: 1.0290 \n",
      "trigger times: 7\n",
      "Epoch: 030, Loss: 1.1599 Test: 1.1394 \n",
      "trigger times: 8\n",
      "Epoch: 031, Loss: 1.2074 Test: 1.0023 \n",
      "trigger times: 9\n",
      "Epoch: 032, Loss: 1.2014 Test: 1.0861 \n",
      "trigger times: 10\n",
      "Epoch: 033, Loss: 1.1613 Test: 1.0929 \n",
      "trigger times: 11\n",
      "Epoch: 034, Loss: 1.1715 Test: 1.0361 \n",
      "trigger times: 12\n",
      "Epoch: 035, Loss: 1.1756 Test: 0.9930 \n",
      "Epoch: 036, Loss: 1.1417 Test: 1.1356 \n",
      "trigger times: 1\n",
      "Epoch: 037, Loss: 1.1627 Test: 1.2097 \n",
      "trigger times: 2\n",
      "Epoch: 038, Loss: 1.2218 Test: 1.0688 \n",
      "trigger times: 3\n",
      "Epoch: 039, Loss: 1.2048 Test: 1.1367 \n",
      "trigger times: 4\n",
      "Epoch: 040, Loss: 1.1547 Test: 1.0267 \n",
      "trigger times: 5\n",
      "Epoch: 041, Loss: 1.1571 Test: 1.1826 \n",
      "trigger times: 6\n",
      "Epoch: 042, Loss: 1.1782 Test: 1.0741 \n",
      "trigger times: 7\n",
      "Epoch: 043, Loss: 1.2125 Test: 0.9984 \n",
      "trigger times: 8\n",
      "Epoch: 044, Loss: 1.2076 Test: 1.0588 \n",
      "trigger times: 9\n",
      "Epoch: 045, Loss: 1.2185 Test: 1.2376 \n",
      "trigger times: 10\n",
      "Epoch: 046, Loss: 1.3105 Test: 1.0199 \n",
      "trigger times: 11\n",
      "Epoch: 047, Loss: 1.1793 Test: 1.0191 \n",
      "trigger times: 12\n",
      "Epoch: 048, Loss: 1.1424 Test: 1.1149 \n",
      "trigger times: 13\n",
      "Epoch: 049, Loss: 1.1601 Test: 1.4211 \n",
      "trigger times: 14\n",
      "Epoch: 050, Loss: 1.2240 Test: 1.1025 \n",
      "trigger times: 15\n",
      "Epoch: 051, Loss: 1.2684 Test: 1.1690 \n",
      "trigger times: 16\n",
      "Epoch: 052, Loss: 1.1403 Test: 1.1389 \n",
      "trigger times: 17\n",
      "Epoch: 053, Loss: 1.1171 Test: 1.1254 \n",
      "trigger times: 18\n",
      "Epoch: 054, Loss: 1.1496 Test: 1.1613 \n",
      "trigger times: 19\n",
      "Epoch: 055, Loss: 1.1747 Test: 1.0387 \n",
      "trigger times: 20\n",
      "Epoch: 056, Loss: 1.1317 Test: 1.0016 \n",
      "trigger times: 21\n",
      "Epoch: 057, Loss: 1.1670 Test: 0.9905 \n",
      "Epoch: 058, Loss: 6.2913 Test: 1.2994 \n",
      "trigger times: 1\n",
      "Epoch: 059, Loss: 1.1766 Test: 1.0747 \n",
      "trigger times: 2\n",
      "Epoch: 060, Loss: 1.1412 Test: 1.0399 \n",
      "trigger times: 3\n",
      "Epoch: 061, Loss: 1.1103 Test: 1.0494 \n",
      "trigger times: 4\n",
      "Epoch: 062, Loss: 1.1074 Test: 1.0167 \n",
      "trigger times: 5\n",
      "Epoch: 063, Loss: 1.1011 Test: 1.0244 \n",
      "trigger times: 6\n",
      "Epoch: 064, Loss: 1.1041 Test: 1.0205 \n",
      "trigger times: 7\n",
      "Epoch: 065, Loss: 1.0858 Test: 1.0846 \n",
      "trigger times: 8\n",
      "Epoch: 066, Loss: 1.0847 Test: 1.0083 \n",
      "trigger times: 9\n",
      "Epoch: 067, Loss: 1.0847 Test: 1.0164 \n",
      "trigger times: 10\n",
      "Epoch: 068, Loss: 1.0868 Test: 1.0206 \n",
      "trigger times: 11\n",
      "Epoch: 069, Loss: 1.0781 Test: 1.0281 \n",
      "trigger times: 12\n",
      "Epoch: 070, Loss: 1.0762 Test: 1.0444 \n",
      "trigger times: 13\n",
      "Epoch: 071, Loss: 1.1055 Test: 1.0533 \n",
      "trigger times: 14\n",
      "Epoch: 072, Loss: 1.0570 Test: 1.0001 \n",
      "trigger times: 15\n",
      "Epoch: 073, Loss: 1.0713 Test: 0.9811 \n",
      "Epoch: 074, Loss: 1.0736 Test: 0.9957 \n",
      "trigger times: 1\n",
      "Epoch: 075, Loss: 1.0698 Test: 1.0241 \n",
      "trigger times: 2\n",
      "Epoch: 076, Loss: 1.0657 Test: 1.0215 \n",
      "trigger times: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 077, Loss: 1.0473 Test: 1.0072 \n",
      "trigger times: 4\n",
      "Epoch: 078, Loss: 1.0713 Test: 1.0192 \n",
      "trigger times: 5\n",
      "Epoch: 079, Loss: 1.0771 Test: 1.2675 \n",
      "trigger times: 6\n",
      "Epoch: 080, Loss: 1.0744 Test: 1.0664 \n",
      "trigger times: 7\n",
      "Epoch: 081, Loss: 1.0719 Test: 1.0944 \n",
      "trigger times: 8\n",
      "Epoch: 082, Loss: 1.0686 Test: 1.0294 \n",
      "trigger times: 9\n",
      "Epoch: 083, Loss: 1.0835 Test: 0.9697 \n",
      "Epoch: 084, Loss: 1.0802 Test: 0.9985 \n",
      "trigger times: 1\n",
      "Epoch: 085, Loss: 1.0765 Test: 1.0354 \n",
      "trigger times: 2\n",
      "Epoch: 086, Loss: 1.0719 Test: 1.0195 \n",
      "trigger times: 3\n",
      "Epoch: 087, Loss: 1.0909 Test: 0.9717 \n",
      "trigger times: 4\n",
      "Epoch: 088, Loss: 1.0623 Test: 1.0056 \n",
      "trigger times: 5\n",
      "Epoch: 089, Loss: 1.0898 Test: 0.9845 \n",
      "trigger times: 6\n",
      "Epoch: 090, Loss: 1.0727 Test: 1.0446 \n",
      "trigger times: 7\n",
      "Epoch: 091, Loss: 1.0710 Test: 1.0070 \n",
      "trigger times: 8\n",
      "Epoch: 092, Loss: 1.0910 Test: 0.9976 \n",
      "trigger times: 9\n",
      "Epoch: 093, Loss: 1.0749 Test: 1.0465 \n",
      "trigger times: 10\n",
      "Epoch: 094, Loss: 1.0839 Test: 1.1124 \n",
      "trigger times: 11\n",
      "Epoch: 095, Loss: 1.0970 Test: 0.9887 \n",
      "trigger times: 12\n",
      "Epoch: 096, Loss: 1.1321 Test: 1.0900 \n",
      "trigger times: 13\n",
      "Epoch: 097, Loss: 1.0989 Test: 0.9599 \n",
      "Epoch: 098, Loss: 1.0597 Test: 1.1105 \n",
      "trigger times: 1\n",
      "Epoch: 099, Loss: 1.1325 Test: 1.0899 \n",
      "trigger times: 2\n",
      "Epoch: 100, Loss: 1.0952 Test: 1.0951 \n",
      "trigger times: 3\n",
      "Epoch: 101, Loss: 1.1283 Test: 1.1431 \n",
      "trigger times: 4\n",
      "Epoch: 102, Loss: 1.1042 Test: 1.2603 \n",
      "trigger times: 5\n",
      "Epoch: 103, Loss: 1.1456 Test: 1.1343 \n",
      "trigger times: 6\n",
      "Epoch: 104, Loss: 1.2101 Test: 1.0088 \n",
      "trigger times: 7\n",
      "Epoch: 105, Loss: 1.1471 Test: 1.2357 \n",
      "trigger times: 8\n",
      "Epoch: 106, Loss: 1.1507 Test: 0.9455 \n",
      "Epoch: 107, Loss: 1.1167 Test: 0.9955 \n",
      "trigger times: 1\n",
      "Epoch: 108, Loss: 1.1116 Test: 1.0409 \n",
      "trigger times: 2\n",
      "Epoch: 109, Loss: 1.1293 Test: 1.4262 \n",
      "trigger times: 3\n",
      "Epoch: 110, Loss: 1.2783 Test: 0.9810 \n",
      "trigger times: 4\n",
      "Epoch: 111, Loss: 1.1538 Test: 1.0875 \n",
      "trigger times: 5\n",
      "Epoch: 112, Loss: 1.1378 Test: 1.0469 \n",
      "trigger times: 6\n",
      "Epoch: 113, Loss: 1.1523 Test: 1.0346 \n",
      "trigger times: 7\n",
      "Epoch: 114, Loss: 1.1108 Test: 0.9959 \n",
      "trigger times: 8\n",
      "Epoch: 115, Loss: 1.1946 Test: 1.0423 \n",
      "trigger times: 9\n",
      "Epoch: 116, Loss: 1.0912 Test: 1.0081 \n",
      "trigger times: 10\n",
      "Epoch: 117, Loss: 1.1341 Test: 0.9742 \n",
      "trigger times: 11\n",
      "Epoch: 118, Loss: 1.1093 Test: 0.9765 \n",
      "trigger times: 12\n",
      "Epoch: 119, Loss: 1.0898 Test: 0.9943 \n",
      "trigger times: 13\n",
      "Epoch: 120, Loss: 1.1139 Test: 1.0070 \n",
      "trigger times: 14\n",
      "Epoch: 121, Loss: 1.1572 Test: 1.0360 \n",
      "trigger times: 15\n",
      "Epoch: 122, Loss: 1.1553 Test: 1.0328 \n",
      "trigger times: 16\n",
      "Epoch: 123, Loss: 1.1215 Test: 1.0303 \n",
      "trigger times: 17\n",
      "Epoch: 124, Loss: 1.2363 Test: 1.0274 \n",
      "trigger times: 18\n",
      "Epoch: 125, Loss: 1.1258 Test: 1.2724 \n",
      "trigger times: 19\n",
      "Epoch: 126, Loss: 1.0852 Test: 1.2442 \n",
      "trigger times: 20\n",
      "Epoch: 127, Loss: 1.1515 Test: 1.1073 \n",
      "trigger times: 21\n",
      "Epoch: 128, Loss: 1.1114 Test: 1.1436 \n",
      "trigger times: 22\n",
      "Epoch: 129, Loss: 1.1372 Test: 1.0319 \n",
      "trigger times: 23\n",
      "Epoch: 130, Loss: 1.1527 Test: 1.1004 \n",
      "trigger times: 24\n",
      "Epoch: 131, Loss: 1.1825 Test: 1.0668 \n",
      "trigger times: 25\n",
      "Epoch: 132, Loss: 1.1938 Test: 1.0213 \n",
      "trigger times: 26\n",
      "Epoch: 133, Loss: 1.0852 Test: 1.0506 \n",
      "trigger times: 27\n",
      "Epoch: 134, Loss: 1.1145 Test: 0.9824 \n",
      "trigger times: 28\n",
      "Epoch: 135, Loss: 1.1295 Test: 1.0652 \n",
      "trigger times: 29\n",
      "Epoch: 136, Loss: 1.1480 Test: 0.9894 \n",
      "trigger times: 30\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    }
   ],
   "source": [
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "for fold in tqdm(range(folds)):\n",
    "    model = GAT().to(device)\n",
    "    # model = define_model(trial).to(device)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=.0023467,\n",
    "    #                              weight_decay=.00095)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.007531829,\n",
    "#                                    weight_decay=0.000252036)\n",
    "#     model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "#                     edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "#                     dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "    result_file_name = 'result_' + str(fold) +  '.csv'\n",
    "    \n",
    "    train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    model\n",
    "    \n",
    "    \n",
    "    train_loader   = DataLoader(train_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    test_loader  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     model = AttentiveFP(in_channels=112, hidden_channels=279, out_channels=1,\n",
    "#                     num_layers=3, num_timesteps=2,\n",
    "#                     dropout=0.047352327938708194).to(device)\n",
    "    best_ret = []\n",
    "    \n",
    "#     model = model.cuda(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "#     best_mae = 0.00\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss,train_rmse=train(model, optimizer,train_loader)\n",
    "        test_loss,test_rmse = test(test_loader, model)\n",
    "#         score = metrics.r2_score(true, prediction)\n",
    "#         , true, prediction\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f} ') #f'score: {score:.4f} '   \n",
    "        \n",
    "        ret = [epoch,train_rmse,test_rmse]\n",
    "        \n",
    "        train_losses.append(train_rmse)\n",
    "        val_losses.append(test_rmse)\n",
    "#         scores.append(score)\n",
    "        # Early Stopping\n",
    "        the_current_loss = test_rmse   #.item()\n",
    "        best_ret.append(ret)\n",
    "        if the_current_loss > the_last_loss:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "    \n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                break\n",
    "        else:\n",
    "            ret = [epoch,train_rmse,test_rmse] #, score\n",
    "            trigger_times = 0\n",
    "            the_last_loss = the_current_loss\n",
    "            best_rmse = the_current_loss\n",
    "            \n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "        # Early stopping\n",
    "#         the_current_loss = test_loss.item()\n",
    "        \n",
    "#         best_ret.append(ret)\n",
    "        \n",
    "#         if the_current_loss > the_last_loss:\n",
    "#             trigger_times += 1\n",
    "#             print('trigger times:', trigger_times)\n",
    "            \n",
    "#             if trigger_times >= patience:\n",
    "#                 print('Early stopping!\\nStart to test process.')\n",
    "#                 break\n",
    "#         else:\n",
    "#             ret = [epoch,train_loss,test_loss.item()]\n",
    "#             trigger_times = 0\n",
    "#             best_mae = the_current_loss\n",
    "#             the_last_loss = the_current_loss\n",
    "            \n",
    "#             torch.save(model.state_dict(), model_file_name)\n",
    "\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_float = \"{:.2f}\".format(best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(format_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resSt = results[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_val = resSt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1.3471797181099583, 1.2614065361126983]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "125\n",
      "96\n",
      "96\n",
      "37\n",
      "37\n",
      "116\n",
      "116\n",
      "127\n",
      "127\n",
      "47\n",
      "47\n",
      "104\n",
      "104\n",
      "42\n",
      "42\n",
      "133\n",
      "133\n",
      "137\n",
      "137\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEWCAYAAACZnQc8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACd90lEQVR4nOydd5hdVbmH37VPL9NbpqVNejIppBFagkkISBEUld5E5VpQVDQqggoKUa7XghpBkSKC9NAhGEISSO/JpGcm03s7ve11/9jnnOnJTDJJJpn9Ps95Zma3s/aec7797d/61m8JKSU6Ojo6OmcHyulugI6Ojo5O/6EHdR0dHZ2zCD2o6+jo6JxF6EFdR0dH5yxCD+o6Ojo6ZxF6UNfR0dE5i9CDus5ZgRDiKSHEQ0dZL4UQo07C+5YIIRZEf/+JEOLvvdn2ON7nQiHEvuNtp87gQQ/qOieMEOJbQohNQoiAEOKpbtbPF0LsFUJ4hRAfCSGGHeVY84QQqhDC3e715kls+9+EEM90s3xy9HxSe3ssKeWvpZR39lO7OtyEpJSrpZRj++PYOmc3elDX6Q8qgYeAJzuvEEKkA68CPwNSgU3Af451PCmls93ryv5ucDueAj4vhHB0Wn4L8JaUsvEkvreOTr+jB3WdE0ZK+aqU8nWgoZvVnwd2SylfklL6gZ8DU4QQ4/r6PkKI8UKIlUKIZiHEbiHEVUfZ9l4hRJUQolIIccdR2r4WqAC+0G5fA3AD8LQQokAIsUII0SCEqBdCPCeESO7hPX8uhPhXu79vFkIcie77007bzhJCrI2eS5UQ4jEhhDm6blV0s+3RJ5UvR59gyntzLaJS1J+FEG8LIVxCiPVCiIKeroHO2YUe1HVONhOB7bE/pJQe4FB0ea8RQpiAN4EPgEzg28BzQogukoQQ4lLgB8BCYDRwLB37GbTMPMYCwAS8CwjgYSAHGA/ko92YjtXeCcBfgZuj+6YBee02iQD3AOnAHGA+8A0AKeVF0W2mRJ9UOjzZ9PJaXA/8AkgBDgK/Olabdc4O9KCuc7JxAi2dlrUACUfZJyeagcZeXwLOjR7rESllUEq5AngLLXh15kvAP6WUu6I3kZ8fo43PAnOFELGgewvwbyllSEp5UEq5XEoZkFLWAb8D5h7jeADXosk3q6SUATT5SY2tlFJullKuk1KGpZQlwN96eVzo3bV4VUq5QUoZBp4Dpvby2DpnOMbT3QCdsx43kNhpWSLgEkIMBYpiC6WUzuivlVLK9lktQogvA2VSSrXd4iNAbjfvmQNs7rRdj0gpS6OSx01CiMeAq4ELo++bCfwx+ncCWiLUdLTjtWtDWbv38Agh4vKUEGIM2g1iBmBH+y5u7nyQox37GNeiut3vXrSbgM4gQM/UdU42u4EpsT+iHZIFaDp7afsO0WMcpxLIF0K0/8wORdPDO1OFJpO03+5YPI2WoX8BKJZSbokufxiQwGQpZSJwE5okcyw6tEEIYUeTYGL8FdgLjI4e9ye9PC707VroDDL0oK5zwgghjEIIK2AADEIIqxAi9hT4GjBJCPGF6Db3AzuklHv7+DbrAQ/wQyGESQgxD7gSeKGbbV8EbhNCTIgG0wd6cfxX0ILwL9ACfIwEtKeNZiFELnBvL9v7MnCFEOKCaAfoL+n4fUsAWgF3tNP4fzrtXwOM7OHYfbkWOoMMPajr9Af3AT5gMVom64suI6pDfwGto64JmA1c19c3kFIGgauAy4B64C/ALd3dHKSU7wK/B1agdRKu6MXxPbQF9ufarfoFcA5aP8DbaOWZvWnvbuCbwL/RsvYmoLzdJj9Aq7BxAU/Qtczz52jVN7E+hfbH7vW10Bl8CH2SDB0dHZ2zBz1T19HR0TmL0IO6jo6OzlmEHtR1dHR0ziL0oK6jo6NzFnHaBh+lp6fL4cOHn66319HR0Tkj2bx5c72UMqOn9actqA8fPpxNmzadrrfX0dHROSMRQhx1hLQuv+jo6OicRehBXUdHR+csQg/qOjo6OmcRA8qlMRQKUV5ejt/vP91N0TlDsFqt5OXlYTKZTndTdHQGBAMqqJeXl5OQkMDw4cMRoreGdTqDFSklDQ0NlJeXM2LEiNPdHB2dAcGAkl/8fj9paWl6QNfpFUII0tLS9Cc7HZ12DKigDugBXadP6J8XHZ2ODLigrqOjo9MfHN5Wh6clcLqbccrRg7qOjs5ZhxpRee9vO9m7tup0N+WUowf1ThgMBqZOnRp/lZSU9Ljtbbfdxssvv9xl+cqVK7niiiu63efhhx9m1KhRjB07lvfff7/HY1977bUcPnwY0EbfFhYWMnnyZObOncuRI20DyoQQ3HzzzfG/w+EwGRkZ8fevqanhiiuuYMqUKUyYMIHPfvazAJSUlGCz2Tqc6zPPPNPzheklvTm/7du3M2fOHAoLC7nyyitpbW0FtOqnW2+9lcLCQsaPH8/DDz8c32fBggU0NfVmalAdHVBViZQQCanH3vgsY0BVvwwEbDYb27ZtOynHLioq4oUXXmD37t1UVlayYMEC9u/fj8Fg6LDd7t27iUQijBzZNpvZRx99RHp6Og888AAPPfQQTzzxBAAOh4Ndu3bh8/mw2WwsX76c3Ny2+Yfvv/9+Fi5cyHe+8x0AduzYEV9XUFDQr+fa2/O78847efTRR5k7dy5PPvkkv/3tb3nwwQd56aWXCAQC7Ny5E6/Xy4QJE7j++usZPnw4N998M3/5y1/46U9/2m/t1Tl7iU3JPRjnABqwQf0Xb+6mqLK1X485ISeRB66c2Of9tm3bxl133YXX66WgoIAnn3ySlJSUDtu89957fPe73yU9PZ1zzjmn2+MsW7aM6667DovFwogRIxg1ahQbNmxgzpw5HbZ77rnn+NznPtftMebMmcMf//jHDssuu+wy3n77ba699lqef/55rr/+elavXg1AVVUVl1xySXzbyZMn9/n8e0tvz2/fvn1cdNFFACxcuJBFixbx4IMPIoTA4/EQDofx+XyYzWYSExMBuOqqq7jwwgv1oK7TK6SqRXNVHXxRXZdfOuHz+eJyxDXXXAPALbfcwpIlS9ixYweFhYX84he/6LCP3+/nq1/9Km+++SarV6+murq622NXVFSQn982yX1eXh4VFV0ngP/kk0+YPn16t8d47733uPrqqzssu+6663jhhRfw+/3s2LGD2bNnx9d985vf5Ctf+QoXX3wxv/rVr6isrIyvO3ToUAf5JXYjaM8999zTYZvY65FHHjnu85s0aRJvvPEGAC+99BJlZWWAJjk5HA6ys7MZOnQoP/jBD0hNTQUgJSWFQCBAQ0NDt9dFR6c9sWAuB2FQH7CZ+vFk1P1BZ/mlpaWF5uZm5s6dC8Ctt97KF7/4xQ777N27lxEjRjB69GgAbrrpJh5//PEux+5uPtjuSvKqqqrIyOjorHnxxRdTU1NDZmYmDz30UId1kydPpqSkhOeffz6umcdYtGgRhw8f5r333uPdd99l2rRp7Nq1C+id/PJ///d/R13fnt6e35NPPsndd9/NL3/5S6666irMZjMAGzZswGAwUFlZSVNTExdeeCELFiyIy1CZmZlUVlaSlpbW6zbpDE5in0U9U9c5bnpTL52XlxfPSkEbQZuTk9NlO5vN1mVAzUcffcSRI0eYOHEi999/f5d9rrrqKn7wgx9w/fXXd1mXmprKDTfcwLPPPsvMmTNZtWpVb04J6Fum3tvzGzduHB988AGbN2/m+uuvp6CgAIB///vfXHrppZhMJjIzMzn//PM72DP7/X5sNluv264zeIlr6npQ1+lMUlISKSkpcWni2WefjWftMcaNG0dxcTGHDh0C4Pnnn+/2WFdddRUvvPACgUCA4uJiDhw4wKxZs7psN378eA4ePNhluc1m4/e//z3PPPMMjY2NHdbdcccd3H///RQWFnZYvmLFCrxeLwAul4tDhw4xdOjQXp69lqlv27aty2vx4sXHfX61tbUAqKrKQw89xF133QXA0KFDWbFiBVJKPB4P69atY9y4cYCWeVVXV6NPrKLTG2RcfjnNDTkN6EG9Fzz99NPce++9TJ48mW3btnXJlK1WK48//jiXX345F1xwAcOGDev2OBMnTuRLX/oSEyZM4NJLL+XPf/5zl8oQgMsvv5yVK1d2e4zs7Gyuv/56/vznP3dYnpeXF69wac/mzZuZMWMGkydPZs6cOdx5553MnDkT6Kqpd+6A7StHO78777wznnU///zzjBkzhnHjxpGTk8Ptt98OaPq/2+1m0qRJzJw5k9tvvz3esbt582bOPfdcjMYBqxjqDCAGs6YuutNBTwUzZsyQnWc+2rNnD+PHjz8t7RlI+Hw+Lr74Yj755JNug/5g5Dvf+Q5XXXUV8+fP77JO/9zodKa13sez961lwoU5XHzjuNPdnH5FCLFZSjmjp/V6pj4Asdls/OIXv+i2cmSwMmnSpG4Duo5Od8SS1cGYqevPsgOURYsWne4mDCi++tWvnu4m6JxBxDtKI4MvqOuZuo6OzllHTFNXB+GQUj2o6+jonHXo1S9HQQhhFUJsEEJsF0LsFkL8optt5gkhWoQQ26KvroXUOjo6OqcIXVM/OgHgM1JKtxDCBKwRQrwrpVzXabvVUsrurQl1dHR0TiH64KOjIDXc0T9N0ddZe6VOpvVuQ0MDF198MU6nk29961tHbceZaL3b2/NrbGxk4cKFjB49moULF3ax1C0tLcXpdPLoo4/Gl+nWuzp9QdUNvY6OEMIghNgG1ALLpZTru9lsTlSieVcI0a1xixDia0KITUKITXV1dcff6pNIzPsl9urPEYxWq5UHH3ywQ7Dqjp6sd3fs2MG8efM6eL+0t94FerTe3b59O0VFRR2G98e8X2KvW2655ZSc3yOPPML8+fM5cOAA8+fP72I5cM8993DZZZd1WBaz3tXR6Q1yEA8+6lVJo5QyAkwVQiQDrwkhJkkpd7XbZAswLCrRfBZ4HRjdzXEeBx4HbfDRUd/03cVQvbM3zes9Qwrhsq6eJceiv6x3HQ4HF1xwQbcWAO05U613e3t+y5Yti4+YvfXWW5k3bx5LliwB4PXXX2fkyJE4HI4O++jWuzp9oc169zQ35DTQp+oXKWUzsBK4tNPy1phEI6V8BzAJIdL7qY2nlJNpvdtbzlTr3d5SU1NDdnY2oNkexLxgPB4PS5Ys4YEHHuiyj269q9MX4h2lg7Ck8ZiZuhAiAwhJKZuFEDZgAbCk0zZDgBoppRRCzEK7WZzYt+84Mur+4GRa7/aWM9V690R54IEHuOeee3A6nd2u1613dXqLOog7Snsjv2QDTwshDGjB+kUp5VtCiLsApJRLgWuB/xFChAEfcJ0cZLfI3ljv9paerHcdDge33XYb999/P7/73e86rI9Z765cubJLNhuz3r3hhhu44oorWLVqVY9PAp255557+Oijj7osv+6667p1auwNWVlZVFVVkZ2dTVVVFZmZmQCsX7+el19+mR/+8Ic0NzejKApWqzXe6apb7+r0Fl1TPwpSyh3AtG6WL233+2PAY/3btIFBe+vdCy+88JjWuwUFBT1a7/aWmPVu507amPVuYWEh9913X3xWINCsd5OSkigsLOzg8LhixQrOPfdc7Hb7cVvv9jdXXXUVTz/9NIsXL+bpp5+O9x+0l39+/vOfd6ii0a13dfqCPp2dzlHpL+td0MoTv/e97/HUU0+Rl5dHUVFRl23OVOvdo51fe+vdxYsXs3z5ckaPHs3y5ct7lfHr1rs6fUG33j0N6Na7PaNb73ZFt97V6QvFO+p55y87yByeyBcX9+hSe0aiW++egejWu13RrXd1+kLMnXEwZur6s+wARbfe7YhuvavTF+Lyy+Cq1wD0TF1HR+csZDAbeulBXUdH56wjXv2iT5Kho6Ojc+YTr1MffDFdD+o6OjpnH7ERpXqdus5Jtd5dvnw506dPp7CwkOnTp7NixYoej302W+++9NJLTJw4EUVRaF/W2rlNd911V3ydbr2r0xcGs6auV790orP3S3+Snp7Om2++SU5ODrt27WLRokXdli32ZL2bnp7OAw88wEMPPcQTTzwBdLTetdlsPVrvxgYm7dixI76uN94vfSFmvbtr1664v0x3TJo0iVdffZWvf/3rXdb11KaY9a7u0qjTG3SbgAHIkg1L2Nu4t1+POS51HD+a9aM+79df1rvTprW5LUycOBG/308gEMBisXTY7my33j2egUK69a5OXxjMQV2XXzpxqqx3X3nlFaZNm9YloMPZb717NIqLi5k2bRpz587t0B7delenLwxmTX3AZurHk1H3B6fCenf37t386Ec/4oMPPuh2/WC13s3Ozqa0tJS0tDQ2b97M1Vdfze7du0lMTAR0612d3tOWqZ/mhpwG9Ey9n+it9W55eTnXXHMNzzzzDAUFBd1u05P17pEjR5g4cWIXQzFos969/vrru6yLWe8+++yzzJw5k1WrVvWqrXBqM3WLxRIP2NOnT6egoID9+/fH1+vWuzq9RZ8kQ6dH+tN6t7m5mcsvv5yHH36Y888/v8f3PNutd3uirq6O1NRUDAYDhw8f5sCBA/HOYt16V6cv6BNP6xyV/rLefeyxxzh48CAPPvhgPOONTeXWnrPdeve1114jLy+PtWvXcvnll8d9blatWsXkyZOZMmUK1157LUuXLo3fuHTrXZ2+MJg7SnXr3QGIbr3bFd16V6cvbHqnhPVvHMZgVLjrsXmnuzn9im69ewaiW+92Rbfe1ekLsWR1MMov+rPsAEW33u2Ibr2r0xcG88xHeqauo6Nz1tE+mA+2wK4HdR0dnbOO9vXp6iAra9SDuo6OzlmHnqnr6OjonEW0z84H20QZelDvxMm03t2wYUP8uFOmTOG1117r8dhns/VuY2MjCxcuZPTo0SxcuDBuqXu066Nb7+r0hQ6Z+uCK6Xr1S2dOpvXupEmT2LRpE0ajkaqqKqZMmcKVV17ZZUDN2W69+8gjjzB//nwWL17MI488wiOPPMKSJUuOen10612dvtBeUx9s8suADerVv/41gT39a71rGT+OIT/5SZ/36y/rXbvdHv/d7/f36BdztlvvLlu2LD5i9tZbb2XevHksWbLkqNdHt97V6Qu6pn4UhBBWIcQGIcR2IcRuIcQvutlGCCH+KIQ4KITYIYToPqqdAZxs693169czceJECgsLWbp0abfD3s92692amhqys7MBzfagvVVCT9dHt97V6QsdNPVBFtR7k6kHgM9IKd1CCBOwRgjxrpRyXbttLgNGR1+zgb9Gfx43x5NR9wcn23p39uzZ7N69mz179nDrrbdy2WWXYbVaO2wzWK134ejXR7fe1ektHTP109iQ08AxM3Wp4Y7+aYq+Ot/6Pgc8E912HZAshMju36YObHprvRtj/PjxcT28M2e79W5WVhZVVVWAdgPLzMzssk1310e33tXpLe2DuqoOrqjeq+oXIYRBCLENqAWWSynXd9okFyhr93d5dFnn43xNCLFJCLGprq7uOJt8amlvvQsc03oX6NF6t7i4mHA4DMCRI0fYt29ft1ayMevdzsSsd5955hkaGxs7rLvjjju4//77KSws7LB8xYoVeL1egOO23t22bVuX1+LFi3t9jM5cddVVPP3004DmgBnrPzja9dGtd3X6QseO0tPXjtNBr4K6lDIipZwK5AGzhBCTOm3SXZraRciSUj4upZwhpZzRWV4YyPSX9e6aNWuYMmVKXK//y1/+Qnp6epftznbr3cWLF7N8+XJGjx7N8uXL4zeIo10f3XpXpy+og7ijtM/Wu0KIBwCPlPLRdsv+BqyUUj4f/XsfME9KWdXTcXTr3Z7RrXe7olvv6vSF9/++i4ObtA74G34+m5QhjtPcov7jhK13hRAZQojk6O82YAHQudbwDeCWaBXMuUDL0QK6ztHRrXe7olvv6vSFjpr64MrUe/Msmw08LYQwoN0EXpRSviWEuAtASrkUeAf4LHAQ8AK3n6T2Dhp0692O6Na7On1hMGvqxwzqUsodwLRuli9t97sEvtm/TdPR0dE5Pgazpq57v+jo6Jx1yEE8+EgP6jo6Omcduk2Ajo6OzllEe7tdPagPck6m9W6M0tJSnE4njz76aI/bDEbr3RjdXR/delenL7SXX/patn2mowf1TsS8X2KvkzGC8Z577uGyyy7rcX1P1rs7duxg3rx5Hbxf2lvvAj1a727fvp2ioqIOw/tj3i+x1y233HJC5xWz3j3azQrarHcPHDjA/Pnzu1gOdHd9Yta7Ojq9ocN0dnr1y8Bg9Yv7qS9zH3vDPpCe7+TCL43p8379Zb0L8PrrrzNy5Egcjp4HQwxW613o+fro1rs6fUGqEoNRIRJWkfrMR4Obk2m96/F4WLJkCQ888MBR2zBYrXePdn10612dvqCqEsWouZcMtomnB2ymfjwZdX9wMq13H3jgAe655x6cTudR2zBYrXePdX10612d3iJVicGgECIy6DpKB2xQP9PojfXu+vXrefnll/nhD39Ic3MziqJgtVq7dCr2ZL3rcDi47bbbuP/++/nd737XYX3MenflypVdstmY9e4NN9zAFVdcwapVq3p8EujMPffcw0cffdRl+XXXXXfcTo0x693s7OwO1rvHuj669a5Ob5GSeKauB3WdDrS33r3wwguPab1bUFDQo/Vue3nj5z//eY9VIjHr3c6dtDHr3cLCQu677z5SU1Pj6+644w6SkpIoLCzs4PC4YsUKzj33XOx2+3Fb7/Y3MevdxYsXd7DePdr10a13dfqCGtXUYfDZBOiaei/oL+vd3jJYrXePhm69q9MXZLugPthGlPbZere/0K13e0a33u2Kbr2r0xf+/fN1CEXQWOnhkq9MZPTMrNPdpH7jhK13dU49uvVuV3TrXZ2+ICWDNlPXn2UHKLr1bkd0612dvqCqEsUQ7SgdZCWNeqauo6Nz1iE7dJTqQV1HR0fnjEYL6rGSxtPcmFOMHtR1dHTOOqQqUQappq4HdR0dnbMOVYLBoMsvOpxc693Odrd33XVXj8c+m613f/aznzF58mSmTp3KJZdcEvej2bBhQ7wtU6ZM4bXXXovvo1vv6vQF2d77ZZAFdb36pROdvV/6m974rfRkvZuens4DDzzAQw89xBNPPAF0tN612Ww9Wu/GBibt2LGjT23pCzHr3V27dsX9Zbrj3nvv5cEHHwTgj3/8I7/85S9ZunQpkyZNYtOmTRiNRqqqqpgyZQpXXnklRqMxbr2ruzTq9IaY90vs98HEgA3qHz31OLVHDvfrMTOHjeTi277W5/3603q3N5zt1ruJiYnx3z0eT9w3x263x5f7/f4Ofjq69e7gofyee3BeeBHJn7/muI/RPlPXO0oHOSfTeheguLiYadOmMXfu3G6tbuHst94F+OlPf0p+fj7PPfccv/zlL+PL169fz8SJEyksLGTp0qVxWwDdenfw4FnzCb4TfIJU2w0+Gmx16gM2Uz+ejLo/OJnWu9nZ2ZSWlpKWlsbmzZu5+uqr2b17d4fMFQaH9e6vfvUrfvWrX/Hwww/z2GOPxW+Us2fPZvfu3ezZs4dbb72Vyy67DKvVCujWu4MFGQ4jw+ETO0Y7+UXVJ8nQOR56Y71rsVjiAWn69OkUFBSwf//+Ltv1ZL175MgRJk6c2MVQDNqsd6+//vou62LWu88++ywzZ85k1apVvT2tk5apx7jhhht45ZVXuiwfP358vL8ghm69OziQoRAyFDqxY7SXX/RMXac9/Wm9W1dXR2pqKgaDgcOHD3PgwIEOnaExznbr3QMHDsSfat544w3GjRsHaNJUfn4+RqORI0eOsG/fvvg10K13BwdSSgiH+yeoG/TqF50eePrpp+MdpSNHjuSf//xnh/XtrXfT09O54IILuq3+WLVqFffffz9GoxGDwcDSpUs7BOYYMevdBQsWdFnX3nr3Zz/7WXz50ax3v/Wtb2E0GlFVNW69W1JSEtfUY9xxxx3cfffdfbk0XRg+fDitra0Eg0Fef/11PvjgAyZMmMCdd97JXXfdxYwZM1i8eDH79u1DURSGDRvG0qVLAVizZg2PPPIIJpMJRVH4y1/+Qnp6evw8dOvdQUA0mJ9IUJdSIiUIRSDE4Kt+0a13ByC69W5XdOvdwYHq8bBv+gwccy9i6N/+dnzHUCV//cZHzLpyBJveLWHq/KHMuaagn1t6+jhh610hRL4Q4iMhxB4hxG4hRJd0UAgxTwjRIoTYFn11FX11eo1uvdsV3Xp3cBDvID2RTD2amQshUIQYdJl6b55lw8D3pZRbhBAJwGYhxHIpZVGn7VZLKbsOo9Q5LnTr3Y7o1ruDg5jsIoP9ENQVTYIZbJr6MTN1KWWVlHJL9HcXsAfIPfpeOjo6On0nlqmfiKauxoO6QCiDL1PvU0mjEGI4MA1Y383qOUKI7UKId4UQE3vY/2tCiE1CiE11dXV9b62Ojs5ZTTxTP4E69Vg3oaIIhDL4Okp7HdSFEE7gFeC7UsrWTqu3AMOklFOAPwGvd3cMKeXjUsoZUsoZnQfX6Ojo6Mj+qH5pr6krgkEW03sX1IUQJrSA/pyU8tXO66WUrVJKd/T3dwCTECK9X1uqo6Nz1iNDJy6/SF1+OTpCGyr5D2CPlPJ3PWwzJLodQohZ0eOekSYdJ9N6FzSXxDlz5sT9TTqPHI1xNlvvNjY2snDhQkaPHs3ChQvjlrq69a5Of2TqMU1dUTQJZrAF9d5Uv5wP3AzsFEJsiy77CTAUQEq5FLgW+B8hRBjwAdfJM3Rs7sm03g2Hw9x00008++yzTJkyhYaGBkwmU5ftznbr3UceeYT58+ezePFiHnnkER555BGWLFmiW+/qQLgfNPWoK6M2+GjwVb8cM6hLKdcARzU2kVI+BjzWX40CaH7zEMFKT38eEnOOg+Qr+z4Iob+sdz/44AMmT57MlClTAHo0pjrbrXeXLVsWtzK49dZbmTdvHkuWLNGtd3X6R1OX7eUXvaN00HMyrXf379+PEIJFixZxzjnn8Jvf/Kbb7c52692amhqys7MBzfagtrY2vk633h3c9Iv8EonJL4NTUx+wRhrHk1H3ByfTejccDrNmzRo2btyI3W5n/vz5TJ8+vctIycFgvdsTuvXu4KY/6tTbd5QqikDVJ8nQOR56Y72bl5fH3LlzSU9Px26389nPfpYtW7Z02e5st97NysqiqqoK0G5gmZmZXbbpT+tdj+cwm7fcQDjcv3KeTv/TP3XqHUeUnqHde8eNHtSPQXvrXeCY1rtAj9a7ixYtYseOHXi9XsLhMB9//DETJkzosl3MerczMevdZ555hsbGxg7r7rjjDu6//34KCws7LF+xYgVerxfguK13t23b1uW1ePHiXh+jM1dddRVPP/00oDlgxvoPiouLCUe/zP1pvdvaup3m5vX4/eXH3WadU0OspJFQ6LiDsdquTn0wyi96UO8FTz/9NPfeey+TJ09m27ZtXTLl9ta7F1xwAcOGDev2OCkpKXzve99j5syZTJ06lXPOOYfLL7+8y3Yx693uaG+9256jWe/OmDGDyZMnM2fOnLj1LnTV1Dt3wB4Pw4cP53vf+x5PPfUUeXl5FBVpFkF33nknMVfOxYsXs3z5ckaPHs3y5cvjN4g1a9YwZcqUeH9Gf1nvqjKo/VSDJ3x+OieXDrLLcUowseoXJS6/DK6grlvvDkB0692unIj1bln5s+zf/3NmTH+JpKQTmxRc5+TS/OprVP3kJwCM3bwJxeHo8zHqSl28+OuNXHZXIZvfLcHqNHHlt6f2c0tPHydsvatz6tGtd7tyIta7UtUz9TMFGQ61+/34dPWOJY2DT34ZsNUvgx3dercjJ2K9q6qB6M8TmyJN5+TTXn453gqYNk0dvfpFR+dsJJahS6kH9QFPu+z8eIN6e019MGbqelDXOetRdfnljKE/MnXd0EtH5yynTX7Rg/pAp0NQP15NvcPgIwZd9Yse1HXOaKSUx/zyx0oadfll4BOvU+cENHXZ5tKoZ+o6J9V697nnnutwbEVRehymfzZb77700ktMnDgRRVFoX9YaDAa5/fbbKSwsZMqUKR1q9Xuy3lW9XsK1tQQOF/f4frr8cubQIVM/znlKZefBR4MrpuvVL505mda7N954IzfeeCMAO3fu5HOf+xxTp07tst3Zbr07adIkXn31Vb7+9a93WB47p507d1JbW8tll13Gxo0bURSlZ+vdaJYeaWyAkSO6fb94UNcz9QGP7MeOUt16d4Dx7rvv9uh2eLwMGTKEyy67rM/79Zf1bntiFrndcbZb7/Y0UKioqChei56ZmUlycjKbNm1i1qxZPVrvxmqSZbDnLDymqUu9pHHA01FTP/GOUsWgyy+DnpNpvdue//znPz0G9bPdercnpkyZwrJlywiHwxQXF7N582bKysqAo1jvRoO6Ggj0eNw2+UUP6gOd/qxTV6KZ+mAL6gM2Uz+ejLo/OJnWuzHWr1+P3W5n0qRJ3a4frNa7d9xxB3v27GHGjBkMGzaM8847r4PXS7fWu/FMvecAEB9RKnVNfaDTYUTp8cov7Vwa9eoXneOmN9a7MV544YUes3Q4+613e8JoNMZdIZctW0Zzc3P8Rgk9WO9GhwvK4LEzdal3lA54+qdOXfs5WF0aB2ymPlBob7174YUXHtN6t6CgoEfrXQBVVXnppZeOGlhj1rudbWZj1ruFhYXcd999pKamxtfdcccdJCUlUVhY2KFqZMWKFZx77rnY7fbjtt49VXi9XqSUOBwOli9fjtFojFsT92i92wdNXe8oPQNoX57aD3XqWlDvj4adOeiZei/oL+tdgFWrVpGXl9ehsqUzZ7v17muvvUZeXh5r167l8ssvj/vc1NbWcs455zB+/HiWLFnCs88+2+E8urPelb3R1HXr3TMGGQohLJb478dDB019EE6SoVvvDkB0692u9GS9G6qpYc+uXeSWlZF2223d7rt23UK83sPk5lzPuHEPdbuNzsCg7Ot34duxg0hTE9m/eojkL3yhz8fYu66K/z61h5sePJct75dSsrOe25dccBJae3rQrXfPQHTr3a70aL0bzcpk4Gjyi16nfqYgw2GUaL/JCXu/6Jq6zkBCt97tSI/WuzLWUdoLTV2XXwY8MhRC2GNB/Xg1de2nUASK0KtfdHTOKNoGH/Wm+kXP1Ac6WqZu137vL01d7yjV0TmD6MvgI11+GfDIUAjFfmJBXbfe1dE5k+mTTYAuvwx0ZCh04pp6u8FHelDvBiFEvhDiIyHEHiHEbiFEl7o5ofFHIcRBIcQOIYQ+u6/OqUEevaNUVcOAGv1dD+oDHRkOIcxmMBhOwPtF+ymE0KazG2Qljb3J1MPA96WU44FzgW8KISZ02uYyYHT09TXgr/3aylPIybTeDYVC3HrrrRQWFjJ+/HgefvjhHo99NlvvNjY2snDhQkaPHs3ChQvjlrrLly9n+vTpFBYWMn36dFasWBHfpyfrXakevaNUtrMG0OWXgY8MhRAmE8Jk6idNHWRED+odkFJWSSm3RH93AXuA3E6bfQ54RmqsA5KFENn93tpTQMz7JfbqMoLxBHjppZcIBALs3LmTzZs387e//a3bm0ZP1rs7duxg3rx5Hbxf2lvvAj1a727fvp2ioqIOw/tj3i+x1y233HJC5xez3n300UePut0jjzzC/PnzOXDgAPPnz4+3KT09nTfffJOdO3fy9NNPd7hZxax3u3CMjtL22bneUXoGEAqfcFDvoqlLBtUApD6VNAohhgPTgPWdVuUCZe3+Lo8uq+q0/9fQMvljDlXfv/9BXO49fWneMUlwjmfMmJ/1eb/+st4VQuDxeAiHw/h8PsxmM4mJiV22O9utd5ctWxYfMXvrrbcyb948lixZwrRp0+LbTJw4Eb/fTyAQwGKx9Gi9G+8o7SFTj+npoBt6nQlombrxxIJ6B0MvEV0GfbBnOqPpdUepEMIJvAJ8V0rZ2nl1N7t0uTVKKR+XUs6QUs7o7EI4UDiZ1rvXXnstDoeD7Oxshg4dyg9+8IMO/i0xznbr3ZqaGrKztQe57Oxsamtru2zzyiuvMG3aNCzRIePHst7tWVNvJ7/omfqAJy6/GI394/0SjeSDqbO0V5m6EMKEFtCfk1K+2s0m5UB+u7/zgMputus1x5NR9wcn03p3w4YNGAwGKisraWpq4sILL2TBggVdfGAGq/VujN27d/OjH/2IDz74oMPybq131aNXv8SCuhBGXX45A5DhMBijmfpxTmcX7WZBEZqmDoMrqPem+kUA/wD2SCl/18NmbwC3RKtgzgVapJRVPWx7VtIb691///vfXHrppZhMJjIzMzn//PPp7H8DZ7/1blZWFlVV2sejqqqKzMzM+Lry8nKuueYannnmGQoKCjrs1531royNKO2hTj0W1A0Ghy6/nAH0R0dph5mPFC3EDaZRpb2RX84HbgY+I4TYFn19VghxlxDirug27wCHgYPAE8A3Tk5zTz3trXeBY1rvAj1a7w4dOpQVK1YgpcTj8bBu3TrGjRvXZbuY9W5nYta7zzzzDI2NjR3W3XHHHdx///0UFhZ2WL5ixQq8Xi/AcVvvtu9Mjb0WL17c62N05qqrruLpp58GNAfMWP9Bc3Mzl19+OQ8//DDnn39+h32Obb3bU1DXlhsNDr2k8QxAhvuno1REtXQ9U+8GKeUaKaWQUk6WUk6Nvt6RUi6VUi6NbiOllN+UUhZIKQullF3TzzOY/rLe/eY3v4nb7WbSpEnMnDmT22+/vduOy7Pdenfx4sUsX76c0aNHs3z58vgN4rHHHuPgwYM8+OCD8TbF9PaerHeP3VEazdSNTqRe0jigkZEIRCIIowlMxg6TUPfpOFLGg3ksuA8mqwDdencAolvvdqUn613/nj3sr6zE9uBDjF75UZf9GhrXsG3brSQlnYPLtZuL5xWdqibr9BE1EGDflKlk3HMPrhX/xZCQyNC/P9Hn43zy8gF2fVzB1/80j50ry1n1wn5u/80F2BPNJ6HVpx7devcMRLfe7UpP1rvyGDYBsr2mrneUDmhirozCeIIljSrt5JdYSePgkV90690Bim6925EerXfjfurda+qRmKZuTABUpIwghP70MxCRoWilUkxTP4pH/tFQZTtNPVq/oGvqOjpnAFr21ftMHfRa9QFNVEMXZhPCaDp+TV1t09QVgxbV1UFkFaAHdZ0zl9gjtRDIUCjuA9OeWEep0ejs8LfOwCMmt5y4/CLjI0kHo/yiB3WdMxc1bscHdG/V2lbSqAV1qdeqD1jiQT1e0nh8/yupyvi4kbYRpf3TxjMBPajrnLnEM3XtY9ydrt5W0qjLLwOduNxiNCKMxuN3aZRtGXosY9cHHw1iTqb1bjAY5Pbbb6ewsJApU6b0WIsOZ6b1LsDDDz/MqFGjGDt2LO+//36322zfvp05c+ZQWFjIlVdeSWtra7dtuuuuu+L7dGe922bcFM3GutHV1S6aup6pD1Q6Z+oc9xyl3dWpD56grle/dKKz90t/8sQTWs3tzp07qa2t5bLLLmPjxo3xocwxerLeTU9P54EHHuChhx6KH6u99a7NZuvRejc2MGnHjh3xdb3xfukLRUVFvPDCC+zevZvKykoWLFjA/v37u9Ta33nnnTz66KPMnTuXJ598kt/+9rc8+OCDR21TzHq3g0tjO00desjUo3JLm/yiZ+oDlXhJYz+MKFU6jygdRJr6gA3qPztQzi63r1+POclp48HReX3er7+sd4uKiuK11pmZmSQnJ7Np0yZmzZrVYbsz1Xp32bJlXHfddVgsFkaMGMGoUaPYsGEDc+bM6bDdvn37uOiiiwBYuHAhixYtigf1nujWejf2RY35e3RTAqeqAYQwoRgs0b/1oD5Qaeso7T+bgLj8ole/DF5OpvXulClTWLZsGeFwmOLiYjZv3kxZWVmX7c5U692Kigry89vMOvPy8rodQDVp0iTeeOMNQJs4pP01KC4uZtq0acydO7dDe7q13o0ZN8U7SruXXxTFjCLM0V10+WWg0lF+OQFNXaVr9csg6igdsJn68WTU/cHJtN6944472LNnDzNmzGDYsGGcd955Xb1MOHOtd7t7xO3OvfLJJ5/k7rvv5pe//CVXXXUVZrMWcLOzsyktLSUtLY3Nmzdz9dVXs3v37vhEIp2td2MOjbFMvaeOUkUxIxSTts1gzdT3vQcf/BT+Zy0YB+Zw+dicpMIczdRPyPtFL2kcNIQCAQI+b78ftzfWu0ajMe56uGzZMpqbm+M3gvacqda7eXl5HbLu8vJycnJyumw3btw4PvjgAzZv3sz1118ft9i1WCzxgD19+nQKCgrYv39/fL8u1rudNfVuOkqlGkRRLG2Z+mDtKK3YDA0Hwdd1nteBQvs6dU5Ufol+JhShV7+c9bga62mt6zrTTk/0p/Wu1+vF4/EA2lyiRqORCRM6z+F95lrvXnXVVbzwwgsEAgGKi4s5cOBAl/4CIO68qKoqDz30ULzKpa6ujkgkAsDhw4c5cOBAvLO4W+vdLpp6d5l6AEWYUWKZ+mDtKPXWaz+D7tPbjqMR7thRiqpqzo19pGP1S9uywcKgC+qRYBA1HO7T41h/We/W1tZyzjnnMH78eJYsWcKzzz7b7XZnqvXuxIkT+dKXvsSECRO4ZOEi/viHP8UrX9pb7z7//POMGTOGcePGkZOTw+233w7AqlWrmDx5MlOmTOHaa69l6dKl8en+urXelZ009R5KGkU7+WXQZuqeWFD3nN52HIUOI0qNpg7L+kL3mvrgCepIKU/La/r06bIzRUVFXZb1J5FwWFYd3C+rDu6X4VDopL7XieD1euXs2bNlOBw+5rbhYFCqqnoKWtV7goGwrClpkV5XoN+Oeffdd8sPP/yww7JQU5P07twpd23eLIvGjpMt777XZb9t278q162/Qrpc++SH/x0pq2ve7rc2nVE8eZmUDyRKWfLp6W5JjzQvWyaLxo6TgeJiWf/kP2XR2HEy7HL1+TjL/rBVvvTIRimllJUHmuRjX/+vPLK7vr+be9oANsmjxNZBlamH22VyauT4OmFOBb213g0Hg9SVlhD092/p54kSy4r6s+KgW+vdzpp6d9UvkYCmqQ/2jtIzIlPvJL9wfJl6B5uAQVj9MuCCujyJvdThdl/6yHH2rJ8qFi1adEztO3aTivTgUHi6iAf1fvxfdmu9G81MhKIggV37BK31HW9wqoxWv/RDR6kaUfG0dG/xO+A5AzT1eAA3tgvqxzH59ImMKN26vJRXH93c5/ccSAyooG61WmloaDhpgb1Dpj7Ag3pviERLwAbaDSqWFZ1sHVOqKs2hEBaDgZDJwbaDdg5squmwTbxOPaapn0BH6f6NNfzr/nWEAn3vvDutqJG2qpeBnKl3sN7V+k6OT1NvG1HaV++XmuIWakpaz+gSyAFVp56Xl0d5eTl1dXUn5fie5iaklKiRMOamZqwO50l5n1OF3+0i6PNhqm/EVntyrtnxEPSHCXjCmKwGrA7TSXsf1eVC3bWLUZdeyh6DFYCAt+MNri2oa5m6PIFM3dXgJxyI4PeEMFnOoIk2fM1td9qBHNTbd5Sao5+b8PFk6iCMx2cT4G0JooYloUAEs3VAhcdeM6BabTKZGDFixEk7/l+/dhMjps6grGgnueMm8Nlvff+kvdepYNmjD3Fw4zryJkziyw90rRs/XWx6p4SNbxxmzOwsFt5+8uacrV+6lLrf/wHzl75M2KjVrwd8nYO6pqkLceLVL0G/lqEHfQPryeiYxKQXgNAZENRPVFPvMPNR3zT1mLzm94TO2KA+oOSXk4nP1Yq3pZm0/KEkpKXhbqg/9k4DnNboE427/dD5U8irDz/A6uef7rI8FvSCvpMrU8RKGBWHnXA0Uw92l6mL/pFfQv7YeZ1hQd3T7rPeLlMPVnlQj3IuEVUy97cf8drW8pPZujjxEaX92FEan/momwlUuuwnJd4W7TPld5+5HeqDJqg3lJUCkJ4/DGdqOq7GsyCo12uDeFyN9adcA5SqSunuHez6aHmXGYcC0eAXCpzc4CeDQYTZjBAC1apJaQFvxy+jjNkEiBOvfoll6p2fBgY83q5BXUpJ3dLtuFb1HLAbPUGONHjZW+U62S3U2hQKgRAIg6EfNHXt975k6iF/hHBI29Dv0YP6gKe+TPMgT8sbSkJaOu6T2CF7Kgj6ffjdLpypaURCIXyu1lP6/u7mRiKhEN6WZqoO7u+wLpbJhvwnN1NXo0EdIGKJBfVOmboMRuUXgRDmE5JfYpn6yT6vfieWqQtDW/VLWEUGIkSOUs1T79bWtfpPUYALh+MZejxTP44iAKnS1fulFx2l7Sub9KA+QAiHQgS83fu61JeXYrbZSUhLJyEtnXAoeMoDYX8SszrIGavZDLjqj91R2lBRRijgP+Z2vaGlps2J8tCmdR3WxeWXkxz8ZLugHrZok2B0r6lr2yiK6YTklzM3U49aSiTlxjN1NXouqrfnc2kL6qfmfGUwFM/Q+01Tj0a43lS/xKQXAL/7DPsft+OsCuor/rmUfy3+TrcTEDeUHyEtLx8hBAmp6QC4G0+PFt0fxKSX3FhQP8a5+Nwunv3R3WxY1nWmpuOhpVYrHUzMyOLgpvUd1sWy5dBJDgYyGGrL1M1aUO+sd8dsAgAUxXyC8ku4w88zBm89WBLBltIW1KPXSfX2fD3iQd13arJWGQp1zdRPVFPvQ6bubW0X1M/mTF0I8aQQolYIsauH9fOEEC1CiG3RV1cLwX5EjUQo3rqpy/JwKMT+tWtorqmifO/uLvvUHSkmfehwAJxRJ0DXGdxZGuskzR2rVZccq+O3eMtGIqEQZbt3HHW73tJSWw1CMHXR5TRWlNFU1Tb69XRk6hFTtPrF2+brI6WKlOF4pi6E6YT81ENnavWLpx7saWB2tsvUo0H9KMGr3qVdq1OWqYfDECtlPIb3S+XB5g5BuD0dNPW49e6x3z8mvyiKOLuDOvAUcOkxtlktpZwaff3yxJvVM7tWfsirj/yc8j0d7zGlu7YR8Gof2L2ffNxhXfme3QQ8HkZM0SaeiGXqJxLUT7ce31pfi8FoJGPYCBSDAVfD0eWXQ9FsuvrQAcJ9yH5KixpY/8bhLstbaqpJSE1n7LkXdDg+tNPUA5G+DUDa+A/49E+93lwL6tqXP2K0A9oMN+Gg9qQW088VxRL9aTqxksbooKOTfbPqd7z14EgHsyOuqcvoOUQ8x5ZfXKdIU5ehUNzIqy1T79o+KSVv/mEbW5eXdn+c45z5yNsSxGBUcKZZz+7qFynlKqDxWNudKsZfMBdbYlIXGWH/uk+w2B2MnnUe+9d/2mGU5cFNazGazAyfok0zZ09ORjEYcB9nBYyUktd/80ve+8vvj/s8YhRv20xN8aE+79daV0tCWgaKwYAzNe2o8ks4FKJ4+xYS0jKIhELUHO5q69sTBzbUsHV5aZebWHNtDUlZWSRmZJIxbEQHCSbgj8TsWAgF+xAAtz0H27q3Le6ODpq60dr2/lH5R1VjmVf/yC+heKnmAMvUa/fCS7dBuIcblqcB7OlgsneRX6Q/jOwh4NXF5ZdTl6n3Rn4J+sKEQyquhu77h467o7Q1gD3RjNVhOusz9d4wRwixXQjxrhBiYj8ds1tMFivnXHYVxVs3UVuiZZCRcJhDG9dRMH0WE+fNx+9q5cjOrYAWgA9uXMfQyVMxWbUvvqIYcKSkdsnUpZS01NZwcOM6Gsq7TjMXo3TXdg5v2ciRXdv61PZQwM++tWva5AFV5Z0//pbXf/PLHjt4e6K1vpbE6OxIWjVPzzeosl3bCfl9nPelGwGo3FfU7XZqJMKH//hrh5tMffkBAq718ew3/v611SRlDAFg1Mw5VOwrwt3UiBpRCQci2JO07LhPlSKtleBv7n7dnjehtarDIhkMopi0gB1S2gX1qAYcz9RFe/nlOKdIi6jxcrcBF9QPrYDdr0Hzke7Xe+vB0b38AqD2oJnXu2Pyy6nM1GMdpT2XNMYCrqe5+8qdjh2lvZ/5yNsSxJ4UDepnc6beC7YAw6SUU4A/Aa/3tKEQ4mtCiE1CiE0nYgUwddHlmG22eLZeVrQTv8fN6HMvYPiUc7A6nOxdo0kwtSWHcdXXMWrmuR2OkZCajruxnqbqSl544If87X9u5Y83f4G/f/srLHv0If714+9SuX9vl/eWUvLpi88Bmo7t92iPs+FgkG3vv416FFP/1f9+mrd+/wgV0aDaWFmB3+PG3djAmhee6dM1cNXVkpCeCaDV3R8lqB/avB6Txcq48y4iJTsn/v6d2bduDds/eJud/30/vqy+ZCVh32pqj7Rp5qFgAHdTI0lZWQCMnXMhSMn+dZ/EBxw5U7Sg3utOxUgY3DXakPbOBNzwn5tg05MdFrfP1ENK2xRtwXimHpNf2lW/HKf80l5yOd6O0vte38mv3u7+2p8QsRuht5sHainB2xDV1NvLL+2Ceg9Zab1LC5rBsIo/dPIlp+47Srv+v2KVKT0FdVWVRGN5/ImxV9UvrUEtU3caB3emLqVslVK6o7+/A5iEEOk9bPu4lHKGlHJG5zk4+4LV4WTygsvYv3YNG994hc1vv47JamP45GkYjCZGzz6PgxvX0VRVwcGN6xBCoWD67A7HcKal01hVyWtLfklDeRnDp5zDlEWXs+DOb/LFn/0aZ0oqr/3mlzRWdhyccWT7Fir372HkdG1Gn9igpv3r1vDfJ/9K8bbuHd7qSkvY9sHbAJTv3glA5YE9AAyfOp1tH7xN1YF9vTr/SDiEu7mJxGhQT0hL73EAklRVDm1az/Ap52A0m8kZO4FDu7fwh81/iG/j8od4+J0iNr7xCgAV0Y5mVY0Q8Gjnt+/TtinwWqMzFyVnapl6Wl4+6UOHs2/t6ni5nzM5FtR7GQw8tdpzc8jTVUZojmqnno6JgBpqJ78oFpSQ9rQTa0Ob/BLV1EWb/PL6/23ttq+gJ9qbeB3vSNkNxY1sLDkJ08nFzLp83QT1gAsiQU1+MTvayS9t56D2oKs3eALxoOg6BZ2lMtxNUO+mTj2eqbcEevjMd9XUe1un7kiyYHOYB3dQF0IMEdH6ISHErOgxT3qt4PTLr8aZmsaq5/5JybbNjJ41B2P0Cz7tsqswmEw8u/i77FzxPjljx2NPTOqwf0KqZhXQUlPN537wUxbd9R3m3fwVpiy8jKGTJvOFn/wSRVF45dcPEIrOFyql5JOXniMxI5N5t9wJQH1ZCQCV0YBcdaD77P6jf/4Ni91B8pBsyoq0oF61fy9Wh5MrvvNDnCmp/PfJpb06d1d9PUhJYkZbUI8NQFr5zBMsf+Kx+LZVB/fhbmqkYIZ2U8sdOwF8IUpL2tq5+kA9b7+/mrqSw6Tm5lNfdgSfq5W6IyVINQAoHN7yaXz7llqtRj0pa0h82dg5F1K5r4imKq3U0ZmiySG9LmtsL610lmBisoK3k1zWrqQxhAljUNsvrqnLjpm6UMyoMoRUJVWHmqkr7f1IyVh2rijiuOWXZm/o5JQHxoJ6d5l67JrFOkrVMISDHeWXbsoaVVXS4A6Sk6RVFZ0SCSbUtU6do8gvakR2K5Mcj6YeCakEPGFNfnEaCfkjRMJnpgl7b0oanwfWAmOFEOVCiK8IIe4SQtwV3eRaYJcQYjvwR+A6eRJLQ1wrV3LwkkVYgiG++tg/+OY/XuArf/w7C7/27fg2GUOHc/OSP5IxbASepkZGz5rT5TjJWdkAXPL1b5M3flLX9UOyufKexbTW1bDprdcA2LtmJdUH9zPnC9eTnJWN2WaLj1StPqgF9e6kjQPrP6GsaCfnf/lmRk6bSeX+vUTCISr37yV79Fgsdgczr/w8NYcP0Fh59IkxoK1GPZ6pR6t59q/7hM1vL2PHh+/RUK5lt1vefROzzc6omdo1yBkzDtVkJrPeRnN1FaW7dlDX0Mw5LdswOhK5+LavAfDS48/zwr9fJexMQrFMo6WmNN7P0BwdeJSU2T6ox6pgtODvTO1jpu6qbPu9swQTy9Q7Ba0OJY3CjLlTUJed5RehyS+elgBqWOJz9V6KifUNOJItxy2/tPhCtJyEoN7UoH0e/K3dSJqeaH5lT9c0dYCgWwvqRu3rH3YFiYQ6BrAWX4iwKhmZodX/n5JMPdSuo/QoNgHtA7m7GwmmvabeZr179Pf2Rj8LsY5SOHNr1XtT/XK9lDJbSmmSUuZJKf8hpVwqpVwaXf+YlHKilHKKlPJcKeWnxzrmiSAtVkrCYfz79iMUBavTSXLWEIyxO3uUxPQMvnT/rzF99wFGL+hakTnx4gXc9MgfmDh3fpd1MfLGT2L07PPY+MYrNFVV8PFz/2RIwWgmzp2PEIK0/GHUlx4hFAxQd6QYxWCg+uCBLv7m25e/S/KQbCYvWETehEmEgwGO7NxGQ0UZ2WPGATBq1nkAHNy4tks7IuFQh8fMlrrYwJ+oph6tu1/1rydxJKdgNFvY9NZrtNbXsn/dGgrnL8Jij5b8pdrw5o0keBD+8Z2v8tKDP6H+r/cyzFeG45x55I2biGI0Ut5aR1PQgy9/NJ48B1IxsPdTrZ+ita4ao9mCPSk53qaU7FwyhxdwZIc2utQRlV9issUul5c1TUfJjFvbB/VOEkVzKZ7IZwi3dvxmdgjqGLEEmgEIduoo7TD4SAZprdeevHyuti9t2d5G/vnDNT1m4bFA7kyxHFdJoz8UIRBWafGF+r0cNujWbnaupm4mVI9n6lFNHSDoQfrCGFO1p6kDn1Ty/t87lgjHyhlHpmv7nKwBSNtdXq7dehB/RO314KP2wbY7XV2NSJQuMx8d/ZrHatQdSRYsZ3tQH2jsK/mUNRddSPFHxx4Zud8f4td+E283drUbNZktZI0oOOYxLrz+ViLhEM//7F48TY185va7ENGRDRn5w6kvO0Lt4UOokQhj51xIOBrgY/hcrZQV7WTsnAtRFEP8qWDjsldASnJGa4OHEtMzGFIwmgMbOt4TW+tr+fu372Tl00/Elx3atB57UjKJ6bHqF+1nKODnwhtuY+K8BexZ/RFrntc6X8+57Mr4vmWtZUQsNqTDycW3fZ3P//gX+KcsYmfCBNSx52M0m8kcNZagCDM5mIOtNoAxnERyRgF7P/kYKSXNNTUkZWbFR+3FGHvehTRVHUaNtHSRXx4prubefT1XFHUI6p3kF7W+nKbQ93A3Te+wXAaDCIsZNaKiCiPmoJuw6K6kUbvBCMWEqobisyP53G2Zem1JK97WIK0N3U8NGNPRHSkWIiG1z4/mzVGJI6xKvH0p8+wFpugTSsjVTWd5zPclpqkDBD2o/giGBBPCpBBqCdJY1fE7EitnHJmhZfcnS375pMnNmmY35YGgFsCjVS/EM/WeNXXoPqi3n/kItMB+rKAeswjQ5JdoUD9DK2DOuKA+9XNfxRLws6nRTfXbzxFu9wUpKipi1aq2Dr3KgPZPqT0Bt8CU7FymXPJZfK5WJs5bQPbosfF1afnD8LtdHIx6n0y//GrtfffviW9zaPMGpKoyOpqJ2xISyRg6XBs8JQRDRo2Jbztq1nlUH9xPa9THJRTws+zRX+FubGD7h+/ibWnG1VjP4S0bmTRvAYpBm6jBnpSEwWgka+RoJlx4MTMuv5pIJMKeNSsZc+4FcZkG4HDlYRRhQEVh0vxFjJg6narh57MyfS4tqna85KGjAEiViVh9Q0hoHU1K9lSaq6vY8PpLtNRWd9DTY4y/YB6T5t+BUOztql+0/09VIEh1INxzluqqajPq6CS/qI3a3+FgWofnaBkMopjN8fcwh70EhGwX1LUvaqjUT81jWxGqESnbgno4qMafJGKjE32t3X+RY46TbR3AfftMNfuC7X7v32BhCWlPQKqnm64sb0x+SWsnv3hQ/WEUqxHFYUIEI10CWKyccUT6yZVfGqJBuzkU6VinLgTCZOoxU09Is4LoXn5R28kvoH2sjvV05I1m6vZEy9kvvww0TPYExvv8lKbk8593E9n7piYJ+P1+3nzzTVauXEkw6rNdHQ3qdcfhH9Ge8669kXO/cB1zb7qjw/KMocMAKFq1goT0DLJGjiIhPYPKfW1B/cCGT0lIzyCz3VNB3oRCQLMBjskiQDzwH9y4DjUS4f2lf6S25DCjr7mMSCjEtg/eYfdHHyJVlYyZc+P7KYqBK767mCu++yOEopA8JDvejzD98s91aHNFu+H8jY3aY3ujR7tesWzSmqhp9OkiGaMxD6NJwZE6hfEXzGPNC89QX1pCUmZWl+uUkJZOau40hDBhTzQjRFvwqw6E8akqrkjXDFdKqWXqadrNpLP8EmnRssiwzO2QxctgEGEyx9/DHPTg6yaou94pJ1TuhqCCqgZpbTdoJaarx4K6twedPdhOU4e+16o3t+uMbDmK30qfUVVsEc2YTnRX/eKtB6NVy9Ljmbob6QsjrEYUuxFDWCXgDRNp97+JlTPGNPWTJb80BLXr2BgKdxhRCkBPQd0dwp5oxpZg7iFTp0NQV4Q4pqbuaQ2CAHuiqS2o65n6qeGdrevwDKtHSInXUU7zoRIA1q5di8/nQ1VVKiq0wFUZ0L6gdcETyzKsTifnf+kmbAmJHZan5WtB3dvSTPZoTRvPGTOeimimHvR5ObJjK6NnnddBqsiPBvWc6D4xUnNyScsbys4V7/Ovn9zDvk9XccGXb+YR+RzB4Yls++Btdn70Aeb8MVz11F5qXW3BadTMc0lulz1ffOvX+Oy3vk/2qLEd3qOxvu2LHwvqrS43c4wltHq0DDakmBASEqQNqyGZ5CF2/F6VS795DxPnLgDaOpo7E/SFMZoVDEYFk1WrIgiqajwjq4neaJES9r6NDIc4dOmlNK4phYxoW9vLL/4WIgHtSxaWWch2EkNMU491YlrCXvxIfDGpw6XVZAui2V9Y6SC/QJuuHnv87qnzNBTX1KOTcRyjrFFKldq695FRI+/2HaT92lkadKGgvYcx2qfQgdhoUiG6yC+KzQhWI6boR7N9EKt3BzAogpwkGwZFnDT5JZ6phztm6sBRM3Wb04Qz2dKj/KJ0yNRFj6NmY3hbgticJhSD0ia/6Jn6qcFQWUTmlPXkq1UErDWUNHpwu92sXbuWkSNHAlBaqlVLVMXkl+OYkbw32BOTMCVqj7TZURklZ8x43A31tNbXcXjrJiKhUJfqm7yJhdiTkuO17u0ZPfs86ktL8LU0c+X3fszIRRfjCro4NDqIr7WF1rpa6nLPIRhR+eRgzwOOEtLSGX/hxV2W+5v9mINaPXcsqJvd1Yw11hFq0nTtlpZWnNKKAQWn1UhCquaFoSgGFt11N5d/54dMuOgzNDZ+gtvd5qUe8YRIKGlhps1A7Z+3kWJRCAYi1LS7qdbE/heVW+CFGwiveZrQkVJ8FR5IGqq5CbbP1JtLUWVK9A8D4SpNTpBSapmd2RzPmo0RPyFUfNEvo3evVqWTeo3mZEnIgJQhXA1+UoZoT0gxXT0uvxwlUzcY277wx8rUm5rXs3PnN2hq0jq+22fn/RrUo9cqKA1YQi1d17urwRmV36Lyi/R7kMEIitWAalQwRxOOzkE9zWFGUQQJVmMX+SUcUfmwqOaEO33b5Jdwh45SiAb17urU3SGsDhOOboJ6TDtv392jGARqL+SX2Chok9mAwaTgP4ovzkDmjAvq6/KyuYN/kWzU6qwPWhp44onHCYVCTJh1EY6kVA7uO8zLSzZR4dW+oPXHmak3NzezdevWo27jiZa/24cOQVVV/FY73vzRvPyn/+XTF/+FPSmZnLEd5+m0ORP4n8f/RUE3QX3ypZcz6ouXc/v/LWXM7PM50qqVTO6wlpIxfAS2hERKpIOFpn2s3lfTY7u8IS9FDV3LKxW3QkZdA9ZgkMbGRlRVYo0FA48W5JtamkiUWtBLtBujw6ZjlSQK4867CKvDye6i71Nc/Mf4sX076kit9+FUIFjmIt0oCHlDlJe2DeCKZeqy8Qi+yEwCW7Q+kLBbQmI2WJM7aurNpUTiQR3CNZrUEJ/Psp2mbgj7CaLG5Zdw1HrBmp+q7RzS5Bd3c4DM4dpTV1x+iWqqXlf3ATfkj2C2GTBbtX6HYH3FUa3/An7tBunza+feXlPvVykjeq3KZCaOSEu8Tf/ZWMqO8matHDR5qLatWfufyuh1EVYjYYPAEo0CFTVu3tiutbveHSTdqQW5RKupS5tX7qvjzmc28UFRz5/B3hCTX5pCkQ42AXD0TN3ijAX1jjfhWPDuoKmLY3eUelqCOBLbRiWfyf4vZ1xQP880npCwUDImh9xGC1ZPHmo4wOzZs/nHpgZ2tZioqqpgUoOH0mbtMbs7+aW2ooL3X3rpqHMXrlu3jmXLlnHkSA+eGkB9gp+wolJldPOXv/yFDz5eTcSZRGMYhFCYeeXnUZTezzz/evlb3Of5C1UhrbO01FXKEO8QDH4Ds75+B5//yS9JaTlIrqGVPQeKe8yU/rHrH9z4zo18sv4THn30USoqKnB73FhCFtLqGnG0tnK4qIyXf7uZVKFp1tZgM1JKmjwtJEpt0InDasDqNOHzdCzFi0S8BIN1+P1tGn2kOYAK7EiyglFgMyi0rN/C9gd+Ed8mlrWHShtpCD2Arzg6eMhngIRssCV1lF+ajqCSgoiNRYmWI8bmJ9WCeixT9xEmEs+iIz7t/28wWVEcRggaNJ1dQlY8qIcIBSPxG0PPmXoYk8WA2aYFneAbP4Hij7vdFiAQ0P5/Ab/2tNBeU28f4E+YaKZeLIdgIgxBNxFV8rNlu3ly9SFoLmsX1DX5RfVEq4JsRkICTEIggPc2V3D381spqmyl3h0gPUEL6glWYxf73aoW7dq+vLnthl3+3Xuo/d//7VPz28svdM7UjcYuQT0cihAOqlgdJpzJ2sjPcDsLg3im3rmj9BhBvbXBp3W+RrE6z1z/lzMuqE81aV/Gksw8soMeEtwjsQ4v5dJLL2V/jYuyoIOwDBMweKiJtH1gAu2CdyQS4cWnn2bt7t2U7+t5aH5ZmVaC176ipu0YAUJhH2uHVvDOedXsL/qYkQVLueaai5gwYQIiNYPbfvdXZlz5+T6d3ycVnwCwp0HT5Q/sP8D5NedzfvX5VLvKOXKkipRoELb7GzhQ6+72OJuqNjG+fjzL312O2+1mzZo17C3Vnm6c7gBBUz6tnhZqi5tJET5UIEF14XK5CISDJAktANijkoMalh2Gyvt82rXxB9pGgoZbAgQVgcVuwphkwYYk6A/jPk8bmCRUlepoP0e4XjuHYEDT5sNeAzIhB2wphP0NtLZGfd+bS4mIdIxpNhQaCTdpbQhVt2LMm4Uwm9rkl7CfsAwTCURQIyqRgBZ4FMWM4jRDQCBlCJCk5ToxmhR8rmCHGW98PXh0B/0RTFZjfIb5oGqHuv3dbgsQCNZ0uD4tvhBpDjMGRfSr/KJ6taBeIrX+FOltoKrFRzCs4qqvhEigLaibokE9+gSrWA3EfNrMAmrqtAz+X+uPUO8KkO7UbriJVlMX+93aaEfqR3traYiWP3rXrsW7setcB90RCjXR7CnFHe2cjcsvx8jUY74vMfkF6JCtx+Yi7aKpHyWo+90hAp4wyVltRQtWh4mAnqmfGtILUhgSDFImhmE2awMmVlTX4g74KWnw0qRquuERYwteo8AU0jTY9hLM2rVrqY9megd3dD9pRCgUoqqqErshzKFDh6isrOywfufOb7Blx//gUvw0JoQwuvdhsfjIzGqloKAAl8tFzLSstbU1rvN3IeCCsPalCEQCbK7RvGP2Ne2joaEB7xYvPpMPZ9jJ4b+/xeYXn0eEFYzBBPKVVlYf6KqrhyIh/If8jGkZQ9KoJObMPpe9e/eye4/m6VKTsQhVpKEagoRNLgxC0mrJxICkqEi7mSRbEwlKsBoEtljH0Z7V8feIBfVgsC5eDx5pCeCXYLYZMSRZMEciRAwWWsZNwCRVshvqqHH7ottq11+1aH0RUhVEpAOsyZTaq9m0+YuEw24tqCuZKIlWjIYqwq3al/61jdXcd+e3kBF7vNPSEPETkWGQEGwOtHNkNCFsRghEP+5KBGuNmyynEZ87FNfTbYnmHqtfQv4wZqsBSzRTD0g7tPTwPwWC8UxdC+rNvhBJdhOJVmO/BvWAS/t8N1nzAXA311FSrwVnNTYSN1nr0MdgBKMVNRqghdWIP6wFO7OApibtKej1rRXUuQNkxOQXm7GL/W5tawCzQSGsSt7YXonq8RBpaSFU1dFJsycOHPg1H2/9RvzveEepubOm3imoRwNtx6Depqu3aertql8UwdES9eZa7XolZ3YM6j49Uz81CEUw1mynnGHYs7VAK/wp/GXrU0TUCOeE7TikhQM2LXiktmiZbEyCaWxsZOXKlQxtdZHc1ERxWfcDYqqrq1FVySWRD7GYTXz88ccUFRXx+FOP84c//i+1datpaVyPkJKh3hScVk2P9rj3xTtsDx/WDKOWLVvGU089RUtLNx1Z/1gE7/0YgK21W/FH/ChCYW/dXl588UUiRJg061MShm6gNC2b2qxMUhudmIMpJCtuat58C9nJGXJ3/W6GNQ+j1lqLITXM8NUGBILincVINYDHMZL0eu2xOWDVbgqWIVpw3bp9OwApjiT8qsQMWKMZm//l70ONptP7/G3XzR+VGCItQXyq1IJ6sgVzRCVitFKfkEgWkvTmRqo92hco7I5+6Wz5CEu0usUD2JJxm31IGcbrLYbmI6gyGUOCGZO5kbDXhhqI8HEkwsdZJuobEwlER2kaIkFUqf2f/TVepBICDJTtbqH4QDNq1N3YYIrg+28ZIw1Cy9RbtaCQnuvA5+p+xGfQH8FsNWIwKSiKSkjaoaW8y3Yx2jJ17dq0eEMk20wk2Uy09KN1rz8a1NVUrWS2taGG4gbtKSjBF01EUoa17WB2xCfIUGxGfNFxHnarERGQfP6cXLzBCKGIjGvqCVZTl+qXWpef0VlOJuYk8sqW8ngwD9fW9moKOrdnH/XBtuqtxp46Sjtn6rGg7uw+qKtx+aVtHyGOXv3SXBMN6u0zdaeuqZ9SCockUUUugbERBCoLi1LY8c6fcIx6hLlGH9lGA01DtUfjghrtH1UXDOH1ennxxRdRFIVpG9aTVVNDhdsdr2tvT1lZGSaTj9S0cmYXpLJv3z5efPFFDpYdRE0uw2CIYFCCTAlmM7MlD4cz2snYupeUlBRSU1M5vHkFNZ88x6FDh1BVlbVrO1kAtJRD7W4o3wDAp5WfYlSMXJx/McH9QWpqatiZtZ4cQyVj7GXYPR4QRlQ5AXMgBYRg5uZlNH+4osNhP93+KfaIHU+2B3OpikO1MiZ9OEgw+T1EjA5SQ9oXP2CtI6IaGDp8GF5poqaqEgEkOhLxqxJTRI3X7frURGg4AIDH3ZalBgJVSFUSaQngCamYbUYiya0YJUQMVupMFrJMBlJbmqkKetm0+UuEAlo/g1AM7D7v15TlziVU1wi2FNyW6IAg72FkUxmRsANDggmjzYUatuLdUkOdWbspFPss+Jv9RFARyHh2HqjzIZUwijBRdagZf0Qifdo+KWlGpC+MHU1T97YEsQkokJJISO3WAz4UiMQ7SS2GAAH1GEE9lqkHqrRRuL4gyXYzSTYT5ZEwfy8/fuvp9oTdDfikmcQMTWLxNNVSEpW28kTUNiApv20HswM1oGkUvh1bcLu0gGY2C+wSrp81lDlDEvkVNrKj3jCa/NLxRlTnDpCZYOEL5+Sxs9rF4d1RD34pCdd2Y1fQDiklXm8JrWhSaq7FRHMorHXytpdfjMYuhl4xndvqMOGIVqu0H4Aku+soNYijWu8213gRiiAh3crjZbXM3bCXxgSFgCfUt5m7BghnZFCfkOokLAyUpSTgEIdJbc7g+++YMLozmIiVzIlvYczRPlhj6rR//Ltv7eXxpX+nrq6OaxYswFrfQFZ1DSptJZBlZWV4o5UB5aVHGDN8AwcmWpieXIpphIlPsz7lw2Ef0pjWphvOiqRjDOVit2lVGS2tmnwxMjeDkjoXa1avxmQyMWbMGDZv3hw/PgAlmn5O3X6IhFlXuY6pGVOZYJxAXkMeIyaMQEnUOiIdFj95+1dS6piOUZoxhhIQGKgaMoQjn27scH0q9lQQMAWYOG4iaY2aHDVBal96p1f7sEcr+lANQYjYuHHnV2hVtYw8QdgRFhMBCYaw2ia/qInQqD19uFtLiIS0jiWXq1zz5I5IvGGJwX6AHcYb8GRuxWSyUhWKMMRuI621mToUWlo247EEaY0G4ASzg8bU8YSrq4lYnPis2sfS27oHNSBBKihOM0ZndDDRynLqbdp5lNsVnFUewkSzz+gxg41uAgmlGAw26src2rlER8ymJWn7WsIqflcAb2uQPItCUrWHRKX7AUhBXxhTVE83KT6C0tZjUJdSEgzWIoSZSMRDJOKmxadl6ok2E4dscN+BCmrrfbz3+E4CJzAYKeJppAUH2dla34TfVc+RBg9GRZAn6ghY0uJVLwCYndo1BSp//AOaK7QnNUWATQrGDUngzqEZzMXEsGotWCbajLgDYcLtBifVtgbISLAwd1IWgblD+Gd121PosSSYYLCOSMSDKxrUR9mtNEc7O7tk6sEQUlWJRJ9y28svFrsRo0npJL9oP9tr6ooijlp62VzrJTHdisGg8Emzm30ePz92eClPMcRtnM8kzsigPs6hBZNyhpI1cw2RrCSs7gBvbizHUPABhuQKGlRtVORQq/YYfChSQmtrCzfeeCP5oRBhgwWjyMJi8FJS8m3271/JP/7xD1588UWklFSUHyY5XQuoH7tW8QIvMHfaXL4z/Ts41VoMplSCqkKSswm706PVxTYVINQ6QqEWRnq3EcTCTv8Qpk4uZMGCBYRCIdavb5v2jRJNo5aRAK+99i/yd+QzsXEirVtb8Rl81OfXk2/WPqUySWXtBA9N0dGQVrsZhyGNiuwcvLv3UF1dTVNTE/X19ZiaTJiy4ZwyI2M92qN3UpWRupG1DKnVAnd2dgLm6BOKM2wmuXE7qWidbokRG97SI/hVifBHsDi0YOZXE6BR87Xx+crw1WsjQLdW1vLfSm1fnyoJWV4EVPyJh7EajNQEQ2QnOEh3teITFnxY8Th9NAQVAr4W8oxNuJ15hKpr8Foi8SJjT0tRvEbdkGjGlKR9MSMtAeqigb8i00B6SwAZHYBjM0YAlSrDb/GmFTFs2P9QX+YieVgCQtUCRkI0xgnAmrQOt/p3ki1awE8wiPiApLond+H+pAI89QT9IUyxTF00oY48SMhb0+0UcuFwK6oaICFBq4/3+ytp9oZItJlItpvxRIuhPtldw6EtdVQeaO7mU947pK+ZZukkPzcPgJCrgeJ6D9OHpZAn6mlwZeBZv6FtB7MDNRgNeD437qgCIpEkKQYSrCamJ2qVT5lVmoSZYNWumztqlRBRJfXuAJkJVoqCQTApbGs3MjoW1EOqZHVjVxM3r1f7DLkVrX5+lN1CS0QlIkSHEaXCZCLS2krZ1+/iwLyLCZZXtJNfjAghutSqd1v9Io5e/dJc44tLL6W+IJMTbNgVhWfnJVLe0r0X0EDmjAzqo+wWjAIq3BdjGv4JqRe+h3+GoGr0aBpGvomyK5em2izs0k2rWIs5HMJvtZLcMoVh+cMJHjpEaf4Cdo79GsOS9iGU/ezc+WuMRiMlJSX8d8NGNuYm8VPTEr7LX9giXJyTeQ4/mf0TLsq7iOEWFY9hCGVBQVJyKfkpmnaZVXkRAK669YwoeR6QgOTckYlkZmYybtw41q9fz/aD27n2jWtZXbGa1pQxrFDOZfuuEiIiguegB3ejm63pW1leuZw8U1t2VD5BkhwKgCIYOjENUyCZgMPOJ8OHsHTpUv7whz/wt7/9DRWVS97dSd6vXiE1kkRzazGGiCQcdKFEMnGYAjhG5uN0aV84azABgAK0J5ZE1YZrz278EoQqMUuJQNWCepNWRhmKVBBszSbsT+Af3ly+G/VRDzqqCKLdrAIJ5QgzuCMqWUYD6dHA2UwKwYQa/KqKqCvCaXQSsKTgqWzAY9CeZCymdLzew0RkMgAGpxlDigUIERbQaNS+tJVpBowSUqNSgU0JkznlZVzOlWRWfon0pJvwuUIk5icgopm63dQmrySOWAGJ/8Gaoj2BOKM6u+oPE9jfhHdXPeob3yUclFrlS8iHY8QnmCato3iYraNlcJRAMGqNnDgZAK+vCpc/TLLdRJLNiD86hHN3VPtuqOxqONdbhL+ZFhzkpyXQKu1E3PWUNfqYOjSZoYZ6mteHqVy8uG0HswMZEqCoSCQ+LKiKQFVVHDJqUxtNHEJlLsItARKjTyixztIGTwBVQmaihVVR582D6emIdC2RClVp/QjLapv44vZDHPB0nEs0FtRVxzkoMkKuWTuu2+7okKljMhLYvx/Pp58iQyEan/wHfncIo8WA0aT9L50pFtxNbcdXuxl8pFW/dH/9pCppqfWSnGlHSkmpP8isJAd/GJZDyCTY0s3MSwOdMzKomxWFkTYrxSmzEbtvwDBkJ413BHBftAGjZwir10wjnDmbVBqZlHkIRW3BnWLF4HNSc6SVwMFD1GdOASB5iNbhl5R8mEtlEokGM3fX+Vkx5AJcJFEnskg2DuMnU7+NEIJU1UK6UbKxuYGSoMBpC5DvPYgSSMBRrw3/f2TdD3jXFqEgJ5XJ7CHNrZVNXnzxxRgMBl7712sk7UviX/58Fk78Bb8v/DLBxHo2jdjE9773Pb72ta+hpqlUuCsYZoZInTZ4Jt0GmWED9kwrKRkmjM1JKFJgD4Q479wpLFi0AMcQOymVu0nbeYikW76lXa/dbwIwuiYNjz2XFGcQc7oNnzCyLysfEcwkJM3METswmKxkqUkEgiEC0S+IWluJVXHhk0nQWEIwWAciiJDZqIF0SmQijVKl1QjW0R8gIpDcbCDgLMdvj2bA2xtIyx8BQAtpBJxlCOmCSD2KsOFQoLEhjIcWhJSk2ybjDdcRFpqtsJJgQjhSMYpKGtPaBomUmqPHjwV1Y4ikEWtw1p/LkOCN1JVpQScxPwEhteCU4AAUkKgYk7UbmWfk69o6ReBtDRKq1gJtqNxNsHibdh2NESJNh7GM3oSUgoocK/6GrtVTwUA0qCdoQb3Zrck0mvxiJBK9uR3yahlm4wkEdWOwmRbpIMluwqUkEHDVE4yojEi1kSPrUOuDhKuqCDdFR+managhI8ggYaMNqRhRTQKhSoxhqd2w633RGyj4dzeQaNMCbayztC5azpiZYOHjRhcGVeK1WimbMRtDUhKhKu1GdzB6fqX+joHR6ytGUcwELKNJoBVrSBsH4rI7OwR1Q1ISisNB/tKlJF9zDc0vv4KvwYXV0aa7Jw9x0FjlbTfvb9dMXTF5UOm+9NfdHCAcUknOstMYiuCJqAy1mjlvWCp2g8K2QPeTWw9kzsigDjDeaaXeIPldyXnYV9+Pf/O3CS+/j/LXv84L+RdSaU8h0+fDmXOYxGAdNLUCksr9TdTsP4zbnovJWYMlvYXysglIKWDoNqoyz6XRkci3I49ye9mHAATsiQxt8lJ5oJk3/vIfALa0NHMkqFCjZLF8zHRsrSPAn4wI2UkwBPh9RgbX3PYVrnFshwqtTNFmq2XanPcoSd9BrjeHoaHxlCpZ1CUm83bqKm6ccCMJzgRycnIYmzoWi5CkmyI01lyAGjYzzGglM+gkL7GWpLU/IFfYuT1wMZf7JvHv1ffx7QPfJrjhWS5dVUTGD+/FWjALvwhxQB7G76tlmnc8Xnsmaf4NmLf8mvennsdH46YTUGw0Z3+OCcoRRmVeRIE6hACCqPRKpOoIVuHCbxkGreX43FqnmM2SR1DJo1bRJJLDSS6cw9aSFJpMYr2ZsK0Br1MLBCllHpwO7bE8EJpAIKEMo6ccY1RSSTEImt0GPGodNl8EJ2moIkwgSZMVDAlmcKSRZHwK/9xkANKbGylF0JS6m0SD9lFOT6jHYPZhr5uIMdlCfZn2ZU4tSIrLL/UNLZiyHAQT6hEmH8GWbLzpO/AlHo5n6qGoFa0MqQS82k3VHKyhouI5hNVH045bkEBJzb+7fDYD0aCekDgJELg8WpBLtptRrEYwaAGnjDBjLApNld0HnN5gDrbiEglYjAa8hqT4RCKj7F6ET8UQlUwCe6MzXZkdqGETMuglaNGGQ4eIYEUgJAS8IUL1LtyTNmDINOPbVU9CLFOPBvVaVwCDGkHdvYlSf5BpqrZ+29iJGHNyCFdq/+dinxbUY8Z6MbzeEmy2YbhIIlG4MPq0ooZWp7NDnfqQ++5j5Lvv4LzwAtLu/AoyHKZ1f0m84x4gLcdB0BfG3aS9Vyy4t9fUkyb8BlP27+J/q2qYhoZV0YnmY+WMtvjNZ6jVglERnJNgZ2NL1xtuIFCDx3PwmP+b08UZG9THOayUB0I4chxsqc5jQ/EkDjYNY+gXpuI12agJRcj2mIiYvYwMtRBS7Dj9NezbWUZlg5bppY/ZgJSC4cZzsVbN4mDuXl4sSGKy+yDnKmvx1mjafY0hg9X/3cU7S3dgST6EVBXKQwruesGbXMOfjffQ5JvIBlGF1ZXPTAw0EeHZPc8i8qZDxRYADhb/CREsYWxBDfdObaJm9BakMFAtshltiHDrhFvj5zc2dSy5UT091DicYGs2oyPJWFUL24Mf8mSam3SjQCAwpo/lm+IW7hhzEwvXB6mZOYK022/HU9LCPlM9G8YGUGr2URAeBUIhQ27hiEVSNGI0AJVpJlzDb6A1nIF5u5aZBBNTCYV8bBi+h99512FVWvGbckGqeBu2AeBsraPe3zYKb3/uYRRDmKyxv8SYfAUALSlaB3JacwiHXQssLd58ImY35qYNmPKTEVYjQ0zQEknAHarA4YlgVzWNc3dgDBgFwmIAexo2w0aakrQv8OQDe2mQUDLjMRIztcCQlqoFFHvjSMoiYdZuriIxw4Y11QZR+UV1ezGkWglkaX0mgW03owQdNE16C6eiDUAKVXvi346AqhmNGXyHOdL8NqI+nZbSC8mt8lMZ2IzP17FePSa/WC05WMyZeKOlhUk2E+HomHyrELQmGBhvM2Cq96F2417ZG6zhVvxGrcMxaE4mCe3JZISxgUBzW/Dz72kX1CNmVE8zyhTNpkJ6qxiqaMHNV+vDlbiBUtv/4S/cTaC4haSoLNPqCyNlhPqWRi6o3MHODz8AYL7VSk5tNeuz8jANGUKoWpNfSqJBPWasF8PrLcFuH0FjWJJikAifNr1jq93ZoU7dkJiIKTNTC9RZuSRedhm+2hYslraAnZarFQLEnnZiMkssU/f7KzElHEBx7Iq7dtbWvs227bdT37CC5lpNM0/OslPq19o71KbFh1nJDna7fbjDbXKdy7WH9RuuYNPma7VxFAOQMzaoj3donTmBScm8OdNOXa6VUbOymPyZcUgBrarKsIRc0g59jsTkRuozk0hTNpBRGSJh2AKspiApI7fgqxvNiJJ8cg5fw9PK7aAE+Yrjt+DJYOgwCylGqCaHOn81qoRIxkH8TUOxh9KYs13lcESrD15pmMxL2a9hcedhtqksyP8MT+9+msYh46HhAJsOvUZD/X+JSDjXWI/j0AqynZp9rV/YuFoxY0eBfe/C/45nrD2X/Kiefm54JIneNJKsWgXAfx0HWZHiJt2oEFDLCLWUkBWYyvV7Ckme/FUmzrwb6Qsjqz0UCS/rx0Gk4SBGYSZBgSzHYZ4acR2mcAhFVSlPN9LKMA4ELsAc/TJUWEcgpZenCwz8yXgJ4axqfFILHq76vUgpSK7ZRq1Ijv9PipPDBFpySEoZhbFS+3K2JGpfmvSAipK4G7MapM6XiQ8r/zp/DFVjxmKbmEaWyYDbkY4vUIXDG6EqVgbvqCdsULTBJHZNijnY1AxA4SGt0qiWIRgzdgGCxJRqIiErZk82q2taaanykJxjRxgEIUVrkyMSQiRbCKaVgWrA1lJA6pFFuJxbCDurCDf5CVV7UbMdCEOQEJqbZkB8SFC6sRwYQTAgGVZvQ0jYsfN/qGsuj5t2BQO1GAxOjEYHFusQgtGadbP/Gfyq1lE+MaRQ7VCIAEkC9t51b8cOzd4Q8mOWfoJmLeMOW1JIwY3VpJAaqsIfDeoiMQn/3qgdtNmBGrYi/S6UyVpQV1qKcAgnRgG+Sg/eFE0u9KTsBAmJ5VrAdPlDHDr8OxLdNzGupYRN4yeRHfQzXfUw9UAR2xJTULKzaRp+mC1bb6YkaodQ1S5TlzKCz3cEu20EDcEwaRYrileTsFwOJ6a8vC6nuevjCp760SdYr7+DoGJDqW6z7UjN0UbJNlRoAbZznXpd/X+jfwdpbtJuHg2Na7R2Vb5Mc40Xo1nBkWSh1BfL1KNBPcmBCmxp1W54zc2b2LL1Bu1ah11UVb/a63/VqeSMDeoTnFqGuM6ssn24hTdmO8i9chhJNhOJyRYkMGLiCDJrryK/YhJexYnxxvdInf4vska7KJj2CapSTmvpLMJNKayzW9kqZvClonfJ2Ho5w7fdwxfsVzDKbqda5pKctJ1wfjHJKSX4Gkfy05qvcGn4ZsoMWnXJx85MHgxtQrhykMYQXxv7Rabvu4L7Nnr4Qs4QXt75YwSQefAzqIYwxSlNNBgz4ucTTMyB+v3w0a/AVUmhq5EJmDD4U7CFk0n0DsNka0ExuXm1fiPL5z2O0yBotDkxVT+JUKz499kwpo0kUGmj5nfrESrslkYq0qAiOlw906CiJBt4IWsRc0KfkE8J1bkeWprCFAUvxJmzmojJjSdiwDGyiMOGPKRQ2DsW/H4t021tLSPsSyZVVFKdez4W6SMrEKTcYsQQmY4zxYLc34AImWlwhLGHw1gSmlAtraQGfTTIDD7gMl4953JuySqgbHwiRiFIyFOQqPwm8btc4bwcXygBg6MCbywo2LWOuJUldRilpMCmBZ/q0HD8GbsQCZnYnRX4m4cjUDjQ7CVFVXDbtI+5K6qpG0WYskiEoLMEiyuPRMVEYv3FgEJr9lpoDhCq9vB2TQuqKEZVtCcar2UX9ogdmzcDqYLBNoLChuG0uo+w8tNruPeFZYCWqVss2v/WYskmEqrGavATaHqShoCWMc8u9RM0CMrSTKQaBHV7Kmh57bX45yEQjtDsDVJ9uIVn7/uUjW8Xd52YI+qREzbHXOVSSRZuhqc5EM2lBJqN1NqSCY6bSGCPFtRdETORiB0Z8hFOywFA1O9GIEg1CIK1Hnwp2lNPk389SqIJU7RfwuXzUFHxPEaaGOcsZuvYicw4vJ+cQAtT9+/BbTBSPHwknkleypp20BLNcMvaWUT7/RVIGcJuH0FDKMwQWyoOqen93hEjsU2d2uW7vntNJaFAhD2HTYQdyagHighFR3jHRpY2RCWsziNK6+s+xKBon5vivdrMXbXVq5FSUFf3X5obqkjKtCMUQZk/SKrJgNOofc6nJzpQgOWlq1i/4XI2b/kyJlMyM2e8TmLiVMrKno5bKw8kztigPtRm4fnJI3lr6igKP6pHkfD9IxWEVElm1Ng/x2El+8dzKRijdWAGy+fSnPMJZTOXEB7+bzyRJFKrpxC2JLNkfDLDa2oYXTyf0qIxyD2luD5pJK/OT1UkG0fyLiakPoSMCDLK51PYOoKKggtRhYEpwQOUJCcTcicjXNpdfv9bO5lYcx6Fhy9hbMtI5tsgoW4qacU3YSaLkmF2qkUuBrQP4QH7UMofuJudLX5uGLcU0yY3k8xJWFqH4Ra7sLi1m8eQzK3YssdjcucQEvCvkbm8a8/C9+kf8a7+DWb5FGmJj6FGR242O/N4/9oPWJ8xGnfQz3Cz5C+Fn8eLkUsj7zDWvZ+yJDst4iEyLvkzRbM2cGDqY9jTBIzfSp3Qnia2puTg9wWQRgfeUB0hTzrJlnqKTbnkUUaOWk2NGEL2zmoCe/YQKi7B7E6j3iLI8EvCuVowy/BDjc3Be1zFGM8BjCYT1zXV8dEQI6mptbzKl1iWugif2UCxaww4qnAHwdXUyt6GFaybnoxMriY5HMB8vva4XNZwAUFnBeZLz0dJaqGxRavJH1qlBZWSSJhQRKUhZuWuhNna6sFrOIDVNZxMo8CUkE1K0hxc2Wuxe4JEgn4yp/8C17CXIZyMyeTF62gm3WPFHHtKtI/At7+ah9Z+E7MS5MLU39O8rRZ3STFmgxbUrZYhCLWWqZk7kTJAnUzEHAwyvU7LCj8RlSQbBR5HNp7165GhAETC/Oa9fSz6/Sp2ra7A1Rhgw5vF/Otna6nd30T9M0W411bGHRqlNVkbUetII0H4KEg1E66ux+9N5VBSLk3ZwwkcLkb1+1l+yI0kGtRtKSBVQhX7UYFUoyDYVE/AWY7NNoxgsAY5uhn1iBbUReAjwmHtabFhQiJuu5Opa1eRWFvO1P3aSOONmdmEhkpq0LxojGHJkda2oB6rfClzp9McjpDtzCYxWo4amjmryxSJDRVuGsrdWB0mdq+uICTNmMIe6v74p/g2abmONvklpqkbBOGwi6bm9eTkXkPIM4T6uvW4Ww+hUkfLoc+AiOD2v09ypoWDB5ewv/kI+da2TvgEo4ECs4e1jXVEhJN/JT7B4fxnsNlyyc+/DZ+vhIaGnk3dThdnbFAHuDgtkRkpTr7xhQl8LyeD7S4fX91dTF2ulsXnWEwIRZBboHV05V36a6pfvpvgJ9/hnYrH+brhH7wwPZ8HRxupdTi5tMiAUVG4+lYTFsMnhEo/JfeIjyZTKu50K5E8laFrrmRUKJf9Xj8r3doHed4nSSiRCC+lXEJQmBFhCwl5TzJ/9CEuSw/yheTRKKYQltLzaQ6D49A8AOqN4xlvNmENB6kSudR4K/lryi9YkTWe14z5hOzV1NcPJXNaPfi0mt4ZGUspHTeSLbWf5/3RpfxnqIHXZ13PJyO+wMb8K3Bf9SB8+Wf4jU9RnfwMd6Q/ia1+HQ1TzuNwQMWU3MAb5imMNdZznvuzTH/tIH5hpyGzgoPeaTwoHuKVlKlMvObvHEnQPh45fhc7lEJsWdtojIwkKJsJu1NxDB/FgYCRfI6QYd5HNdkor26j5LrrCR45gjU4hAaTlYyAwG/bCKqZTL+NvSlGWkUiXw7/h2XnjCbFZOTeKTa+OfkCXhVfZtIRPwY1wp7AOMIJdfht1WzeehkVVf+LokpchkSSjUdISCsnORKgPKIF8ZbZVWCApKpkJGCNaO3f0ORif42LpujfUomwofEwEVxYW4dhUQRKphWPcSEhWz0Jzv00D/0vyUll1A49hDQESM/ZjVQgtaQcc4KWNNSoQ0gN1XDdeQsZ1yDISayi5YP/EKIeqq1IVWKxZqPgZ17eJ5jNmdSRRa7aRK5bu+FYEw9gEgI1pYBwVRWh35wHL93Kf/fUUNsS4ODWOkZNz+QLP5yOSYWWZ4vwFzXgXlMBviaCqhVZOYZXf7gG1TSESEAw0RmiZtt8TDMeJJQzlfK0PKQaZv1HH7K5MgSKnYARfC4fFulCbQ3hTjSSZhT4wztASEaO+A4Avpy9qO4Q40xGkuXb2GzDafRk88SMW0jz+5i1YwsHl+/FEnGSEJastZrABJWh4QAUeCW1kYjmfy8l1a8/DsBf1mnn71SMOCuTcUg3vqi9Rnv2ra9GKILL7ppEODovbPK0CbQsW4Z/7148nkMkDf8IkfQfDh58lLqm/+DI2gWoNDR8jJQhMjMWYDVNRViL2LFWqwSbPOsbRLwFJAxdg23Y3zhS+jgl3lZyjG01780tmxkeXMNhZQJP25bwriuVxyu0m0dmxqVYLEMoLfsHUvbvnLMnyhkd1GNcPS2X707M47ohqSxvaCVoUjBUe8kza4/bGdGf9QYjE8Zkc8iWz7/z0pjYovJxppHlw2ycc9DHcJ+JK789leSLvkDu3/+DYt7LkB3vA1BNNhn7v0xS4BLqi9ewJ2hg++ipOPwqLwjJzANFvDX8MzTPOI/cjT/EqJr5zjSFey/eS3PB26i14/m3exo3LUhErb6IiCuTEnc6HPCQ1hSkWs3FvUiwc5iWqbxXUI0aMtNcMoLk8ZNYFUpFhC2U5Vk5oK4hRAsrh2uPoKGsBPzWJPyJQ/jwrd0se+spDs7ZTcusFTiGf8KWw/cyK3MlR8KC8lGvcoQRLBgykrIp1+Nya5nJJ9t/wuue2wHYGbiIltZ1lAhNdrhtTysekUDDuCPsOaSgOL3IJhvFyuU0hVWGUU6WUoZXODDd/QPs584GVcVhLaBBJJMqW6hP2kZ16yTSo9+ZCa4Sxji3EWzx8vGscfwh0U6WqGRWcAs3bjnIpOr97LIVErY1EprzW8JhN+tbHmDUlgjN3jycHkGgOp+ERkGtOQHFn0yjTauPH12RAEozo6yrSTJU0tqwj1X762mOVmlIJUSjqj05WFqHA7DVH+SrL9oRETOeYctpGPkGfo+DiEmlNftTEnK3YwyCo3EYfpmGWcDhzS68u8xc711BVqnWOdjqqCJsaUbU2XGvLsdq1jLW0SmHGTLkczQoeeQYyml2qyQFvBywa6WDluxMJBLP3krY+xZJjTvIDytE/GGGTgmQmWPjAofEFIxgHJ9KuMFPze4W/tPwO0a4EpltgJaiLPZ+kMWsg+/SnLWZoK2ay4fOxRbJpemrYdzGe0hLU0AYaDUZeVm0UJcaxPXZMK7RL5BiEARNOxCqkRTOwW4fxRtqK/sSFC5OqCPRsIfc3OtY5ruGI9YR/CrDStiWx1bbxWya/iOGVQTZGLagolDTeiEAM5Ja8JsExYeacS1fTuuRDQgfWDZoFWEl60sp3T8DB25qO03yIVXJ/g01DJ2YSs7oFAqmaU8/qQsuwpCYyJGPHmXDxqsIOR4jbcLrHCn9G5V1D5M/9w9Utt5BecVzmEypJCVNI2/4+RjMXlr8LxMJZFAweTJjJt6INbmcsOl98vK+Qj0Z2Nwfo6ohfL5ydu/6LhONtXilmWV1zUxy2tjv9VPuD6IoJvLzbqWpaS2r18ymqOjeuDXE6easCOox/m9cPqVzp/D7zCxM25tYtkULehnRHvX6YBjHbYtYOnsUo4wmnmwysXTvYRZtdnPtuiKue+A8MoZqA3EUi4Whj/+N2ffcDMB7e+7C6akgz/5FHCNfACQVqSYMrSHmT8zis65GqhIzcBWM4p6mFMIvT2A701itzKPB8780fnozn45yU2MRfJroIOWDH9BisTIzFGFE0Ewlw3HlGjlkykKREbZGzmXvW/9LhqueMudEXg6GsXhy8NkV0pI/w8iV/8dOeT4A1ZlBRl7zA0Ze92NGXbGY9MJlNFrn8K8d36N05Y9oLj6flDHLmTb7R+zK8hIRRmYYE2j0BHhryGwS3V525CSxO99Oij/MQWsW9qZsal15DG11s6DeiSIj7E9LIXBbBWrIQm7LOLbUaLLWCNHKEKJuhNdcS/7SpQz7979xTbqMBpFBVtIHKFJlw9ZZZPi1bOuGXWtQFMm/PvoT7tZtDA19i5+In/KLXY1cnNbC1JJmDiYX4BU2MPgo++i7vFWUS5MYToMliaQKO5mPClLdKk1OA7KpEISKCCXhVIegBKrZ4qjmyxnf5+6Slyj83s2cW6LZO6jSw1cbXgRVYHTnArCxrgJ/xIrSNB3XkA2oRh/Og5nY3FA/dDmGrB046uZQH/wtqZVTuCzJRMphhfrdCdQsWcrbFf9LyDWE1pwNSEMIm0mh5f0SIuvbhsxnqCOoU5NIM5ZQbqxjhKeKTY5CWpK3sufy56n7khGvOomAKYm7ja8x22wjdeIbVLR+kcPPX4UpKNjkCVFySCu13bncSEQ1MSxJ04DNxnqqfx2mZcZ/qC58gtK5D1M0/APy857DP1UihMqsCSUAfJqdwivDklkzw4T7chV32lv40nfhT9qPuTqFw/MuoXlvKr8JX8Ut59o5MN7NETmCT8UiPkqbyxy5hql79lE/6VqMITdD6laSXRPEZzBzpHkUYUMqaTSQ79CcQdduLKfq979GHWXF6E1kbplmbaFsb6au+WKcuKmPTZIdpeJAM57mAGNnaTfGGZ8djsVuJL0gHcPSz1E+9iMSnOMZO/QN9r78V/ITPmZM3vtUrruTiGyguXkD6WkXI4SB3GHaTcaSWE1y0rkoikJu3uewWvMYPuwbOPO/TxgTScEiNm+5jrXr5hMMNfL58bdgFHBLThp/nqBJoB81ahVdzSk38GTiv5BJl1JT+w47d30jXmFzOjmrgroQAoMQXDIhiwtGpfOrt/dwuM5Nhilq11rdxIJ1+6gPhvnZ8GxybyvkrapW7nnmBxzyHKBF1R6j1h1uYOnHh3h5Rw3P7nSBlBQl5mD/6t8Qd37AkuH/S1mKQkOiAW9LgEsnDuHSIakYw2E+ff8d7l7zR3ZKkELBKODd4VNIuG8RB/PzEarKuiQf3pwhSCEoCBqY4JPUKcnU7XgMVRi4+qMP8BtNFKfZGJ1zkFU1FvajktByEYk155Hx39spttloUOxktAapFUP4oH4u9tQvsDPzT3zH9Ar3iLt4t/A8ht94NVmJP8LmS8YzvJWDqjZ03fnWEXZuq6bamc5kr5vSTBMGKfneLi2V/nTss+y1TWKaazMp7vVMaQ6zIziPhI3fIX/Fn8gNncu+kKaPjmhsC+rFoQhCUbCfM401QpNFZho/wrpnJIWNeVxcFuCnu/3MrfAhsXFR1tNs2XItXn8d8q1FOItHYvBMYI5pCqpQKG69hMT3LsHbNIzJNRFWub+F36yQ0lyJM9TAWLUcl13B7ZoEQIIvBWP6aDwlR7AetlF3cAxjtpZRY03Eb9Cy4oYxb/DO+fPZ3jCdiF/reziv5S8A1NbOBsBYPhN71TCUhqlEnFVIkw9rtZPQjt/Tuvkp/KpEHXclSecKtmfcSEs4C3PNVPxJ2jVJ9ryDdVQCga1adql4HLS+/msiQiGDWkTWDpzeFhqcTh6Z4efXhgd4fO7NtFidrLbdwWeUbUyy7Cdj/NuYQipHhh3AVbACR0Y9OypH4An6SRNGpu5/E0dQEExqwnfOExg8CeRu+R5Dnr8YU7EJw5h/05q7huS9C7FuSSVg2k7QVseqKZqctz81HeMBBWODmX9NLubO5B9jTzyPhNvuZE3jPADGsoe3nOfyE+VR7jnYSKLLzS3ySZp9G8lLHEZu5WpSzJVs9Gra+wFvIYfCBvJMEUzulQDUpf+C8h+U4c/z4s06l605miuoOWQgPTMVW1ClIaD9L76yq5j/21fJ+mWHMVkNDJ8SnQg9L4Gv/O+FREwrKGn6O1mZVzBt2nMMGTYOgYmmKj9GJZ3W0tkMS3+RUaN+zIgR3+KRd/fy2w9bMUZtCfJHatM8mkzJnDdnJQUF36cs2hk/JnkkbvcecnNvYM65HzImfQob50xgyZg8xtgt5FpMfNSgneevDlfzX5eNn/luInv0o7S0bOHAwV8fV+zqT86qoB5DUQSPfnEKZqPCPS9uR5GQZDSwusVNJKySuLuZP72+m6c/LeHtSDqHv3Ajb46Yw49e2cFv39/LdY+v45F393Lvyzt4eUMZiVJwwdRshiRZaciaxvD8HF5M1TJOqyfMRWMyyFm4gFnVZXyUPYxUh5mnLryaJIPC3cOyeLe+hXv3lqF4QmQermJNbgZH7Nqln/q5MYwdm44UgneHOFGk5M6927GGA8jRaxk21sjKvbXkJFkZc8G3KTD+DEtmEhunaBUPP5+sjdJ8se6LVCr/w58b88gwWrDua8EI/DPg5oIvTmTSRW/R2pzKSvcFJHrC5FV4uetQgHdEAtOatGNdVquysCFCuir4/YE6qozJTLG6yR7xLBfnpFNsSaMxPIFQJMQR8R4fD88k3R8gzZNElj+IAShuN6vPG/UBRkUOk1Cdx6Y9PyYQySKraTfXlIdInjCLeRdt4u3Kh/jXnpu4b833+K83B9n4GpHt91G47gGMqsqnri+TnZaDMeRlYtBAY3TCh7F7N5Nyw/VcNFuTiDzhFpSwFXvZ+bjVela2FLNo67u0bKondayb5XNnUjb/Jobs/Cq1hvP5t+0Wnkr8CtTuQ420stCwmnuzNpO6sYas3bcR3nEDG1pvZNuWrxH0J6CGTYjnVxEq38Xj+el8aIqQbTSwwv5b6jKmMbrqA9Jbp8fPPeCy4G/6A0WNCkrQSXL5Z1mnPABAmi9I2siPkNZmXEYLyw2XkOML8rH4DPs/F2ZE7QXsOvQlMqY8g/AYifz3Chz1hVQWPEv6uI8Jma1UGAJEpjyD585D1I14hcoRD6AaAuTtXoyjbhLrxBco2vY7ZMP/kOS5k8zSG0gpuxkhDawfvZa1QwrIj5ThEolUfDSZpGdVPjLNpFmkUp3/WfYOuYwD1gkIqfKDxr/yp707OXdzM3eGjLzy6mYyWpNxZe0jw6QwyhDCmTuefypuhshKthkKKbfayS1vJCNqH1Fedx6GnRcwccL/sbnlFtZNuBSAzYYITdMzsXgttKpGtlXt4e26FpZU1vJpyMdF142htu4lNmy8mprad3F79rFn709ITp7FhAmPYjBYMJoMJGfaaKhwxztKjcYEhg29k7DI5sk1xTz5SQlNUktoUlPPj/+fYh2zsYFHF4z5BhdesJGxYx7AatVM0rItZoQQCCH4TFoiq5pcbGnxsKrJzVWZyZT6A9xdNZL03K9RXv4sVdWvdxuXtrZ6j2os1l+clUEdYEiSlYeunsT2smbmPPxfRlb6MW1t4OfJqfx5wQR2VbTyy7eKmF2QzjX3f4t7rpjMyn11/PmjQ1w3M58tP1vI6h9ezKeL53NOWgIlgSB73D5mrN3Na84I/mytM3bukCSsJgOm7GyuXXARVUkpRF58keph2Thbw3w9LwOHotAQiTC0IcyckSMJGgXLpicDMDIzAWeeVv+9OsPIaKuFc156gWkiyIcZs/hWxRjetYbJKEzHOiqF1C+OIf3WiXyaZ2Gi08qVw9IwC4FMMfP4vkpcERWxt5nk2gDfys/kg4ZW1jS5SEzI5tzPrCKQOoJzc5IxfmsK5kuGknVOFpeNzWS0MPDlFu3jMMwdIZSsae2rWYTvK6tYt7cepGTFoiwKnV+mLi/MnsQUJmanMdT2DcZV/Yp8q5kdTVppWYkvwF5fgMrDCYyf/yQXXTuMCz6bRMLoVGTQTcKCCzAarXz/iqvZVHceQ1JHcu//fZ8x/36W4T9cRMHkDQxD8nEi7MkfxXnrfoah5hXGGtcBUGbOIPUH93Lp+ElY3V7uG3s5oS1/IrHiIr5lNfKr2Tey9id/JP+Jv2GfbeU648cMH5pEUtX5vGO6C4BaayYfp5fhW/k79q3N4/oNS8nYvRZr2VwaW3xM2fFn7I2V1G25gdotl7Pz3IvY9pVpNM5cyJMRH1II8swWRiQeJq/sU1LqC5B+rRP1g5r7WHbgZmpafDgfGYZxz3k0mrUgYalNwZRSxSWp72NSA9xTvoelG4KoGHndOZuy839C/U3rsSRVYX4llWGHksje9k0S7J+hNeN9Ri34GSz4KS25q5E2K42j3iSQ2UrS3puwenKpDIHDVUKes5XiT2ex/u3Z1IQkSY5pWCsu5LkhBdilm68FnwRg84QLCdinc0ho2fPyIynsW1eNe3Qy4xxWCjdfwpwjw7mnHC58p4zEnFko7lEE7AdwOyI4JlxFtnEmaspBxrObPWljaU1IIGftdmYUzQP4//bOPD6q8tzj32fWZCbJZJ3sC4QAAQTCJhRBNmURca+iVaTeuit6vVdFrMu99arXWtuqRepyLRWR1lq3uqAoWkSURUCWhDVkX8m+zWTmvX/MEVkSQAQySd/v5zOfzHnPO2d+c3LO77zvc97zvNTWD2db7hwais6kcr2HRAIhUXNyKG/tLCcuJIEmwnhl2yIA3J423jkrghr3p+TmLaC5eS9bttzKunUXYbW4GDTw95hM3z+oFJ0URsmuWlYuCQxztRhpI1bmVeLx+RmQGMFTq0dhjvx37LbYI/yioMWDAGmOUCwWZ6e+MjE6nEafn9u2FxBmNvHrfqk8PzCDrY0tvOi7nKioMXg91dR626k4aHz+p9X1TF+/g2Vl+zvd9snCcuwq3ZfzhyQREWpl2doCPt5cwcxsN78Y1xsR4bqzevHKmn08PGsQIsKcMRmU1beSnRDBhTmBOGu0M2BsvR121pY1cdO2fTjMZjxK4U92Is3tzMpOPPB9U2NcmKWQpwrLUVYTFdv285hnG+3l+wmJd/DXWUMQm5k3v97O6vZWHGJi+q9XUtbkgSlJKJMwyR1oNV/Rvz/zdhbz5rCZWICvUCwqrOCGVDcN7T7W1jVxU6obm8lEToSDgjTYVNOKtMCevP08PTuHczMT+GtVLQ/vKuHDEX3x2k3s9/mYGB9JarILUgLfFQP8E1idWsXk57+ifW87DIkGBau/qWDmvkb2VDUxKDOD99sUv7rmXXZUxdPe1MwAk5W4SwMtVO/yTaxsbWSpr4Ci6MChNSkyloz4SIiPBKC9rg8tGzcT0i/w0FZyZCj/vHsiYXYLFuNRf7Jngc/LuYluFpZXc/NXJpaYhInbP+XTaAVnTuaNlLHcXNNKSlQoal0d1rMTmJdjJcksbDWbkEYvYelJhA1Oprb+Sias/S3tq85jTcT/8oGEcXPrJl6XVJ74yUXsKLIxZ9v7lBe42Btv5iFzPU3uMNbdeB+WrTv4bzWGzclOvGcG9N0U5uLzfdvYnxBKVp2JxLsupyzchr/JRMgWE20jILvsS3weDwMmZTE7+Rquq1vPdt8oRCXh+MTDWxEXcfHeHby/dy4Zi16gZg+MLvawImkaU+q2kekPwbU/GzlzMv49VUhxLl8+0sbYUBstNylCwvoQ9dEsQup7Uze0EUf+P4i7/AZKXsol4qwkViUlkBMVzs9TEwLzrrb5qF5bzpd7f8o6cTO+Ygtf1N6BpLSzcfxkBs+5HPaUE+b18XWzj0vOSeWZkFZmuMJIefR+Sh5YSSYW/MpJUVs7eTuGkpa8gsbJrxD26TWQFsMK/3YyfSF8aj0HgOUpY7jntulEr8uFEckkNNax/MWtZCnIy7IQaTFz0bAUHn5nGxeN6U9TQz0b1FRSKOQu66Pc73+Ca/MTmeFYwLwzLsVa+x4lpX+hT5/7+W1xO6MjGzg7OnAPLDHTxe4NFUi4lQ9DPWS2tJBIJMu3lRPjtLHshtFcskhxy8p2lvVqoldECE7z93MH72tpI95mwW46ejt3XFQ4FoHdLW3cnOomwmLmnFgX89Lj+U1+OecM/D2hjlDO/zoXiwhfjs7GZjLxamnAzF8qruLyhOgjhm6eTI5p6iLyEjATqFBKDepgvQC/A2YAzcC1SqkNJ1voiXJ23zjO7htHi8eHzWI6sDPvPy+b2ydl4XIErvYmkzB/enaH28h02Gny+cltauXVwb0ZFu4gZ9nXqEYvkya7D9SLsVkY4wrj85pGTEBEQztLvy7kkpxk5p+TTZwxkW+qyUyh30dLTSu9Yp3cO70/DzbVUuFtZ0xk4LHnmYnRfF7fzEiXk8vio7g9t4AHd5WwqaGFijYv7QomxQRa+CNdTtbXN6Fi7AxsVIwf24vzByciIizITOLmbft4vqgSt3HDeESEg46IDLUFspJXtWICMp12IhMi2FJSx5OXDSEyPYIrN+/hXecgPqwqR1raKdlcRUv/ZN7dXEpZaSOkOFnwj21YxriR1nb+c/yh+9TichF+9rhDv9dhO2SZpKGQNJSLG5pZWF7NsAuz2TTuJaYOTMSEwO4SlEeRV9ZARKgFafPzixAXr4W2EmqzYN1UjjfNye+b6pjocbNvxI3khfTG3VbBUqkn1B/JLesfwJx9B0/HTmTpGVO4fM4Mkjau5tXw0exv8JGY4eLu0BjeTc/GKsKV8dGcE+fiV7tLeMfbzJNXDCXTFkLd4m1UPL8D5cnEHGXl5aRHqK4uZ+4TF5Jqgx01PvJfWkvR+bPxucFdvp+R61cy2u/DExtP0n334Rw3BudPFOPX5rO6uY6nvNex7uqJNKwqpuGTApQ1lPfT4LFr5hFXs5/7+g7hkoQYtqzbRF19PXGTx5MWNx2Avv89lrcranl9az6v17Xgdjo43x0JgP28Xry33kNsixdzcxaf51dhcljId1p5r6aZFDGTUuplc5qdhDNTqfkql6ERDkSEmDk51OYV8/6Ly/BbYhl268W4ku3s3fc/MK0BiyWc0ZVrWL1/NBjP1JV5QljyVQF4fbxXXElShpMB4uKtihrykm2MCA/lkv4pPLl8B9tL6lERwnZzIlPtVvbXzuE60zt8GjKeZS3DeGPdXt4Yej4jR1zBC0WVPLWvGHthBa8O7s3YqHDOmJhC1kg3sxevZXOrj6c+3smE/m4+za1g5uBEWk3QNDKW6jYPUzYGJnpxmE24bRZC/bCzqZVodezARbjFzEiXk3V1zfwi9fsW/53pCayorufuvGIU4FOKBp+fv5fXMiUmgg+q6kiyW9nc0MI3Dc0Mi+i8N/BjOZ6W+svAM8DiTtZPB7KM15nAQuNvUBFqMx+yLCIHDP1Y9HUEQi03pMQdMNILbQ6I/j7X9Hec545kVW0jwyOcPHTNSEAxPD36kDqzkqN5trCSSanRLLk4EBNeurGVyppGRrkC/2yn2XzgbjvAcwMyuCO3gI+r63HbLFzgjmSkcWCMcjl5xkg/8uKUAaSH2g987iJ3JG9V1PCr3aWMcjkJNZkOpFg4nEhjf6SEhfCTxGhSQ+387OeZFNe0cEaKC79SZITaWLCjiJp2H9OtoXy4JZ+pJXVU1LfR54xY8sxC84TAaIXsFjP9EsKPax93xBnhDu7rncj/7CllRP940tLiKdtVTIhJEJ8ir6ye/omB7fdx2Fk7NDDOOecf+dTVeigcFssZXwRGX2AaAsbPviUllph+rzPXmcYzG0sxDYpmYa84NsTEU9zqBYF9QGNNI3OTY7k9LR63MeVeqEm4ZONu9iSaCEuPpq1/NP7Wduwj4inp72JViZ1akvkyN/APkUYvkb0jKEyws7yhgUEJbt79j9+xZtVmfvfodUTHB44nLMK80b34y4ebKMyIocLnI35CKo4cN+t2VPJIUwS9qispNoVyx45iVtY2MTo7gt/uK+Uzpw2P34/NFJgz9H/3lpLlsOOymLl9+z6S7VaGuZw8V1jB1sZWXhyUwabGUr7Mq8K0v42mFCef1TRwU2ocQ7Id3LhtH6+UBEai5BjJ50P6RJLQJ5K81kt5f0sZG8clI3IdDmcs27bfjc0aw5qKaayvmELvNBt7WjwMjwvjsfdz8eTEYA+3sfjzQmYMTuSztDBsAo/1TSUixMrsUaks2lMOg6JQwCefl7C2OR27tTf7mzzMm+bkDZuXuVv2snBAOo/sLmF8VBhlbe3M+XYvf8vpw5BwBxvKG9hUVMfZfeP4bEcl97+5hca2dob0i2HWhp2UedsZXqfYXlzP9ef0oc7v55O91eQ1tWIOtTApNuK4jsuH+iRT1Ooh0f59Y8RqEp7JTufcdXkk2K28NiSTa7/dyx8KK6hrb8erFM8NSGf25j38X3FV15q6UupzEck4SpULgMUqcAdgjYhEikiiUur4ZqDtBoyNCuPlQb2YFPO9QT11+dAO606PdfHAzmLOjY1geHpUh3UmREfwbGElI+O+394sdxQpITYirR3/S6wmOcTkD2aEcSEYGxl2iKFD4OL1VP80pqzN44vaRsZEOrGYOu76xYTZiHHauHliJlf1TztQ/l0YyiTCnKRYHt5dQpbDzguj+rMuM4H5b3xLtNPG4qmD+KKlmfKmNpZvLefx0X06/J4fwm1pbnY3t/FkfjllbV52NbeRYLfiiHGSV95AvTGJc8RBF9eIUCsJZgu/HJLJx9X1DItwMCg8lCpPOxUeLxOjI8CcQhIwNaaZD6SedfXNjHY5CQnx8/o/87lzZAZ3jul1RDd5bFQ4F7ojeaaggtGuMMZdOxCAVTUNXL81H4vVzMORkWwpqGVTXTMNiSGUhln5e20d58a4+M9eCRTZwykMiSLNfegFT0R49exszvpqO7/JL+PxfqkU2+B6Ty1xdit/mzGOy57+gtZeVt6kFtVuo04UC0urWFhUwZ3p8URbLexqbuOlQRmMdDmZtm4HMzbsZHBYKHnNrZwX5+K8uEha443sla2KcuP7p8W6Dhw/r5RUE2oS+h3WAHho1kBuntDnwH5JSLiA6OizsFojcWU0cpXdzOK6Oj6squOuiSn8e+VGMlIj2er34o518nZtPf7sSB7uk0xvY4z+3LG9eD4vYBcmr5/sEDtv3TUWq9nEHcs2svCDHVw1JZM/mz1csnE3Vr/C/00VZ8aF8aFLmL1pN2/lZPGHlbtwh9tZdPVwZj2zir/sroCh0dxdVYXTbOKvQzOJ9cKU33zGnjWl7KxopGp/M/PG9+bWiX1w2o8vGj0k3MGQ8CN7u1nOEFaO6k+U1UKExcwtaW5u3V7AE3vLGBweyqjIMC5LiGZpaTUPZSYTYzs10W85nruxhqm/20n45V3gMaXUKmN5BXCPUmpdB3WvB64HSEtLG75v377Dq/QI8lvaSLJbsXUSn/P4/SzYWcz1KXFkOUM6rPND+WNhBSNdYeR0Elr5sraRS77Zxbz0eO7pndhhHQjMamPuxPQBar3t/HTjbub3TmSi0Wtp9/nx+tQRvaGThcfvZ/6OIt4or6XF72dMpJOErfWs3l1NWrSDb4vreP3GMYzICPSI7n/zWxIiQrh1UtYxt93k81HlaSctJDDCQSnFmxuLmTowAUcnJ11Zm5cLNuxkX6uHmXEuKj3tfFXXREaojdeGZJJx0IVVKcXmxhaS7NYDz0sci3vyCllSWs0fB2Zw744iPH7FW8Oy6OcMYfGX+fzy7a3ETU2jvL0dW1EzTelO+jjsB/KX54Q7eG94FiJCaZuH18tq+Ki6ngqPl7dzsnDbrWwprmPm06sY1SuakqGR1Hh9bB47ELMIY9ZsY2+Lh5ERTt4Zfux9eDgev59WvyLCyKHyVH4Zj+8tY2FKIjftKyauDTbPyDnkgnn13zbyUTRYS1v4eNKgAz08r8/PDX9ezye5FfjiQvAOiSIpv5nUVthe1kCrTWg/043VJPBFOQsmZXH28GRu+2YPm70erH7FtWlx/FtK3IEL1v1vfssrawpwh9t55sphjOoVfeSPOAl4/YrRa7ZR3Obl0b4pzE2OJbephQlf57GgdyK3pcef0HZFZL1SakSn60+Cqf8DePQwU79bKbX+aNscMWKEWrfuCN/XnEJym1pIDbEdcoOoO9HU7uOj6noyHXa251Xz2Pu5ZMQ4OCPZxfwZ2YRYT9/vavH5ebaggqcLyom1Wrgx1c2VidE4LT9eQ3mbl9FrttHiVyTarSwd0pv+Rou5xePjrMc/oVL8eMa4wSTMckeycEA671XW8fS+ch7pm8JI19G7961eH4Me/JDZo9KYNC6NNr9iWlzgxvmduQUsLd3P9Slx/FdW8o/+Pa+VVnNHbiHZzhD2Nrfxcq8UJqTHHFJndWENF+/I51p7GI+ddeiFpN3nJ7esgfiIEFwOKzbjhnpDq5cV2yt4e28lH4T7USLYrSZa/Ypws4mhbSbu7Z/M8NRDe8y1zR4Wf7mP2aPSDtznOlW8WlLNI3tK+eLM/gd64Q/vKmZ8VPiBRtEP5XSY+iJgpVJqqbGcB0w4VvhFm7qmJ9Di82MV6TSkdaIsLKjgncpaFg3MOCTJFEBBdTOVjW2s9rawx9vO4/1SOu0VHo2VeRX0Swgn0XVoiGVZ6X7m5RawcEA6F8V3HEL8IXy+v4GfbgpMrPKrrGT+LSWuw3qVLR7iQm0drjsWG+ubWVJajdNsIt5m5bKEaGJPUXjjh6KUOqmjXY5l6ifjV78N3CoirxG4QVrXk+LpGs3RCDWfmkc9bkpzc1Oau8N1aTEO0mIcDOfHGe6Efh1v/7w4F3lNbs45wZbk4SQaN5lzwh3MTT5yjPh3nKihAwyNcDC0k9BjV3Mqhy92xPEMaVwKTABiRaQIeBACTw4opZ4D3iMwnHEXgSGNc0+VWI1Gc+oJs5h5oE/SSdteb4edm1PdXJUUjfk0G9y/Iscz+mX2MdYr4JaTpkij0fQozCIn9SKhOTo9Nk2ARqPR/CuiTV2j0Wh6ENrUNRqNpgehTV2j0Wh6ENrUNRqNpgehTV2j0Wh6ENrUNRqNpgehTV2j0Wh6EMeV++WUfLFIJYG01T+EWKDqFMg5lWjNp57uphe05tNFd9N8PHrTlVIdJ9ChC039RBCRdUdLZBOMaM2nnu6mF7Tm00V303wy9Orwi0aj0fQgtKlrNBpND6K7mfofu1rACaA1n3q6m17Qmk8X3U3zj9bbrWLqGo1Gozk63a2lrtFoNJqjoE1do9FoehDdxtRFZJqI5InILhG5t6v1HI6IpIrIpyKyXUS2isg8ozxaRD4SkZ3G3x8/6eNJRkTMIvKNiLxrLAe1ZhGJFJHXRSTX2N9jglmziNxpHBNbRGSpiIQEo14ReUlEKkRky0FlneoUkfnG+ZgnIlODRO8TxnGxWUT+LiKRwaK3M80HrfsPEVEiEntQ2Q/W3C1MXUTMwLPAdGAAMFtEBnStqiNoB+5SSmUDo4FbDI33AiuUUlnACmM52JgHbD9oOdg1/w74QCnVHxhCQHtQahaRZOB2YIQxcbsZuILg1PsyMO2wsg51Gsf2FcBA4zN/MM7T08nLHKn3I2CQUmowsAOYD0GjFzrWjIikAucABQeVnZDmbmHqwChgl1Jqj1LKA7wGXNDFmg5BKVWqlNpgvG8gYDTJBHT+yaj2J+DCLhHYCSKSApwHvHBQcdBqFpEIYDzwIoBSyqOUqiWINROYNjJURCyAAyghCPUqpT4H9h9W3JnOC4DXlFJtSqm9BOYoHnU6dH5HR3qVUsuVUu3G4hogxXjf5XoNfR3tY4CngLuBg0eunJDm7mLqyUDhQctFRllQIiIZQA7wFRCvlCqFgPEDHU/h3nX8lsDB5D+oLJg19wYqgf8zQkYviIiTINWslCoGfk2gBVYK1CmllhOkejugM53d4Zz8OfC+8T5o9YrILKBYKbXpsFUnpLm7mHpHU5AH5VhMEQkD/gbcoZSq72o9R0NEZgIVSqn1Xa3lB2ABhgELlVI5QBPBEbroECMGfQHQC0gCnCLys65VdVII6nNSRBYQCIku+a6og2pdrldEHMAC4IGOVndQdkzN3cXUi4DUg5ZTCHRhgwoRsRIw9CVKqTeM4nIRSTTWJwIVXaWvA8YCs0Qkn0BIa5KIvEJway4CipRSXxnLrxMw+WDVPAXYq5SqVEp5gTeAnxC8eg+nM51Be06KyBxgJnCV+v5BnGDVm0nggr/JOA9TgA0iksAJau4upr4WyBKRXiJiI3Dz4O0u1nQIIiIE4rzblVK/OWjV28Ac4/0c4K3Tra0zlFLzlVIpSqkMAvv0E6XUzwhuzWVAoYj0M4omA9sIXs0FwGgRcRjHyGQC91uCVe/hdKbzbeAKEbGLSC8gC/i6C/QdgohMA+4BZimlmg9aFZR6lVLfKqXcSqkM4zwsAoYZx/mJaVZKdYsXMIPA3ezdwIKu1tOBvrMIdI02AxuN1wwghsCogZ3G3+iu1tqJ/gnAu8b7oNYMDAXWGfv6TSAqmDUDDwO5wBbgz4A9GPUCSwnE/b2GuVx3NJ0Ewga7gTxgepDo3UUgDv3dOfhcsOjtTPNh6/OB2B+jWacJ0Gg0mh5Edwm/aDQajeY40Kau0Wg0PQht6hqNRtOD0Kau0Wg0PQht6hqNRtOD0Kau0ZwAIjLhu6yWGk0woU1do9FoehDa1DU9GhH5mYh8LSIbRWSRkTu+UUSeFJENIrJCROKMukNFZM1BubijjPI+IvKxiGwyPpNpbD5Mvs/rvsR4YlSj6VK0qWt6LCKSDVwOjFVKDQV8wFWAE9iglBoGfAY8aHxkMXCPCuTi/vag8iXAs0qpIQTytpQa5TnAHQRy/PcmkEtHo+lSLF0tQKM5hUwGhgNrjUZ0KIGEVH5gmVHnFeANEXEBkUqpz4zyPwF/FZFwIFkp9XcApVQrgLG9r5VSRcbyRiADWHXKf5VGcxS0qWt6MgL8SSk1/5BCkV8eVu9ouTKOFlJpO+i9D30+aYIAHX7R9GRWAJeKiBsOzLeZTuC4v9SocyWwSilVB9SIyDij/GrgMxXIiV8kIhca27AbObA1mqBEtyw0PRal1DYRuR9YLiImApnxbiEwscZAEVkP1BGIu0MgtexzhmnvAeYa5VcDi0Tkv4xtXHYaf4ZG84PQWRo1/3KISKNSKqyrdWg0pwIdftFoNJoehG6pazQaTQ9Ct9Q1Go2mB6FNXaPRaHoQ2tQ1Go2mB6FNXaPRaHoQ2tQ1Go2mB/H/rPcOkFofsvsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = 10\n",
    "for fold in range(folds):\n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "    for res in results[fold]:\n",
    "        train_loss_arr.append(res[1])\n",
    "        test_loss_arr.append(res[2])\n",
    "    \n",
    "    print(len(train_loss_arr))\n",
    "    print(len(test_loss_arr))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    ax.WindowState = 'maximized';\n",
    "\n",
    "    format_mae = \"{:.2f}\".format(best_rmse_arr[fold])\n",
    "    \n",
    "  #  ax.plot([e for e in range(1,len(train_loss_arr) + 1)], train_loss_arr, label=\"train_loss\")\n",
    "    ax.plot([e for e in range(1,len(test_loss_arr) + 1)],\n",
    "            test_loss_arr, label=\"Fold \" + str(fold) + \" (RMSE = \" + format_mae + \")\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    ax.title.set_text('10-Fold Validation')\n",
    "    ax.legend()\n",
    "    ax.figure.savefig('Visualization/'+str(fold)+'.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10 fold cross validation set results. Calculating pearson relations.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "# fig = plt.figure()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('R2 and RMSE')\n",
    "# ax1.plot(x, y)\n",
    "# ax2.plot(x, -y)\n",
    "for fold in tqdm(range(folds)):\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = 40\n",
    "    test_loader  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
    "                             weight_decay=10**-5)\n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "#     result_file_name = 'novelresult_' + str(fold) +  '.csv'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(torch.load(model_file_name))\n",
    "    \n",
    "    test_loss,test_rmse, true, prediction = predicting(test_loader, model)\n",
    "    \n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('Test R2: ', score)\n",
    "    print('Test RMSE: ', test_rmse)\n",
    "    ax1.scatter(prediction, true, lw=2, alpha=0.5, \n",
    "                label='Fold %d (R2 = %0.2f)' % (fold+1,score))\n",
    "ax1.plot([min(prediction),max(true)], [min(prediction),max(true)], \n",
    "         linestyle='--', lw=2, color='black')\n",
    "ax1.xlabel('True')\n",
    "ax1.ylabel('Predicted')\n",
    "ax1.title('10-Fold Validation')\n",
    "ax1.legend()\n",
    "ax1.savefig('TestR2.png')\n",
    "# plt.show()\n",
    "# avg = bestrmsesum/10\n",
    "# print('10 fold avg is : ', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole data set as training dataset\n",
    "def createTestData(path,filename,datasetname):\n",
    "    iy = 0\n",
    "#     folds = 10\n",
    "#     for fold in tqdm(range(folds)):\n",
    "#     df_train = pd.read_csv('New_fold/fold_'+str(iy)+'_'+'x_train.csv')\n",
    "#     df_test  = pd.read_csv('New_fold/fold_'+str(iy)+'_'+'x_test.csv')\n",
    "#     smiles = df_train['SMILES']\n",
    "#         codIds = df_train['CODID']\n",
    "#     solubility = df_train['logS']\n",
    "#     solubility = solubility.to_numpy()\n",
    "    df_test = pd.read_csv(path + '/' + filename)\n",
    "#     df_test  = pd.read_csv('New_fold/testset_novel.csv')\n",
    "    smiles_test = df_test['SMILES']\n",
    "#         codIds_test = df_test['CODID']\n",
    "    solubility_test = df_test['logS']\n",
    "    solubility_test = solubility_test.to_numpy()\n",
    "\n",
    "\n",
    "#     smile_graph = {}\n",
    "#     solubility_arr = []\n",
    "#     smiles_array = []\n",
    "    smile_graph_test = {}\n",
    "    solubility_arr_test = []\n",
    "    smiles_array_test = []\n",
    "\n",
    "    for i,smile in enumerate(smiles_test):\n",
    "        g = gd.smile_to_graph(smile)\n",
    "        if g != None:\n",
    "            smile_graph_test[smile] = g\n",
    "            solubility_arr_test.append(smiles_test[i])\n",
    "            smiles_array_test.append(smile)\n",
    "\n",
    "#     train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(iy),y=band_gap_arr,\n",
    "#                                smile_graph=smile_graph,smiles=smiles_array)\n",
    "\n",
    "    noveltest_data = Molecule_data(root='data', dataset=datasetname,y=solubility_test,\n",
    "                               smile_graph=smile_graph_test,smiles=smiles_array_test)\n",
    "    return noveltest_data\n",
    "#     iy+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "noveltest_data = createTestData('New_fold','testset_novel.csv','testset_novel')\n",
    "# noveltest_data = Molecule_data(root='data', dataset='testset_novel',y=None,smile_graph=None,smiles=None)\n",
    "noveltest_loader  = DataLoader(noveltest_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     model = AttentiveFP(in_channels=112, hidden_channels=279, out_channels=1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predicting(loader, model):\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # out = model(data)\n",
    "        # mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "        # return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "        y = data.y.view([-1])\n",
    "        out1 = out.view([-1])\n",
    "        # print(\"test : \", y.shape)\n",
    "        test_loss = F.mse_loss(out1, y)\n",
    "        # print(\"no of graphs: \", data.num_graphs)\n",
    "        total_loss += float(test_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "        total_preds = torch.cat((total_preds, out.view(-1, 1).cpu()), 0)\n",
    "        total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "#         print(\"total_labels : \", total_labels.shape)\n",
    "#         print(\"total_preds : \", total_preds.shape)\n",
    "        # mse.append(test_loss).cpu()\n",
    "    # return test_loss,float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    return total_loss,sqrt(total_loss / total_examples),total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1afa9944cf14f839b7593b5614061ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2:  0.5620946217480056\n",
      "Test RMSE:  1.4742943701303723\n",
      "Test R2:  0.5066979566343093\n",
      "Test RMSE:  1.5630250354992792\n",
      "Test R2:  0.6857048644607735\n",
      "Test RMSE:  1.225492850445466\n",
      "Test R2:  0.4688989160974357\n",
      "Test RMSE:  1.5537526226757463\n",
      "Test R2:  0.3539823846078044\n",
      "Test RMSE:  1.7413094724735096\n",
      "Test R2:  0.6178359931804998\n",
      "Test RMSE:  1.346382407225885\n",
      "Test R2:  0.5027372081599021\n",
      "Test RMSE:  1.556597472247931\n",
      "Test R2:  0.7067893782337602\n",
      "Test RMSE:  1.221677728277639\n",
      "Test R2:  0.45442047310056144\n",
      "Test RMSE:  1.6057811153938162\n",
      "Test R2:  0.5485235517923928\n",
      "Test RMSE:  1.5479478454289926\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10 fold cross validation set results. Calculating pearson correlations.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "fig = plt.figure()\n",
    "for fold in tqdm(range(folds)):\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    \n",
    "#     TRAIN_BATCH_SIZE = 40\n",
    "    test_loader  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "#     result_file_name = 'novelresult_' + str(fold) +  '.csv'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(torch.load(model_file_name))\n",
    "    \n",
    "    test_loss,test_rmse, true, prediction = predicting(test_loader, model)\n",
    "    \n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('Test R2: ', score)\n",
    "    print('Test RMSE: ', test_rmse)\n",
    "    plt.scatter(prediction, true, lw=2, alpha=0.5, \n",
    "                label='Fold %d (R2 = %0.2f)' % (fold+1,score))\n",
    "plt.plot([min(prediction),max(true)], [min(prediction),max(true)], \n",
    "         linestyle='--', lw=2, color='black')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('10-Fold Validation')\n",
    "plt.legend()\n",
    "plt.savefig('TestR2.png')\n",
    "plt.show()\n",
    "# avg = bestrmsesum/10\n",
    "# print('10 fold avg is : ', avg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train whole dataset\n",
    "wholemodel_file_name = 'saved_models/wholemodel.model'\n",
    "wholetrain_data = createTestData('Data_Prep','solubility_1.csv','solubility_1')\n",
    "# noveltest_data = Molecule_data(root='data', dataset='testset_novel',y=None,smile_graph=None,smiles=None)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        wholetrain_loader  = DataLoader(wholetrain_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     model = AttentiveFP(in_channels=112, hidden_channels=279, out_channels=1,\n",
    "        model = GAT().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
    "                                     weight_decay=10**-5)\n",
    "        model_file_name = 'saved_models/model_' +  str(5) +  '.model'\n",
    "        result_file_name = 'wholeresult.csv'\n",
    "        checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        scores = []\n",
    "        true_val = []\n",
    "        pred_val = []\n",
    "        the_last_loss = 100\n",
    "        train_loss,train_rmse=train(model, optimizer,wholetrain_loader)\n",
    "#         test_loss,test_rmse, true, prediction = predicting(noveltest_loader, model)\n",
    "#         true_val.append(true)\n",
    "#         pred_val.append(prediction)\n",
    "#         score = metrics.r2_score(true, prediction)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} ') # f'Test: {test_rmse:.4f} ' f'R2: {score:.4f}'\n",
    "        \n",
    "#         ret = [epoch,train_rmse,test_rmse,score]\n",
    "        \n",
    "        train_losses.append(train_rmse)\n",
    "#         val_losses.append(test_rmse)\n",
    "        # Early Stopping\n",
    "        the_current_loss = train_rmse   #.item()\n",
    "#         best_ret.append(ret)\n",
    "        if the_current_loss > the_last_loss:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "    \n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                break\n",
    "        else:\n",
    "#             ret = [epoch,train_rmse,test_rmse]\n",
    "            trigger_times = 0\n",
    "            the_last_loss = the_current_loss\n",
    "            best_rmse = the_current_loss\n",
    "            \n",
    "torch.save(model.state_dict(), wholemodel_file_name)            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# testing on whole dataset trained model \n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "# fig = plt.figure()\n",
    "# for fold in tqdm(range(folds)):\n",
    "# val_losses = []\n",
    "# train_losses = []\n",
    "# mae_arr = []\n",
    "# patience = 30\n",
    "# trigger_times = 0\n",
    "# the_last_loss = 100\n",
    "model = GAT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
    "                             weight_decay=10**-5)\n",
    "model_file_name = 'saved_models/wholemodel.model'\n",
    "result_file_name = 'wholetrained_novelresult_' + str(fold) +  '.csv'\n",
    "checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint)\n",
    "#     model.load_state_dict(torch.load(model_file_name))\n",
    "\n",
    "test_loss,test_rmse, true, prediction = predicting(noveltest_loader, model)\n",
    "\n",
    "best_ret = []\n",
    "bestrmsesum = bestrmsesum + test_rmse\n",
    "results.append(best_ret)\n",
    "best_rmse_arr.append(best_rmse)\n",
    "true_val.append(true)\n",
    "pred_val.append(prediction)\n",
    "score = metrics.r2_score(true, prediction)\n",
    "scores.append(score)\n",
    "print('Test R2: ', score)\n",
    "print('Test RMSE:', test_rmse)\n",
    "#     plt.scatter(prediction, true, lw=2, alpha=0.5, \n",
    "#                 label='Fold %d (R2 = %0.2f)' % (fold+1,score))\n",
    "# plt.plot([min(prediction),max(true)], [min(prediction),max(true)], \n",
    "#          linestyle='--', lw=2, color='black')\n",
    "# plt.xlabel('True')\n",
    "# plt.ylabel('Predicted')\n",
    "# plt.title('Model Validation')\n",
    "# plt.legend()\n",
    "# plt.savefig('WholeTestR2.png')\n",
    "# plt.show()\n",
    "# avg = bestrmsesum/10\n",
    "# print('10 fold avg is : ', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a8daccb9d9414099c228a94d8c2fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.4616 \n",
      "Epoch: 001, Loss: 1.2266 \n",
      "Epoch: 002, Loss: 1.2681 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2538 \n",
      "trigger times: 2\n",
      "Epoch: 004, Loss: 1.2525 \n",
      "trigger times: 3\n",
      "Epoch: 005, Loss: 1.2969 \n",
      "trigger times: 4\n",
      "Epoch: 006, Loss: 1.2833 \n",
      "trigger times: 5\n",
      "Epoch: 007, Loss: 1.2495 \n",
      "trigger times: 6\n",
      "Epoch: 008, Loss: 1.2655 \n",
      "trigger times: 7\n",
      "Epoch: 009, Loss: 1.2597 \n",
      "trigger times: 8\n",
      "Epoch: 010, Loss: 1.3201 \n",
      "trigger times: 9\n",
      "Epoch: 011, Loss: 1.2658 \n",
      "trigger times: 10\n",
      "Epoch: 012, Loss: 1.2386 \n",
      "trigger times: 11\n",
      "Epoch: 013, Loss: 1.2260 \n",
      "Epoch: 014, Loss: 1.2200 \n",
      "Epoch: 015, Loss: 1.2598 \n",
      "trigger times: 1\n",
      "Epoch: 016, Loss: 1.2413 \n",
      "trigger times: 2\n",
      "Epoch: 017, Loss: 1.3764 \n",
      "trigger times: 3\n",
      "Epoch: 018, Loss: 1.2098 \n",
      "Epoch: 019, Loss: 1.2302 \n",
      "trigger times: 1\n",
      "Epoch: 020, Loss: 1.2934 \n",
      "trigger times: 2\n",
      "Epoch: 021, Loss: 1.4111 \n",
      "trigger times: 3\n",
      "Epoch: 022, Loss: 1.2686 \n",
      "trigger times: 4\n",
      "Epoch: 023, Loss: 1.2925 \n",
      "trigger times: 5\n",
      "Epoch: 024, Loss: 1.2284 \n",
      "trigger times: 6\n",
      "Epoch: 025, Loss: 1.1850 \n",
      "Epoch: 026, Loss: 1.2088 \n",
      "trigger times: 1\n",
      "Epoch: 027, Loss: 1.2321 \n",
      "trigger times: 2\n",
      "Epoch: 028, Loss: 1.2526 \n",
      "trigger times: 3\n",
      "Epoch: 029, Loss: 1.2380 \n",
      "trigger times: 4\n",
      "Epoch: 030, Loss: 1.2298 \n",
      "trigger times: 5\n",
      "Epoch: 031, Loss: 1.2835 \n",
      "trigger times: 6\n",
      "Epoch: 032, Loss: 1.2686 \n",
      "trigger times: 7\n",
      "Epoch: 033, Loss: 1.2365 \n",
      "trigger times: 8\n",
      "Epoch: 034, Loss: 1.2771 \n",
      "trigger times: 9\n",
      "Epoch: 035, Loss: 1.2980 \n",
      "trigger times: 10\n",
      "Epoch: 036, Loss: 1.3607 \n",
      "trigger times: 11\n",
      "Epoch: 037, Loss: 1.2828 \n",
      "trigger times: 12\n",
      "Epoch: 038, Loss: 1.2136 \n",
      "trigger times: 13\n",
      "Epoch: 039, Loss: 1.2649 \n",
      "trigger times: 14\n",
      "Epoch: 040, Loss: 1.2050 \n",
      "trigger times: 15\n",
      "Epoch: 041, Loss: 1.2889 \n",
      "trigger times: 16\n",
      "Epoch: 042, Loss: 1.2956 \n",
      "trigger times: 17\n",
      "Epoch: 043, Loss: 1.3673 \n",
      "trigger times: 18\n",
      "Epoch: 044, Loss: 1.3058 \n",
      "trigger times: 19\n",
      "Epoch: 045, Loss: 1.2446 \n",
      "trigger times: 20\n",
      "Epoch: 046, Loss: 1.2455 \n",
      "trigger times: 21\n",
      "Epoch: 047, Loss: 1.2561 \n",
      "trigger times: 22\n",
      "Epoch: 048, Loss: 1.2469 \n",
      "trigger times: 23\n",
      "Epoch: 049, Loss: 1.2835 \n",
      "trigger times: 24\n",
      "Epoch: 050, Loss: 1.2397 \n",
      "trigger times: 25\n",
      "Epoch: 051, Loss: 1.2071 \n",
      "trigger times: 26\n",
      "Epoch: 052, Loss: 1.2590 \n",
      "trigger times: 27\n",
      "Epoch: 053, Loss: 1.2322 \n",
      "trigger times: 28\n",
      "Epoch: 054, Loss: 1.4948 \n",
      "trigger times: 29\n",
      "Epoch: 055, Loss: 1.2075 \n",
      "trigger times: 30\n",
      "Epoch: 056, Loss: 1.2399 \n",
      "trigger times: 31\n",
      "Epoch: 057, Loss: 1.2543 \n",
      "trigger times: 32\n",
      "Epoch: 058, Loss: 1.3556 \n",
      "trigger times: 33\n",
      "Epoch: 059, Loss: 1.2144 \n",
      "trigger times: 34\n",
      "Epoch: 060, Loss: 1.2502 \n",
      "trigger times: 35\n",
      "Epoch: 061, Loss: 1.2437 \n",
      "trigger times: 36\n",
      "Epoch: 062, Loss: 1.2044 \n",
      "trigger times: 37\n",
      "Epoch: 063, Loss: 1.2412 \n",
      "trigger times: 38\n",
      "Epoch: 064, Loss: 1.2172 \n",
      "trigger times: 39\n",
      "Epoch: 065, Loss: 1.2173 \n",
      "trigger times: 40\n",
      "Epoch: 066, Loss: 1.2218 \n",
      "trigger times: 41\n",
      "Epoch: 067, Loss: 1.2153 \n",
      "trigger times: 42\n",
      "Epoch: 068, Loss: 1.2984 \n",
      "trigger times: 43\n",
      "Epoch: 069, Loss: 1.3039 \n",
      "trigger times: 44\n",
      "Epoch: 070, Loss: 1.2546 \n",
      "trigger times: 45\n",
      "Epoch: 071, Loss: 1.2085 \n",
      "trigger times: 46\n",
      "Epoch: 072, Loss: 1.2853 \n",
      "trigger times: 47\n",
      "Epoch: 073, Loss: 1.3142 \n",
      "trigger times: 48\n",
      "Epoch: 074, Loss: 1.2056 \n",
      "trigger times: 49\n",
      "Epoch: 075, Loss: 1.2606 \n",
      "trigger times: 50\n",
      "Epoch: 076, Loss: 1.2521 \n",
      "trigger times: 51\n",
      "Epoch: 077, Loss: 1.2448 \n",
      "trigger times: 52\n",
      "Epoch: 078, Loss: 1.2138 \n",
      "trigger times: 53\n",
      "Epoch: 079, Loss: 1.2557 \n",
      "trigger times: 54\n",
      "Epoch: 080, Loss: 1.3304 \n",
      "trigger times: 55\n",
      "Epoch: 081, Loss: 1.2314 \n",
      "trigger times: 56\n",
      "Epoch: 082, Loss: 1.3384 \n",
      "trigger times: 57\n",
      "Epoch: 083, Loss: 1.2415 \n",
      "trigger times: 58\n",
      "Epoch: 084, Loss: 1.2528 \n",
      "trigger times: 59\n",
      "Epoch: 085, Loss: 1.1994 \n",
      "trigger times: 60\n",
      "Epoch: 086, Loss: 1.3100 \n",
      "trigger times: 61\n",
      "Epoch: 087, Loss: 1.3579 \n",
      "trigger times: 62\n",
      "Epoch: 088, Loss: 1.2531 \n",
      "trigger times: 63\n",
      "Epoch: 089, Loss: 1.2634 \n",
      "trigger times: 64\n",
      "Epoch: 090, Loss: 1.2109 \n",
      "trigger times: 65\n",
      "Epoch: 091, Loss: 1.2193 \n",
      "trigger times: 66\n",
      "Epoch: 092, Loss: 1.2146 \n",
      "trigger times: 67\n",
      "Epoch: 093, Loss: 1.3199 \n",
      "trigger times: 68\n",
      "Epoch: 094, Loss: 1.2027 \n",
      "trigger times: 69\n",
      "Epoch: 095, Loss: 1.2155 \n",
      "trigger times: 70\n",
      "Epoch: 096, Loss: 1.2378 \n",
      "trigger times: 71\n",
      "Epoch: 097, Loss: 1.2158 \n",
      "trigger times: 72\n",
      "Epoch: 098, Loss: 1.2491 \n",
      "trigger times: 73\n",
      "Epoch: 099, Loss: 1.2729 \n",
      "trigger times: 74\n",
      "Epoch: 100, Loss: 1.2151 \n",
      "trigger times: 75\n",
      "Epoch: 101, Loss: 1.2586 \n",
      "trigger times: 76\n",
      "Epoch: 102, Loss: 1.2019 \n",
      "trigger times: 77\n",
      "Epoch: 103, Loss: 1.2155 \n",
      "trigger times: 78\n",
      "Epoch: 104, Loss: 1.3408 \n",
      "trigger times: 79\n",
      "Epoch: 105, Loss: 1.1945 \n",
      "trigger times: 80\n",
      "Epoch: 106, Loss: 1.2194 \n",
      "trigger times: 81\n",
      "Epoch: 107, Loss: 1.2178 \n",
      "trigger times: 82\n",
      "Epoch: 108, Loss: 1.2316 \n",
      "trigger times: 83\n",
      "Epoch: 109, Loss: 1.2110 \n",
      "trigger times: 84\n",
      "Epoch: 110, Loss: 1.2681 \n",
      "trigger times: 85\n",
      "Epoch: 111, Loss: 1.2345 \n",
      "trigger times: 86\n",
      "Epoch: 112, Loss: 1.3038 \n",
      "trigger times: 87\n",
      "Epoch: 113, Loss: 1.1966 \n",
      "trigger times: 88\n",
      "Epoch: 114, Loss: 1.2908 \n",
      "trigger times: 89\n",
      "Epoch: 115, Loss: 1.2765 \n",
      "trigger times: 90\n",
      "Epoch: 116, Loss: 1.3603 \n",
      "trigger times: 91\n",
      "Epoch: 117, Loss: 1.2459 \n",
      "trigger times: 92\n",
      "Epoch: 118, Loss: 1.2548 \n",
      "trigger times: 93\n",
      "Epoch: 119, Loss: 1.2046 \n",
      "trigger times: 94\n",
      "Epoch: 120, Loss: 1.2138 \n",
      "trigger times: 95\n",
      "Epoch: 121, Loss: 1.3665 \n",
      "trigger times: 96\n",
      "Epoch: 122, Loss: 1.2225 \n",
      "trigger times: 97\n",
      "Epoch: 123, Loss: 1.2813 \n",
      "trigger times: 98\n",
      "Epoch: 124, Loss: 1.1842 \n",
      "Epoch: 125, Loss: 1.3330 \n",
      "trigger times: 1\n",
      "Epoch: 126, Loss: 1.1844 \n",
      "trigger times: 2\n",
      "Epoch: 127, Loss: 1.3628 \n",
      "trigger times: 3\n",
      "Epoch: 128, Loss: 1.2010 \n",
      "trigger times: 4\n",
      "Epoch: 129, Loss: 1.2033 \n",
      "trigger times: 5\n",
      "Epoch: 130, Loss: 1.3049 \n",
      "trigger times: 6\n",
      "Epoch: 131, Loss: 1.4596 \n",
      "trigger times: 7\n",
      "Epoch: 132, Loss: 1.2114 \n",
      "trigger times: 8\n",
      "Epoch: 133, Loss: 1.2077 \n",
      "trigger times: 9\n",
      "Epoch: 134, Loss: 1.1995 \n",
      "trigger times: 10\n",
      "Epoch: 135, Loss: 1.1952 \n",
      "trigger times: 11\n",
      "Epoch: 136, Loss: 1.2390 \n",
      "trigger times: 12\n",
      "Epoch: 137, Loss: 1.3197 \n",
      "trigger times: 13\n",
      "Epoch: 138, Loss: 1.3610 \n",
      "trigger times: 14\n",
      "Epoch: 139, Loss: 1.1862 \n",
      "trigger times: 15\n",
      "Epoch: 140, Loss: 1.1884 \n",
      "trigger times: 16\n",
      "Epoch: 141, Loss: 1.2541 \n",
      "trigger times: 17\n",
      "Epoch: 142, Loss: 1.3241 \n",
      "trigger times: 18\n",
      "Epoch: 143, Loss: 1.3323 \n",
      "trigger times: 19\n",
      "Epoch: 144, Loss: 1.2238 \n",
      "trigger times: 20\n",
      "Epoch: 145, Loss: 1.2318 \n",
      "trigger times: 21\n",
      "Epoch: 146, Loss: 1.1909 \n",
      "trigger times: 22\n",
      "Epoch: 147, Loss: 1.2432 \n",
      "trigger times: 23\n",
      "Epoch: 148, Loss: 1.5032 \n",
      "trigger times: 24\n",
      "Epoch: 149, Loss: 1.2121 \n",
      "trigger times: 25\n",
      "Epoch: 150, Loss: 1.2740 \n",
      "trigger times: 26\n",
      "Epoch: 151, Loss: 1.2120 \n",
      "trigger times: 27\n",
      "Epoch: 152, Loss: 1.1846 \n",
      "trigger times: 28\n",
      "Epoch: 153, Loss: 1.2085 \n",
      "trigger times: 29\n",
      "Epoch: 154, Loss: 1.1819 \n",
      "Epoch: 155, Loss: 1.2684 \n",
      "trigger times: 1\n",
      "Epoch: 156, Loss: 1.2138 \n",
      "trigger times: 2\n",
      "Epoch: 157, Loss: 1.2681 \n",
      "trigger times: 3\n",
      "Epoch: 158, Loss: 1.1815 \n",
      "Epoch: 159, Loss: 1.2073 \n",
      "trigger times: 1\n",
      "Epoch: 160, Loss: 1.3054 \n",
      "trigger times: 2\n",
      "Epoch: 161, Loss: 1.3595 \n",
      "trigger times: 3\n",
      "Epoch: 162, Loss: 1.2038 \n",
      "trigger times: 4\n",
      "Epoch: 163, Loss: 1.2242 \n",
      "trigger times: 5\n",
      "Epoch: 164, Loss: 1.1937 \n",
      "trigger times: 6\n",
      "Epoch: 165, Loss: 1.2533 \n",
      "trigger times: 7\n",
      "Epoch: 166, Loss: 1.2750 \n",
      "trigger times: 8\n",
      "Epoch: 167, Loss: 1.1962 \n",
      "trigger times: 9\n",
      "Epoch: 168, Loss: 1.2882 \n",
      "trigger times: 10\n",
      "Epoch: 169, Loss: 1.2381 \n",
      "trigger times: 11\n",
      "Epoch: 170, Loss: 1.2644 \n",
      "trigger times: 12\n",
      "Epoch: 171, Loss: 1.1944 \n",
      "trigger times: 13\n",
      "Epoch: 172, Loss: 1.2954 \n",
      "trigger times: 14\n",
      "Epoch: 173, Loss: 1.3237 \n",
      "trigger times: 15\n",
      "Epoch: 174, Loss: 1.1829 \n",
      "trigger times: 16\n",
      "Epoch: 175, Loss: 1.3742 \n",
      "trigger times: 17\n",
      "Epoch: 176, Loss: 1.2049 \n",
      "trigger times: 18\n",
      "Epoch: 177, Loss: 1.2036 \n",
      "trigger times: 19\n",
      "Epoch: 178, Loss: 1.1920 \n",
      "trigger times: 20\n",
      "Epoch: 179, Loss: 1.3320 \n",
      "trigger times: 21\n",
      "Epoch: 180, Loss: 1.2415 \n",
      "trigger times: 22\n",
      "Epoch: 181, Loss: 1.3243 \n",
      "trigger times: 23\n",
      "Epoch: 182, Loss: 1.2597 \n",
      "trigger times: 24\n",
      "Epoch: 183, Loss: 1.2279 \n",
      "trigger times: 25\n",
      "Epoch: 184, Loss: 1.2023 \n",
      "trigger times: 26\n",
      "Epoch: 185, Loss: 1.2819 \n",
      "trigger times: 27\n",
      "Epoch: 186, Loss: 1.2178 \n",
      "trigger times: 28\n",
      "Epoch: 187, Loss: 1.2418 \n",
      "trigger times: 29\n",
      "Epoch: 188, Loss: 1.2177 \n",
      "trigger times: 30\n",
      "Epoch: 189, Loss: 1.3232 \n",
      "trigger times: 31\n",
      "Epoch: 190, Loss: 1.2964 \n",
      "trigger times: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191, Loss: 1.2460 \n",
      "trigger times: 33\n",
      "Epoch: 192, Loss: 1.2491 \n",
      "trigger times: 34\n",
      "Epoch: 193, Loss: 1.2301 \n",
      "trigger times: 35\n",
      "Epoch: 194, Loss: 1.3075 \n",
      "trigger times: 36\n",
      "Epoch: 195, Loss: 1.2526 \n",
      "trigger times: 37\n",
      "Epoch: 196, Loss: 1.2396 \n",
      "trigger times: 38\n",
      "Epoch: 197, Loss: 1.2483 \n",
      "trigger times: 39\n",
      "Epoch: 198, Loss: 1.2436 \n",
      "trigger times: 40\n",
      "Epoch: 199, Loss: 1.1700 \n",
      "Epoch: 000, Loss: 1.4859 \n",
      "Epoch: 001, Loss: 1.2759 \n",
      "Epoch: 002, Loss: 1.2082 \n",
      "Epoch: 003, Loss: 1.2597 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2177 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2572 \n",
      "trigger times: 3\n",
      "Epoch: 006, Loss: 1.2438 \n",
      "trigger times: 4\n",
      "Epoch: 007, Loss: 1.2260 \n",
      "trigger times: 5\n",
      "Epoch: 008, Loss: 1.2468 \n",
      "trigger times: 6\n",
      "Epoch: 009, Loss: 1.3042 \n",
      "trigger times: 7\n",
      "Epoch: 010, Loss: 1.2503 \n",
      "trigger times: 8\n",
      "Epoch: 011, Loss: 1.2387 \n",
      "trigger times: 9\n",
      "Epoch: 012, Loss: 1.3132 \n",
      "trigger times: 10\n",
      "Epoch: 013, Loss: 1.2321 \n",
      "trigger times: 11\n",
      "Epoch: 014, Loss: 1.2461 \n",
      "trigger times: 12\n",
      "Epoch: 015, Loss: 1.3400 \n",
      "trigger times: 13\n",
      "Epoch: 016, Loss: 1.2981 \n",
      "trigger times: 14\n",
      "Epoch: 017, Loss: 1.2845 \n",
      "trigger times: 15\n",
      "Epoch: 018, Loss: 1.2753 \n",
      "trigger times: 16\n",
      "Epoch: 019, Loss: 1.3084 \n",
      "trigger times: 17\n",
      "Epoch: 020, Loss: 1.3608 \n",
      "trigger times: 18\n",
      "Epoch: 021, Loss: 1.1746 \n",
      "Epoch: 022, Loss: 1.2029 \n",
      "trigger times: 1\n",
      "Epoch: 023, Loss: 1.2613 \n",
      "trigger times: 2\n",
      "Epoch: 024, Loss: 1.2192 \n",
      "trigger times: 3\n",
      "Epoch: 025, Loss: 1.2200 \n",
      "trigger times: 4\n",
      "Epoch: 026, Loss: 1.2342 \n",
      "trigger times: 5\n",
      "Epoch: 027, Loss: 1.2453 \n",
      "trigger times: 6\n",
      "Epoch: 028, Loss: 1.2629 \n",
      "trigger times: 7\n",
      "Epoch: 029, Loss: 1.2203 \n",
      "trigger times: 8\n",
      "Epoch: 030, Loss: 1.2202 \n",
      "trigger times: 9\n",
      "Epoch: 031, Loss: 1.2823 \n",
      "trigger times: 10\n",
      "Epoch: 032, Loss: 1.3141 \n",
      "trigger times: 11\n",
      "Epoch: 033, Loss: 1.2155 \n",
      "trigger times: 12\n",
      "Epoch: 034, Loss: 1.2102 \n",
      "trigger times: 13\n",
      "Epoch: 035, Loss: 1.2857 \n",
      "trigger times: 14\n",
      "Epoch: 036, Loss: 1.2642 \n",
      "trigger times: 15\n",
      "Epoch: 037, Loss: 1.2776 \n",
      "trigger times: 16\n",
      "Epoch: 038, Loss: 1.3984 \n",
      "trigger times: 17\n",
      "Epoch: 039, Loss: 1.1977 \n",
      "trigger times: 18\n",
      "Epoch: 040, Loss: 1.2442 \n",
      "trigger times: 19\n",
      "Epoch: 041, Loss: 1.1701 \n",
      "Epoch: 042, Loss: 1.1824 \n",
      "trigger times: 1\n",
      "Epoch: 043, Loss: 1.1865 \n",
      "trigger times: 2\n",
      "Epoch: 044, Loss: 1.2546 \n",
      "trigger times: 3\n",
      "Epoch: 045, Loss: 1.2175 \n",
      "trigger times: 4\n",
      "Epoch: 046, Loss: 1.2831 \n",
      "trigger times: 5\n",
      "Epoch: 047, Loss: 1.2684 \n",
      "trigger times: 6\n",
      "Epoch: 048, Loss: 4.5672 \n",
      "trigger times: 7\n",
      "Epoch: 049, Loss: 1.2309 \n",
      "trigger times: 8\n",
      "Epoch: 050, Loss: 1.1447 \n",
      "Epoch: 051, Loss: 1.1532 \n",
      "trigger times: 1\n",
      "Epoch: 052, Loss: 1.1225 \n",
      "Epoch: 053, Loss: 1.1123 \n",
      "Epoch: 054, Loss: 1.1381 \n",
      "trigger times: 1\n",
      "Epoch: 055, Loss: 1.1203 \n",
      "trigger times: 2\n",
      "Epoch: 056, Loss: 1.1229 \n",
      "trigger times: 3\n",
      "Epoch: 057, Loss: 1.1174 \n",
      "trigger times: 4\n",
      "Epoch: 058, Loss: 1.1232 \n",
      "trigger times: 5\n",
      "Epoch: 059, Loss: 1.1191 \n",
      "trigger times: 6\n",
      "Epoch: 060, Loss: 1.1351 \n",
      "trigger times: 7\n",
      "Epoch: 061, Loss: 1.1250 \n",
      "trigger times: 8\n",
      "Epoch: 062, Loss: 1.1346 \n",
      "trigger times: 9\n",
      "Epoch: 063, Loss: 1.1365 \n",
      "trigger times: 10\n",
      "Epoch: 064, Loss: 1.1316 \n",
      "trigger times: 11\n",
      "Epoch: 065, Loss: 1.1469 \n",
      "trigger times: 12\n",
      "Epoch: 066, Loss: 1.1498 \n",
      "trigger times: 13\n",
      "Epoch: 067, Loss: 1.1721 \n",
      "trigger times: 14\n",
      "Epoch: 068, Loss: 1.1774 \n",
      "trigger times: 15\n",
      "Epoch: 069, Loss: 1.1561 \n",
      "trigger times: 16\n",
      "Epoch: 070, Loss: 1.1695 \n",
      "trigger times: 17\n",
      "Epoch: 071, Loss: 1.1787 \n",
      "trigger times: 18\n",
      "Epoch: 072, Loss: 1.1692 \n",
      "trigger times: 19\n",
      "Epoch: 073, Loss: 1.2164 \n",
      "trigger times: 20\n",
      "Epoch: 074, Loss: 1.1568 \n",
      "trigger times: 21\n",
      "Epoch: 075, Loss: 1.1438 \n",
      "trigger times: 22\n",
      "Epoch: 076, Loss: 1.1754 \n",
      "trigger times: 23\n",
      "Epoch: 077, Loss: 1.1898 \n",
      "trigger times: 24\n",
      "Epoch: 078, Loss: 1.2051 \n",
      "trigger times: 25\n",
      "Epoch: 079, Loss: 1.1976 \n",
      "trigger times: 26\n",
      "Epoch: 080, Loss: 1.2262 \n",
      "trigger times: 27\n",
      "Epoch: 081, Loss: 1.2158 \n",
      "trigger times: 28\n",
      "Epoch: 082, Loss: 1.2076 \n",
      "trigger times: 29\n",
      "Epoch: 083, Loss: 1.2340 \n",
      "trigger times: 30\n",
      "Epoch: 084, Loss: 1.2313 \n",
      "trigger times: 31\n",
      "Epoch: 085, Loss: 1.3136 \n",
      "trigger times: 32\n",
      "Epoch: 086, Loss: 1.1964 \n",
      "trigger times: 33\n",
      "Epoch: 087, Loss: 1.2009 \n",
      "trigger times: 34\n",
      "Epoch: 088, Loss: 1.2102 \n",
      "trigger times: 35\n",
      "Epoch: 089, Loss: 1.2637 \n",
      "trigger times: 36\n",
      "Epoch: 090, Loss: 1.2022 \n",
      "trigger times: 37\n",
      "Epoch: 091, Loss: 1.2000 \n",
      "trigger times: 38\n",
      "Epoch: 092, Loss: 1.2573 \n",
      "trigger times: 39\n",
      "Epoch: 093, Loss: 1.2064 \n",
      "trigger times: 40\n",
      "Epoch: 094, Loss: 1.2884 \n",
      "trigger times: 41\n",
      "Epoch: 095, Loss: 1.1873 \n",
      "trigger times: 42\n",
      "Epoch: 096, Loss: 1.2393 \n",
      "trigger times: 43\n",
      "Epoch: 097, Loss: 1.2969 \n",
      "trigger times: 44\n",
      "Epoch: 098, Loss: 1.2050 \n",
      "trigger times: 45\n",
      "Epoch: 099, Loss: 1.2988 \n",
      "trigger times: 46\n",
      "Epoch: 100, Loss: 1.1748 \n",
      "trigger times: 47\n",
      "Epoch: 101, Loss: 1.2307 \n",
      "trigger times: 48\n",
      "Epoch: 102, Loss: 1.1984 \n",
      "trigger times: 49\n",
      "Epoch: 103, Loss: 1.2729 \n",
      "trigger times: 50\n",
      "Epoch: 104, Loss: 1.3799 \n",
      "trigger times: 51\n",
      "Epoch: 105, Loss: 1.2073 \n",
      "trigger times: 52\n",
      "Epoch: 106, Loss: 1.2697 \n",
      "trigger times: 53\n",
      "Epoch: 107, Loss: 1.3075 \n",
      "trigger times: 54\n",
      "Epoch: 108, Loss: 1.2325 \n",
      "trigger times: 55\n",
      "Epoch: 109, Loss: 1.2487 \n",
      "trigger times: 56\n",
      "Epoch: 110, Loss: 1.2144 \n",
      "trigger times: 57\n",
      "Epoch: 111, Loss: 1.1487 \n",
      "trigger times: 58\n",
      "Epoch: 112, Loss: 1.2161 \n",
      "trigger times: 59\n",
      "Epoch: 113, Loss: 1.2883 \n",
      "trigger times: 60\n",
      "Epoch: 114, Loss: 1.1935 \n",
      "trigger times: 61\n",
      "Epoch: 115, Loss: 1.2405 \n",
      "trigger times: 62\n",
      "Epoch: 116, Loss: 1.2296 \n",
      "trigger times: 63\n",
      "Epoch: 117, Loss: 1.2785 \n",
      "trigger times: 64\n",
      "Epoch: 118, Loss: 1.3093 \n",
      "trigger times: 65\n",
      "Epoch: 119, Loss: 1.1686 \n",
      "trigger times: 66\n",
      "Epoch: 120, Loss: 1.2207 \n",
      "trigger times: 67\n",
      "Epoch: 121, Loss: 1.2085 \n",
      "trigger times: 68\n",
      "Epoch: 122, Loss: 1.2033 \n",
      "trigger times: 69\n",
      "Epoch: 123, Loss: 1.2943 \n",
      "trigger times: 70\n",
      "Epoch: 124, Loss: 1.2115 \n",
      "trigger times: 71\n",
      "Epoch: 125, Loss: 1.1860 \n",
      "trigger times: 72\n",
      "Epoch: 126, Loss: 1.3159 \n",
      "trigger times: 73\n",
      "Epoch: 127, Loss: 1.2858 \n",
      "trigger times: 74\n",
      "Epoch: 128, Loss: 1.2586 \n",
      "trigger times: 75\n",
      "Epoch: 129, Loss: 1.2222 \n",
      "trigger times: 76\n",
      "Epoch: 130, Loss: 1.1514 \n",
      "trigger times: 77\n",
      "Epoch: 131, Loss: 1.1923 \n",
      "trigger times: 78\n",
      "Epoch: 132, Loss: 1.1994 \n",
      "trigger times: 79\n",
      "Epoch: 133, Loss: 1.1873 \n",
      "trigger times: 80\n",
      "Epoch: 134, Loss: 1.2700 \n",
      "trigger times: 81\n",
      "Epoch: 135, Loss: 1.2742 \n",
      "trigger times: 82\n",
      "Epoch: 136, Loss: 1.2424 \n",
      "trigger times: 83\n",
      "Epoch: 137, Loss: 1.2547 \n",
      "trigger times: 84\n",
      "Epoch: 138, Loss: 1.2225 \n",
      "trigger times: 85\n",
      "Epoch: 139, Loss: 1.2625 \n",
      "trigger times: 86\n",
      "Epoch: 140, Loss: 1.2156 \n",
      "trigger times: 87\n",
      "Epoch: 141, Loss: 1.2074 \n",
      "trigger times: 88\n",
      "Epoch: 142, Loss: 1.3618 \n",
      "trigger times: 89\n",
      "Epoch: 143, Loss: 1.1809 \n",
      "trigger times: 90\n",
      "Epoch: 144, Loss: 1.2293 \n",
      "trigger times: 91\n",
      "Epoch: 145, Loss: 1.3600 \n",
      "trigger times: 92\n",
      "Epoch: 146, Loss: 1.2964 \n",
      "trigger times: 93\n",
      "Epoch: 147, Loss: 1.2140 \n",
      "trigger times: 94\n",
      "Epoch: 148, Loss: 1.2289 \n",
      "trigger times: 95\n",
      "Epoch: 149, Loss: 1.2453 \n",
      "trigger times: 96\n",
      "Epoch: 150, Loss: 1.2478 \n",
      "trigger times: 97\n",
      "Epoch: 151, Loss: 1.1600 \n",
      "trigger times: 98\n",
      "Epoch: 152, Loss: 1.2644 \n",
      "trigger times: 99\n",
      "Epoch: 153, Loss: 1.1525 \n",
      "trigger times: 100\n",
      "Epoch: 154, Loss: 1.2468 \n",
      "trigger times: 101\n",
      "Epoch: 155, Loss: 1.2285 \n",
      "trigger times: 102\n",
      "Epoch: 156, Loss: 1.2467 \n",
      "trigger times: 103\n",
      "Epoch: 157, Loss: 1.1788 \n",
      "trigger times: 104\n",
      "Epoch: 158, Loss: 1.2091 \n",
      "trigger times: 105\n",
      "Epoch: 159, Loss: 1.2154 \n",
      "trigger times: 106\n",
      "Epoch: 160, Loss: 1.2396 \n",
      "trigger times: 107\n",
      "Epoch: 161, Loss: 1.2211 \n",
      "trigger times: 108\n",
      "Epoch: 162, Loss: 1.2733 \n",
      "trigger times: 109\n",
      "Epoch: 163, Loss: 1.3138 \n",
      "trigger times: 110\n",
      "Epoch: 164, Loss: 1.3654 \n",
      "trigger times: 111\n",
      "Epoch: 165, Loss: 1.3148 \n",
      "trigger times: 112\n",
      "Epoch: 166, Loss: 1.2773 \n",
      "trigger times: 113\n",
      "Epoch: 167, Loss: 1.1972 \n",
      "trigger times: 114\n",
      "Epoch: 168, Loss: 1.3053 \n",
      "trigger times: 115\n",
      "Epoch: 169, Loss: 1.2024 \n",
      "trigger times: 116\n",
      "Epoch: 170, Loss: 1.2017 \n",
      "trigger times: 117\n",
      "Epoch: 171, Loss: 1.2318 \n",
      "trigger times: 118\n",
      "Epoch: 172, Loss: 1.1934 \n",
      "trigger times: 119\n",
      "Epoch: 173, Loss: 1.2011 \n",
      "trigger times: 120\n",
      "Epoch: 174, Loss: 1.2793 \n",
      "trigger times: 121\n",
      "Epoch: 175, Loss: 1.2323 \n",
      "trigger times: 122\n",
      "Epoch: 176, Loss: 1.2313 \n",
      "trigger times: 123\n",
      "Epoch: 177, Loss: 1.2194 \n",
      "trigger times: 124\n",
      "Epoch: 178, Loss: 1.1760 \n",
      "trigger times: 125\n",
      "Epoch: 179, Loss: 1.2689 \n",
      "trigger times: 126\n",
      "Epoch: 180, Loss: 1.3317 \n",
      "trigger times: 127\n",
      "Epoch: 181, Loss: 1.2253 \n",
      "trigger times: 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182, Loss: 1.2583 \n",
      "trigger times: 129\n",
      "Epoch: 183, Loss: 1.2254 \n",
      "trigger times: 130\n",
      "Epoch: 184, Loss: 1.2115 \n",
      "trigger times: 131\n",
      "Epoch: 185, Loss: 1.1967 \n",
      "trigger times: 132\n",
      "Epoch: 186, Loss: 1.2260 \n",
      "trigger times: 133\n",
      "Epoch: 187, Loss: 1.1654 \n",
      "trigger times: 134\n",
      "Epoch: 188, Loss: 1.2637 \n",
      "trigger times: 135\n",
      "Epoch: 189, Loss: 1.1812 \n",
      "trigger times: 136\n",
      "Epoch: 190, Loss: 1.2140 \n",
      "trigger times: 137\n",
      "Epoch: 191, Loss: 1.2122 \n",
      "trigger times: 138\n",
      "Epoch: 192, Loss: 1.1755 \n",
      "trigger times: 139\n",
      "Epoch: 193, Loss: 1.2529 \n",
      "trigger times: 140\n",
      "Epoch: 194, Loss: 1.2638 \n",
      "trigger times: 141\n",
      "Epoch: 195, Loss: 1.1976 \n",
      "trigger times: 142\n",
      "Epoch: 196, Loss: 1.1530 \n",
      "trigger times: 143\n",
      "Epoch: 197, Loss: 1.1723 \n",
      "trigger times: 144\n",
      "Epoch: 198, Loss: 1.1929 \n",
      "trigger times: 145\n",
      "Epoch: 199, Loss: 1.2657 \n",
      "trigger times: 146\n",
      "Epoch: 000, Loss: 1.6942 \n",
      "Epoch: 001, Loss: 1.2859 \n",
      "Epoch: 002, Loss: 1.3251 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2722 \n",
      "Epoch: 004, Loss: 1.2297 \n",
      "Epoch: 005, Loss: 1.3154 \n",
      "trigger times: 1\n",
      "Epoch: 006, Loss: 1.2566 \n",
      "trigger times: 2\n",
      "Epoch: 007, Loss: 1.2401 \n",
      "trigger times: 3\n",
      "Epoch: 008, Loss: 1.3198 \n",
      "trigger times: 4\n",
      "Epoch: 009, Loss: 1.2863 \n",
      "trigger times: 5\n",
      "Epoch: 010, Loss: 1.2565 \n",
      "trigger times: 6\n",
      "Epoch: 011, Loss: 1.2809 \n",
      "trigger times: 7\n",
      "Epoch: 012, Loss: 1.2836 \n",
      "trigger times: 8\n",
      "Epoch: 013, Loss: 1.2409 \n",
      "trigger times: 9\n",
      "Epoch: 014, Loss: 1.3311 \n",
      "trigger times: 10\n",
      "Epoch: 015, Loss: 1.2464 \n",
      "trigger times: 11\n",
      "Epoch: 016, Loss: 1.3139 \n",
      "trigger times: 12\n",
      "Epoch: 017, Loss: 1.2744 \n",
      "trigger times: 13\n",
      "Epoch: 018, Loss: 1.3433 \n",
      "trigger times: 14\n",
      "Epoch: 019, Loss: 1.2455 \n",
      "trigger times: 15\n",
      "Epoch: 020, Loss: 1.2703 \n",
      "trigger times: 16\n",
      "Epoch: 021, Loss: 1.3333 \n",
      "trigger times: 17\n",
      "Epoch: 022, Loss: 1.3847 \n",
      "trigger times: 18\n",
      "Epoch: 023, Loss: 1.2739 \n",
      "trigger times: 19\n",
      "Epoch: 024, Loss: 1.2784 \n",
      "trigger times: 20\n",
      "Epoch: 025, Loss: 1.2643 \n",
      "trigger times: 21\n",
      "Epoch: 026, Loss: 1.2931 \n",
      "trigger times: 22\n",
      "Epoch: 027, Loss: 1.2917 \n",
      "trigger times: 23\n",
      "Epoch: 028, Loss: 1.3174 \n",
      "trigger times: 24\n",
      "Epoch: 029, Loss: 1.2295 \n",
      "Epoch: 030, Loss: 1.2092 \n",
      "Epoch: 031, Loss: 5.4994 \n",
      "trigger times: 1\n",
      "Epoch: 032, Loss: 1.4293 \n",
      "trigger times: 2\n",
      "Epoch: 033, Loss: 1.2173 \n",
      "trigger times: 3\n",
      "Epoch: 034, Loss: 1.2026 \n",
      "Epoch: 035, Loss: 1.1771 \n",
      "Epoch: 036, Loss: 1.1600 \n",
      "Epoch: 037, Loss: 1.1486 \n",
      "Epoch: 038, Loss: 1.1538 \n",
      "trigger times: 1\n",
      "Epoch: 039, Loss: 1.1617 \n",
      "trigger times: 2\n",
      "Epoch: 040, Loss: 1.1722 \n",
      "trigger times: 3\n",
      "Epoch: 041, Loss: 1.1367 \n",
      "Epoch: 042, Loss: 1.1750 \n",
      "trigger times: 1\n",
      "Epoch: 043, Loss: 1.1610 \n",
      "trigger times: 2\n",
      "Epoch: 044, Loss: 1.1411 \n",
      "trigger times: 3\n",
      "Epoch: 045, Loss: 1.1330 \n",
      "Epoch: 046, Loss: 1.1565 \n",
      "trigger times: 1\n",
      "Epoch: 047, Loss: 1.1648 \n",
      "trigger times: 2\n",
      "Epoch: 048, Loss: 1.1345 \n",
      "trigger times: 3\n",
      "Epoch: 049, Loss: 1.1419 \n",
      "trigger times: 4\n",
      "Epoch: 050, Loss: 1.1333 \n",
      "trigger times: 5\n",
      "Epoch: 051, Loss: 1.1414 \n",
      "trigger times: 6\n",
      "Epoch: 052, Loss: 1.1368 \n",
      "trigger times: 7\n",
      "Epoch: 053, Loss: 1.1475 \n",
      "trigger times: 8\n",
      "Epoch: 054, Loss: 1.1585 \n",
      "trigger times: 9\n",
      "Epoch: 055, Loss: 1.1545 \n",
      "trigger times: 10\n",
      "Epoch: 056, Loss: 1.1678 \n",
      "trigger times: 11\n",
      "Epoch: 057, Loss: 1.1676 \n",
      "trigger times: 12\n",
      "Epoch: 058, Loss: 1.1679 \n",
      "trigger times: 13\n",
      "Epoch: 059, Loss: 1.2379 \n",
      "trigger times: 14\n",
      "Epoch: 060, Loss: 1.1847 \n",
      "trigger times: 15\n",
      "Epoch: 061, Loss: 1.1912 \n",
      "trigger times: 16\n",
      "Epoch: 062, Loss: 1.1999 \n",
      "trigger times: 17\n",
      "Epoch: 063, Loss: 1.1766 \n",
      "trigger times: 18\n",
      "Epoch: 064, Loss: 1.2271 \n",
      "trigger times: 19\n",
      "Epoch: 065, Loss: 1.2281 \n",
      "trigger times: 20\n",
      "Epoch: 066, Loss: 1.2471 \n",
      "trigger times: 21\n",
      "Epoch: 067, Loss: 1.2322 \n",
      "trigger times: 22\n",
      "Epoch: 068, Loss: 1.2224 \n",
      "trigger times: 23\n",
      "Epoch: 069, Loss: 1.2264 \n",
      "trigger times: 24\n",
      "Epoch: 070, Loss: 1.2018 \n",
      "trigger times: 25\n",
      "Epoch: 071, Loss: 1.2324 \n",
      "trigger times: 26\n",
      "Epoch: 072, Loss: 1.2565 \n",
      "trigger times: 27\n",
      "Epoch: 073, Loss: 1.2271 \n",
      "trigger times: 28\n",
      "Epoch: 074, Loss: 1.2547 \n",
      "trigger times: 29\n",
      "Epoch: 075, Loss: 1.2927 \n",
      "trigger times: 30\n",
      "Epoch: 076, Loss: 1.2237 \n",
      "trigger times: 31\n",
      "Epoch: 077, Loss: 1.2586 \n",
      "trigger times: 32\n",
      "Epoch: 078, Loss: 1.2527 \n",
      "trigger times: 33\n",
      "Epoch: 079, Loss: 1.2214 \n",
      "trigger times: 34\n",
      "Epoch: 080, Loss: 1.2263 \n",
      "trigger times: 35\n",
      "Epoch: 081, Loss: 1.2937 \n",
      "trigger times: 36\n",
      "Epoch: 082, Loss: 1.2613 \n",
      "trigger times: 37\n",
      "Epoch: 083, Loss: 1.2952 \n",
      "trigger times: 38\n",
      "Epoch: 084, Loss: 1.3237 \n",
      "trigger times: 39\n",
      "Epoch: 085, Loss: 1.1848 \n",
      "trigger times: 40\n",
      "Epoch: 086, Loss: 1.2081 \n",
      "trigger times: 41\n",
      "Epoch: 087, Loss: 1.2791 \n",
      "trigger times: 42\n",
      "Epoch: 088, Loss: 1.2530 \n",
      "trigger times: 43\n",
      "Epoch: 089, Loss: 1.5124 \n",
      "trigger times: 44\n",
      "Epoch: 090, Loss: 1.2021 \n",
      "trigger times: 45\n",
      "Epoch: 091, Loss: 1.2056 \n",
      "trigger times: 46\n",
      "Epoch: 092, Loss: 1.2318 \n",
      "trigger times: 47\n",
      "Epoch: 093, Loss: 1.2343 \n",
      "trigger times: 48\n",
      "Epoch: 094, Loss: 1.2427 \n",
      "trigger times: 49\n",
      "Epoch: 095, Loss: 1.1964 \n",
      "trigger times: 50\n",
      "Epoch: 096, Loss: 1.1882 \n",
      "trigger times: 51\n",
      "Epoch: 097, Loss: 1.2266 \n",
      "trigger times: 52\n",
      "Epoch: 098, Loss: 1.2606 \n",
      "trigger times: 53\n",
      "Epoch: 099, Loss: 1.2297 \n",
      "trigger times: 54\n",
      "Epoch: 100, Loss: 1.2283 \n",
      "trigger times: 55\n",
      "Epoch: 101, Loss: 1.2414 \n",
      "trigger times: 56\n",
      "Epoch: 102, Loss: 1.2505 \n",
      "trigger times: 57\n",
      "Epoch: 103, Loss: 1.2472 \n",
      "trigger times: 58\n",
      "Epoch: 104, Loss: 1.2215 \n",
      "trigger times: 59\n",
      "Epoch: 105, Loss: 1.2363 \n",
      "trigger times: 60\n",
      "Epoch: 106, Loss: 1.2544 \n",
      "trigger times: 61\n",
      "Epoch: 107, Loss: 1.2316 \n",
      "trigger times: 62\n",
      "Epoch: 108, Loss: 1.2569 \n",
      "trigger times: 63\n",
      "Epoch: 109, Loss: 1.2449 \n",
      "trigger times: 64\n",
      "Epoch: 110, Loss: 1.2394 \n",
      "trigger times: 65\n",
      "Epoch: 111, Loss: 1.2052 \n",
      "trigger times: 66\n",
      "Epoch: 112, Loss: 1.2029 \n",
      "trigger times: 67\n",
      "Epoch: 113, Loss: 1.2532 \n",
      "trigger times: 68\n",
      "Epoch: 114, Loss: 1.2579 \n",
      "trigger times: 69\n",
      "Epoch: 115, Loss: 1.2677 \n",
      "trigger times: 70\n",
      "Epoch: 116, Loss: 1.2589 \n",
      "trigger times: 71\n",
      "Epoch: 117, Loss: 1.3369 \n",
      "trigger times: 72\n",
      "Epoch: 118, Loss: 1.2513 \n",
      "trigger times: 73\n",
      "Epoch: 119, Loss: 4.1795 \n",
      "trigger times: 74\n",
      "Epoch: 120, Loss: 1.7002 \n",
      "trigger times: 75\n",
      "Epoch: 121, Loss: 1.1662 \n",
      "trigger times: 76\n",
      "Epoch: 122, Loss: 1.1314 \n",
      "Epoch: 123, Loss: 1.1172 \n",
      "Epoch: 124, Loss: 1.1545 \n",
      "trigger times: 1\n",
      "Epoch: 125, Loss: 1.1129 \n",
      "Epoch: 126, Loss: 1.1083 \n",
      "Epoch: 127, Loss: 1.1408 \n",
      "trigger times: 1\n",
      "Epoch: 128, Loss: 1.1063 \n",
      "Epoch: 129, Loss: 1.1321 \n",
      "trigger times: 1\n",
      "Epoch: 130, Loss: 1.1139 \n",
      "trigger times: 2\n",
      "Epoch: 131, Loss: 1.1362 \n",
      "trigger times: 3\n",
      "Epoch: 132, Loss: 1.1201 \n",
      "trigger times: 4\n",
      "Epoch: 133, Loss: 1.1218 \n",
      "trigger times: 5\n",
      "Epoch: 134, Loss: 1.1165 \n",
      "trigger times: 6\n",
      "Epoch: 135, Loss: 1.1260 \n",
      "trigger times: 7\n",
      "Epoch: 136, Loss: 1.1494 \n",
      "trigger times: 8\n",
      "Epoch: 137, Loss: 1.1376 \n",
      "trigger times: 9\n",
      "Epoch: 138, Loss: 1.1833 \n",
      "trigger times: 10\n",
      "Epoch: 139, Loss: 1.1713 \n",
      "trigger times: 11\n",
      "Epoch: 140, Loss: 1.1751 \n",
      "trigger times: 12\n",
      "Epoch: 141, Loss: 1.1891 \n",
      "trigger times: 13\n",
      "Epoch: 142, Loss: 1.2241 \n",
      "trigger times: 14\n",
      "Epoch: 143, Loss: 1.1337 \n",
      "trigger times: 15\n",
      "Epoch: 144, Loss: 1.1844 \n",
      "trigger times: 16\n",
      "Epoch: 145, Loss: 1.1853 \n",
      "trigger times: 17\n",
      "Epoch: 146, Loss: 1.1718 \n",
      "trigger times: 18\n",
      "Epoch: 147, Loss: 1.1730 \n",
      "trigger times: 19\n",
      "Epoch: 148, Loss: 1.1655 \n",
      "trigger times: 20\n",
      "Epoch: 149, Loss: 1.1800 \n",
      "trigger times: 21\n",
      "Epoch: 150, Loss: 1.2188 \n",
      "trigger times: 22\n",
      "Epoch: 151, Loss: 1.1872 \n",
      "trigger times: 23\n",
      "Epoch: 152, Loss: 1.2855 \n",
      "trigger times: 24\n",
      "Epoch: 153, Loss: 1.1807 \n",
      "trigger times: 25\n",
      "Epoch: 154, Loss: 1.2073 \n",
      "trigger times: 26\n",
      "Epoch: 155, Loss: 1.2054 \n",
      "trigger times: 27\n",
      "Epoch: 156, Loss: 1.2449 \n",
      "trigger times: 28\n",
      "Epoch: 157, Loss: 1.2333 \n",
      "trigger times: 29\n",
      "Epoch: 158, Loss: 1.2327 \n",
      "trigger times: 30\n",
      "Epoch: 159, Loss: 1.1757 \n",
      "trigger times: 31\n",
      "Epoch: 160, Loss: 1.1946 \n",
      "trigger times: 32\n",
      "Epoch: 161, Loss: 1.2191 \n",
      "trigger times: 33\n",
      "Epoch: 162, Loss: 1.2444 \n",
      "trigger times: 34\n",
      "Epoch: 163, Loss: 1.2354 \n",
      "trigger times: 35\n",
      "Epoch: 164, Loss: 1.2130 \n",
      "trigger times: 36\n",
      "Epoch: 165, Loss: 1.3428 \n",
      "trigger times: 37\n",
      "Epoch: 166, Loss: 1.2244 \n",
      "trigger times: 38\n",
      "Epoch: 167, Loss: 1.2653 \n",
      "trigger times: 39\n",
      "Epoch: 168, Loss: 1.2089 \n",
      "trigger times: 40\n",
      "Epoch: 169, Loss: 1.2911 \n",
      "trigger times: 41\n",
      "Epoch: 170, Loss: 1.2547 \n",
      "trigger times: 42\n",
      "Epoch: 171, Loss: 1.2179 \n",
      "trigger times: 43\n",
      "Epoch: 172, Loss: 1.1959 \n",
      "trigger times: 44\n",
      "Epoch: 173, Loss: 1.2759 \n",
      "trigger times: 45\n",
      "Epoch: 174, Loss: 1.2481 \n",
      "trigger times: 46\n",
      "Epoch: 175, Loss: 1.1893 \n",
      "trigger times: 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176, Loss: 1.2364 \n",
      "trigger times: 48\n",
      "Epoch: 177, Loss: 1.1735 \n",
      "trigger times: 49\n",
      "Epoch: 178, Loss: 1.2911 \n",
      "trigger times: 50\n",
      "Epoch: 179, Loss: 1.2321 \n",
      "trigger times: 51\n",
      "Epoch: 180, Loss: 1.2125 \n",
      "trigger times: 52\n",
      "Epoch: 181, Loss: 1.2411 \n",
      "trigger times: 53\n",
      "Epoch: 182, Loss: 1.2293 \n",
      "trigger times: 54\n",
      "Epoch: 183, Loss: 1.3161 \n",
      "trigger times: 55\n",
      "Epoch: 184, Loss: 1.2011 \n",
      "trigger times: 56\n",
      "Epoch: 185, Loss: 1.2056 \n",
      "trigger times: 57\n",
      "Epoch: 186, Loss: 1.2372 \n",
      "trigger times: 58\n",
      "Epoch: 187, Loss: 1.2567 \n",
      "trigger times: 59\n",
      "Epoch: 188, Loss: 1.2078 \n",
      "trigger times: 60\n",
      "Epoch: 189, Loss: 1.2373 \n",
      "trigger times: 61\n",
      "Epoch: 190, Loss: 1.3177 \n",
      "trigger times: 62\n",
      "Epoch: 191, Loss: 1.2299 \n",
      "trigger times: 63\n",
      "Epoch: 192, Loss: 1.2038 \n",
      "trigger times: 64\n",
      "Epoch: 193, Loss: 1.2043 \n",
      "trigger times: 65\n",
      "Epoch: 194, Loss: 1.2807 \n",
      "trigger times: 66\n",
      "Epoch: 195, Loss: 1.3004 \n",
      "trigger times: 67\n",
      "Epoch: 196, Loss: 1.2810 \n",
      "trigger times: 68\n",
      "Epoch: 197, Loss: 1.2969 \n",
      "trigger times: 69\n",
      "Epoch: 198, Loss: 1.2031 \n",
      "trigger times: 70\n",
      "Epoch: 199, Loss: 1.2295 \n",
      "trigger times: 71\n",
      "Epoch: 000, Loss: 1.5526 \n",
      "Epoch: 001, Loss: 1.2097 \n",
      "Epoch: 002, Loss: 1.2306 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2064 \n",
      "Epoch: 004, Loss: 1.2366 \n",
      "trigger times: 1\n",
      "Epoch: 005, Loss: 1.1957 \n",
      "Epoch: 006, Loss: 1.2072 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.2964 \n",
      "trigger times: 2\n",
      "Epoch: 008, Loss: 1.2450 \n",
      "trigger times: 3\n",
      "Epoch: 009, Loss: 1.3052 \n",
      "trigger times: 4\n",
      "Epoch: 010, Loss: 1.3791 \n",
      "trigger times: 5\n",
      "Epoch: 011, Loss: 1.2373 \n",
      "trigger times: 6\n",
      "Epoch: 012, Loss: 1.2530 \n",
      "trigger times: 7\n",
      "Epoch: 013, Loss: 1.2340 \n",
      "trigger times: 8\n",
      "Epoch: 014, Loss: 1.2213 \n",
      "trigger times: 9\n",
      "Epoch: 015, Loss: 1.2023 \n",
      "trigger times: 10\n",
      "Epoch: 016, Loss: 1.2408 \n",
      "trigger times: 11\n",
      "Epoch: 017, Loss: 1.2349 \n",
      "trigger times: 12\n",
      "Epoch: 018, Loss: 1.3543 \n",
      "trigger times: 13\n",
      "Epoch: 019, Loss: 1.2224 \n",
      "trigger times: 14\n",
      "Epoch: 020, Loss: 1.6205 \n",
      "trigger times: 15\n",
      "Epoch: 021, Loss: 1.2009 \n",
      "trigger times: 16\n",
      "Epoch: 022, Loss: 1.1749 \n",
      "Epoch: 023, Loss: 1.1904 \n",
      "trigger times: 1\n",
      "Epoch: 024, Loss: 1.1871 \n",
      "trigger times: 2\n",
      "Epoch: 025, Loss: 1.2102 \n",
      "trigger times: 3\n",
      "Epoch: 026, Loss: 1.2150 \n",
      "trigger times: 4\n",
      "Epoch: 027, Loss: 1.1959 \n",
      "trigger times: 5\n",
      "Epoch: 028, Loss: 1.2411 \n",
      "trigger times: 6\n",
      "Epoch: 029, Loss: 1.2545 \n",
      "trigger times: 7\n",
      "Epoch: 030, Loss: 1.2732 \n",
      "trigger times: 8\n",
      "Epoch: 031, Loss: 1.2451 \n",
      "trigger times: 9\n",
      "Epoch: 032, Loss: 1.3048 \n",
      "trigger times: 10\n",
      "Epoch: 033, Loss: 1.2186 \n",
      "trigger times: 11\n",
      "Epoch: 034, Loss: 1.2196 \n",
      "trigger times: 12\n",
      "Epoch: 035, Loss: 1.2146 \n",
      "trigger times: 13\n",
      "Epoch: 036, Loss: 1.2663 \n",
      "trigger times: 14\n",
      "Epoch: 037, Loss: 1.3199 \n",
      "trigger times: 15\n",
      "Epoch: 038, Loss: 1.2217 \n",
      "trigger times: 16\n",
      "Epoch: 039, Loss: 1.2257 \n",
      "trigger times: 17\n",
      "Epoch: 040, Loss: 1.2697 \n",
      "trigger times: 18\n",
      "Epoch: 041, Loss: 1.2601 \n",
      "trigger times: 19\n",
      "Epoch: 042, Loss: 1.2441 \n",
      "trigger times: 20\n",
      "Epoch: 043, Loss: 1.2404 \n",
      "trigger times: 21\n",
      "Epoch: 044, Loss: 1.2178 \n",
      "trigger times: 22\n",
      "Epoch: 045, Loss: 1.2525 \n",
      "trigger times: 23\n",
      "Epoch: 046, Loss: 1.2255 \n",
      "trigger times: 24\n",
      "Epoch: 047, Loss: 1.2919 \n",
      "trigger times: 25\n",
      "Epoch: 048, Loss: 1.2462 \n",
      "trigger times: 26\n",
      "Epoch: 049, Loss: 1.2414 \n",
      "trigger times: 27\n",
      "Epoch: 050, Loss: 1.2984 \n",
      "trigger times: 28\n",
      "Epoch: 051, Loss: 1.3207 \n",
      "trigger times: 29\n",
      "Epoch: 052, Loss: 1.2002 \n",
      "trigger times: 30\n",
      "Epoch: 053, Loss: 1.2280 \n",
      "trigger times: 31\n",
      "Epoch: 054, Loss: 1.2809 \n",
      "trigger times: 32\n",
      "Epoch: 055, Loss: 1.2483 \n",
      "trigger times: 33\n",
      "Epoch: 056, Loss: 1.2357 \n",
      "trigger times: 34\n",
      "Epoch: 057, Loss: 1.4457 \n",
      "trigger times: 35\n",
      "Epoch: 058, Loss: 1.2089 \n",
      "trigger times: 36\n",
      "Epoch: 059, Loss: 1.2251 \n",
      "trigger times: 37\n",
      "Epoch: 060, Loss: 1.2120 \n",
      "trigger times: 38\n",
      "Epoch: 061, Loss: 1.2563 \n",
      "trigger times: 39\n",
      "Epoch: 062, Loss: 1.1621 \n",
      "Epoch: 063, Loss: 1.2225 \n",
      "trigger times: 1\n",
      "Epoch: 064, Loss: 1.2608 \n",
      "trigger times: 2\n",
      "Epoch: 065, Loss: 1.2093 \n",
      "trigger times: 3\n",
      "Epoch: 066, Loss: 1.1779 \n",
      "trigger times: 4\n",
      "Epoch: 067, Loss: 1.3155 \n",
      "trigger times: 5\n",
      "Epoch: 068, Loss: 1.2742 \n",
      "trigger times: 6\n",
      "Epoch: 069, Loss: 1.2196 \n",
      "trigger times: 7\n",
      "Epoch: 070, Loss: 1.2472 \n",
      "trigger times: 8\n",
      "Epoch: 071, Loss: 1.2250 \n",
      "trigger times: 9\n",
      "Epoch: 072, Loss: 1.2249 \n",
      "trigger times: 10\n",
      "Epoch: 073, Loss: 1.2207 \n",
      "trigger times: 11\n",
      "Epoch: 074, Loss: 1.3083 \n",
      "trigger times: 12\n",
      "Epoch: 075, Loss: 1.1970 \n",
      "trigger times: 13\n",
      "Epoch: 076, Loss: 1.2263 \n",
      "trigger times: 14\n",
      "Epoch: 077, Loss: 1.2353 \n",
      "trigger times: 15\n",
      "Epoch: 078, Loss: 1.2226 \n",
      "trigger times: 16\n",
      "Epoch: 079, Loss: 1.2398 \n",
      "trigger times: 17\n",
      "Epoch: 080, Loss: 1.2301 \n",
      "trigger times: 18\n",
      "Epoch: 081, Loss: 1.2605 \n",
      "trigger times: 19\n",
      "Epoch: 082, Loss: 1.2223 \n",
      "trigger times: 20\n",
      "Epoch: 083, Loss: 1.2398 \n",
      "trigger times: 21\n",
      "Epoch: 084, Loss: 1.3292 \n",
      "trigger times: 22\n",
      "Epoch: 085, Loss: 1.2460 \n",
      "trigger times: 23\n",
      "Epoch: 086, Loss: 1.2003 \n",
      "trigger times: 24\n",
      "Epoch: 087, Loss: 1.2086 \n",
      "trigger times: 25\n",
      "Epoch: 088, Loss: 1.2965 \n",
      "trigger times: 26\n",
      "Epoch: 089, Loss: 1.2243 \n",
      "trigger times: 27\n",
      "Epoch: 090, Loss: 1.2717 \n",
      "trigger times: 28\n",
      "Epoch: 091, Loss: 1.2149 \n",
      "trigger times: 29\n",
      "Epoch: 092, Loss: 1.2334 \n",
      "trigger times: 30\n",
      "Epoch: 093, Loss: 1.1881 \n",
      "trigger times: 31\n",
      "Epoch: 094, Loss: 1.2632 \n",
      "trigger times: 32\n",
      "Epoch: 095, Loss: 1.2331 \n",
      "trigger times: 33\n",
      "Epoch: 096, Loss: 1.2576 \n",
      "trigger times: 34\n",
      "Epoch: 097, Loss: 1.2159 \n",
      "trigger times: 35\n",
      "Epoch: 098, Loss: 1.4862 \n",
      "trigger times: 36\n",
      "Epoch: 099, Loss: 1.2030 \n",
      "trigger times: 37\n",
      "Epoch: 100, Loss: 1.1951 \n",
      "trigger times: 38\n",
      "Epoch: 101, Loss: 1.1839 \n",
      "trigger times: 39\n",
      "Epoch: 102, Loss: 1.2586 \n",
      "trigger times: 40\n",
      "Epoch: 103, Loss: 1.2027 \n",
      "trigger times: 41\n",
      "Epoch: 104, Loss: 1.2328 \n",
      "trigger times: 42\n",
      "Epoch: 105, Loss: 1.2653 \n",
      "trigger times: 43\n",
      "Epoch: 106, Loss: 1.2056 \n",
      "trigger times: 44\n",
      "Epoch: 107, Loss: 1.2192 \n",
      "trigger times: 45\n",
      "Epoch: 108, Loss: 1.3924 \n",
      "trigger times: 46\n",
      "Epoch: 109, Loss: 1.2578 \n",
      "trigger times: 47\n",
      "Epoch: 110, Loss: 1.2429 \n",
      "trigger times: 48\n",
      "Epoch: 111, Loss: 1.2475 \n",
      "trigger times: 49\n",
      "Epoch: 112, Loss: 1.2423 \n",
      "trigger times: 50\n",
      "Epoch: 113, Loss: 1.2149 \n",
      "trigger times: 51\n",
      "Epoch: 114, Loss: 1.2636 \n",
      "trigger times: 52\n",
      "Epoch: 115, Loss: 1.2904 \n",
      "trigger times: 53\n",
      "Epoch: 116, Loss: 1.2316 \n",
      "trigger times: 54\n",
      "Epoch: 117, Loss: 1.2199 \n",
      "trigger times: 55\n",
      "Epoch: 118, Loss: 1.2087 \n",
      "trigger times: 56\n",
      "Epoch: 119, Loss: 1.2219 \n",
      "trigger times: 57\n",
      "Epoch: 120, Loss: 1.2273 \n",
      "trigger times: 58\n",
      "Epoch: 121, Loss: 1.2178 \n",
      "trigger times: 59\n",
      "Epoch: 122, Loss: 1.3558 \n",
      "trigger times: 60\n",
      "Epoch: 123, Loss: 1.2150 \n",
      "trigger times: 61\n",
      "Epoch: 124, Loss: 1.2393 \n",
      "trigger times: 62\n",
      "Epoch: 125, Loss: 1.2072 \n",
      "trigger times: 63\n",
      "Epoch: 126, Loss: 1.2757 \n",
      "trigger times: 64\n",
      "Epoch: 127, Loss: 1.2271 \n",
      "trigger times: 65\n",
      "Epoch: 128, Loss: 1.2262 \n",
      "trigger times: 66\n",
      "Epoch: 129, Loss: 1.2333 \n",
      "trigger times: 67\n",
      "Epoch: 130, Loss: 1.2051 \n",
      "trigger times: 68\n",
      "Epoch: 131, Loss: 1.2750 \n",
      "trigger times: 69\n",
      "Epoch: 132, Loss: 1.2274 \n",
      "trigger times: 70\n",
      "Epoch: 133, Loss: 1.1908 \n",
      "trigger times: 71\n",
      "Epoch: 134, Loss: 1.2616 \n",
      "trigger times: 72\n",
      "Epoch: 135, Loss: 1.2134 \n",
      "trigger times: 73\n",
      "Epoch: 136, Loss: 1.2329 \n",
      "trigger times: 74\n",
      "Epoch: 137, Loss: 1.2235 \n",
      "trigger times: 75\n",
      "Epoch: 138, Loss: 1.2205 \n",
      "trigger times: 76\n",
      "Epoch: 139, Loss: 1.2522 \n",
      "trigger times: 77\n",
      "Epoch: 140, Loss: 1.2756 \n",
      "trigger times: 78\n",
      "Epoch: 141, Loss: 1.3242 \n",
      "trigger times: 79\n",
      "Epoch: 142, Loss: 1.2472 \n",
      "trigger times: 80\n",
      "Epoch: 143, Loss: 1.2762 \n",
      "trigger times: 81\n",
      "Epoch: 144, Loss: 1.2040 \n",
      "trigger times: 82\n",
      "Epoch: 145, Loss: 1.2922 \n",
      "trigger times: 83\n",
      "Epoch: 146, Loss: 1.2006 \n",
      "trigger times: 84\n",
      "Epoch: 147, Loss: 1.2893 \n",
      "trigger times: 85\n",
      "Epoch: 148, Loss: 1.1775 \n",
      "trigger times: 86\n",
      "Epoch: 149, Loss: 1.3037 \n",
      "trigger times: 87\n",
      "Epoch: 150, Loss: 1.2134 \n",
      "trigger times: 88\n",
      "Epoch: 151, Loss: 1.2117 \n",
      "trigger times: 89\n",
      "Epoch: 152, Loss: 1.1686 \n",
      "trigger times: 90\n",
      "Epoch: 153, Loss: 1.2203 \n",
      "trigger times: 91\n",
      "Epoch: 154, Loss: 1.3165 \n",
      "trigger times: 92\n",
      "Epoch: 155, Loss: 1.2471 \n",
      "trigger times: 93\n",
      "Epoch: 156, Loss: 1.2454 \n",
      "trigger times: 94\n",
      "Epoch: 157, Loss: 1.1675 \n",
      "trigger times: 95\n",
      "Epoch: 158, Loss: 1.1826 \n",
      "trigger times: 96\n",
      "Epoch: 159, Loss: 1.2735 \n",
      "trigger times: 97\n",
      "Epoch: 160, Loss: 1.2372 \n",
      "trigger times: 98\n",
      "Epoch: 161, Loss: 1.1868 \n",
      "trigger times: 99\n",
      "Epoch: 162, Loss: 1.1828 \n",
      "trigger times: 100\n",
      "Epoch: 163, Loss: 1.2728 \n",
      "trigger times: 101\n",
      "Epoch: 164, Loss: 1.2390 \n",
      "trigger times: 102\n",
      "Epoch: 165, Loss: 1.2514 \n",
      "trigger times: 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss: 1.2802 \n",
      "trigger times: 104\n",
      "Epoch: 167, Loss: 1.2214 \n",
      "trigger times: 105\n",
      "Epoch: 168, Loss: 1.2034 \n",
      "trigger times: 106\n",
      "Epoch: 169, Loss: 1.2304 \n",
      "trigger times: 107\n",
      "Epoch: 170, Loss: 1.3291 \n",
      "trigger times: 108\n",
      "Epoch: 171, Loss: 1.3174 \n",
      "trigger times: 109\n",
      "Epoch: 172, Loss: 1.2337 \n",
      "trigger times: 110\n",
      "Epoch: 173, Loss: 1.2949 \n",
      "trigger times: 111\n",
      "Epoch: 174, Loss: 1.2201 \n",
      "trigger times: 112\n",
      "Epoch: 175, Loss: 1.1579 \n",
      "Epoch: 176, Loss: 1.1735 \n",
      "trigger times: 1\n",
      "Epoch: 177, Loss: 1.2030 \n",
      "trigger times: 2\n",
      "Epoch: 178, Loss: 1.2454 \n",
      "trigger times: 3\n",
      "Epoch: 179, Loss: 1.2225 \n",
      "trigger times: 4\n",
      "Epoch: 180, Loss: 1.2142 \n",
      "trigger times: 5\n",
      "Epoch: 181, Loss: 1.2871 \n",
      "trigger times: 6\n",
      "Epoch: 182, Loss: 1.2477 \n",
      "trigger times: 7\n",
      "Epoch: 183, Loss: 1.1874 \n",
      "trigger times: 8\n",
      "Epoch: 184, Loss: 1.3012 \n",
      "trigger times: 9\n",
      "Epoch: 185, Loss: 1.2956 \n",
      "trigger times: 10\n",
      "Epoch: 186, Loss: 1.2082 \n",
      "trigger times: 11\n",
      "Epoch: 187, Loss: 1.2187 \n",
      "trigger times: 12\n",
      "Epoch: 188, Loss: 1.1994 \n",
      "trigger times: 13\n",
      "Epoch: 189, Loss: 1.2367 \n",
      "trigger times: 14\n",
      "Epoch: 190, Loss: 1.2539 \n",
      "trigger times: 15\n",
      "Epoch: 191, Loss: 1.2222 \n",
      "trigger times: 16\n",
      "Epoch: 192, Loss: 1.1939 \n",
      "trigger times: 17\n",
      "Epoch: 193, Loss: 1.2258 \n",
      "trigger times: 18\n",
      "Epoch: 194, Loss: 1.2338 \n",
      "trigger times: 19\n",
      "Epoch: 195, Loss: 1.2483 \n",
      "trigger times: 20\n",
      "Epoch: 196, Loss: 1.2392 \n",
      "trigger times: 21\n",
      "Epoch: 197, Loss: 1.3256 \n",
      "trigger times: 22\n",
      "Epoch: 198, Loss: 1.2683 \n",
      "trigger times: 23\n",
      "Epoch: 199, Loss: 1.3683 \n",
      "trigger times: 24\n",
      "Epoch: 000, Loss: 1.5499 \n",
      "Epoch: 001, Loss: 1.2406 \n",
      "Epoch: 002, Loss: 1.2517 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.1850 \n",
      "Epoch: 004, Loss: 1.3004 \n",
      "trigger times: 1\n",
      "Epoch: 005, Loss: 1.2911 \n",
      "trigger times: 2\n",
      "Epoch: 006, Loss: 1.3273 \n",
      "trigger times: 3\n",
      "Epoch: 007, Loss: 1.2612 \n",
      "trigger times: 4\n",
      "Epoch: 008, Loss: 1.2212 \n",
      "trigger times: 5\n",
      "Epoch: 009, Loss: 1.1918 \n",
      "trigger times: 6\n",
      "Epoch: 010, Loss: 1.2112 \n",
      "trigger times: 7\n",
      "Epoch: 011, Loss: 1.2452 \n",
      "trigger times: 8\n",
      "Epoch: 012, Loss: 1.2315 \n",
      "trigger times: 9\n",
      "Epoch: 013, Loss: 1.2617 \n",
      "trigger times: 10\n",
      "Epoch: 014, Loss: 1.3070 \n",
      "trigger times: 11\n",
      "Epoch: 015, Loss: 1.2377 \n",
      "trigger times: 12\n",
      "Epoch: 016, Loss: 1.2930 \n",
      "trigger times: 13\n",
      "Epoch: 017, Loss: 1.2453 \n",
      "trigger times: 14\n",
      "Epoch: 018, Loss: 1.1984 \n",
      "trigger times: 15\n",
      "Epoch: 019, Loss: 1.2245 \n",
      "trigger times: 16\n",
      "Epoch: 020, Loss: 3.5118 \n",
      "trigger times: 17\n",
      "Epoch: 021, Loss: 1.1887 \n",
      "trigger times: 18\n",
      "Epoch: 022, Loss: 1.1522 \n",
      "Epoch: 023, Loss: 1.1256 \n",
      "Epoch: 024, Loss: 1.1220 \n",
      "Epoch: 025, Loss: 1.1440 \n",
      "trigger times: 1\n",
      "Epoch: 026, Loss: 1.1562 \n",
      "trigger times: 2\n",
      "Epoch: 027, Loss: 1.1392 \n",
      "trigger times: 3\n",
      "Epoch: 028, Loss: 1.1561 \n",
      "trigger times: 4\n",
      "Epoch: 029, Loss: 1.1423 \n",
      "trigger times: 5\n",
      "Epoch: 030, Loss: 1.1253 \n",
      "trigger times: 6\n",
      "Epoch: 031, Loss: 1.1479 \n",
      "trigger times: 7\n",
      "Epoch: 032, Loss: 1.1400 \n",
      "trigger times: 8\n",
      "Epoch: 033, Loss: 1.1777 \n",
      "trigger times: 9\n",
      "Epoch: 034, Loss: 1.1439 \n",
      "trigger times: 10\n",
      "Epoch: 035, Loss: 1.1425 \n",
      "trigger times: 11\n",
      "Epoch: 036, Loss: 1.1716 \n",
      "trigger times: 12\n",
      "Epoch: 037, Loss: 1.2131 \n",
      "trigger times: 13\n",
      "Epoch: 038, Loss: 1.1898 \n",
      "trigger times: 14\n",
      "Epoch: 039, Loss: 1.1971 \n",
      "trigger times: 15\n",
      "Epoch: 040, Loss: 1.1502 \n",
      "trigger times: 16\n",
      "Epoch: 041, Loss: 1.1813 \n",
      "trigger times: 17\n",
      "Epoch: 042, Loss: 1.1831 \n",
      "trigger times: 18\n",
      "Epoch: 043, Loss: 1.2011 \n",
      "trigger times: 19\n",
      "Epoch: 044, Loss: 1.1889 \n",
      "trigger times: 20\n",
      "Epoch: 045, Loss: 1.2081 \n",
      "trigger times: 21\n",
      "Epoch: 046, Loss: 1.2145 \n",
      "trigger times: 22\n",
      "Epoch: 047, Loss: 1.1892 \n",
      "trigger times: 23\n",
      "Epoch: 048, Loss: 1.2057 \n",
      "trigger times: 24\n",
      "Epoch: 049, Loss: 1.2093 \n",
      "trigger times: 25\n",
      "Epoch: 050, Loss: 1.2723 \n",
      "trigger times: 26\n",
      "Epoch: 051, Loss: 1.1818 \n",
      "trigger times: 27\n",
      "Epoch: 052, Loss: 1.2658 \n",
      "trigger times: 28\n",
      "Epoch: 053, Loss: 1.2182 \n",
      "trigger times: 29\n",
      "Epoch: 054, Loss: 1.2735 \n",
      "trigger times: 30\n",
      "Epoch: 055, Loss: 1.2730 \n",
      "trigger times: 31\n",
      "Epoch: 056, Loss: 1.2730 \n",
      "trigger times: 32\n",
      "Epoch: 057, Loss: 1.2726 \n",
      "trigger times: 33\n",
      "Epoch: 058, Loss: 1.2431 \n",
      "trigger times: 34\n",
      "Epoch: 059, Loss: 1.2433 \n",
      "trigger times: 35\n",
      "Epoch: 060, Loss: 1.2617 \n",
      "trigger times: 36\n",
      "Epoch: 061, Loss: 1.2199 \n",
      "trigger times: 37\n",
      "Epoch: 062, Loss: 1.3316 \n",
      "trigger times: 38\n",
      "Epoch: 063, Loss: 1.2193 \n",
      "trigger times: 39\n",
      "Epoch: 064, Loss: 1.1937 \n",
      "trigger times: 40\n",
      "Epoch: 065, Loss: 1.2869 \n",
      "trigger times: 41\n",
      "Epoch: 066, Loss: 1.3017 \n",
      "trigger times: 42\n",
      "Epoch: 067, Loss: 1.2190 \n",
      "trigger times: 43\n",
      "Epoch: 068, Loss: 1.2617 \n",
      "trigger times: 44\n",
      "Epoch: 069, Loss: 1.1850 \n",
      "trigger times: 45\n",
      "Epoch: 070, Loss: 1.2197 \n",
      "trigger times: 46\n",
      "Epoch: 071, Loss: 1.2271 \n",
      "trigger times: 47\n",
      "Epoch: 072, Loss: 1.2080 \n",
      "trigger times: 48\n",
      "Epoch: 073, Loss: 1.2296 \n",
      "trigger times: 49\n",
      "Epoch: 074, Loss: 1.2378 \n",
      "trigger times: 50\n",
      "Epoch: 075, Loss: 1.3045 \n",
      "trigger times: 51\n",
      "Epoch: 076, Loss: 1.2266 \n",
      "trigger times: 52\n",
      "Epoch: 077, Loss: 1.1787 \n",
      "trigger times: 53\n",
      "Epoch: 078, Loss: 1.2888 \n",
      "trigger times: 54\n",
      "Epoch: 079, Loss: 1.2305 \n",
      "trigger times: 55\n",
      "Epoch: 080, Loss: 1.2985 \n",
      "trigger times: 56\n",
      "Epoch: 081, Loss: 1.3240 \n",
      "trigger times: 57\n",
      "Epoch: 082, Loss: 1.2369 \n",
      "trigger times: 58\n",
      "Epoch: 083, Loss: 1.3182 \n",
      "trigger times: 59\n",
      "Epoch: 084, Loss: 1.2099 \n",
      "trigger times: 60\n",
      "Epoch: 085, Loss: 1.3047 \n",
      "trigger times: 61\n",
      "Epoch: 086, Loss: 1.2546 \n",
      "trigger times: 62\n",
      "Epoch: 087, Loss: 1.1687 \n",
      "trigger times: 63\n",
      "Epoch: 088, Loss: 1.2327 \n",
      "trigger times: 64\n",
      "Epoch: 089, Loss: 1.2134 \n",
      "trigger times: 65\n",
      "Epoch: 090, Loss: 1.3491 \n",
      "trigger times: 66\n",
      "Epoch: 091, Loss: 1.1861 \n",
      "trigger times: 67\n",
      "Epoch: 092, Loss: 1.2057 \n",
      "trigger times: 68\n",
      "Epoch: 093, Loss: 1.2312 \n",
      "trigger times: 69\n",
      "Epoch: 094, Loss: 1.1918 \n",
      "trigger times: 70\n",
      "Epoch: 095, Loss: 1.2079 \n",
      "trigger times: 71\n",
      "Epoch: 096, Loss: 1.2714 \n",
      "trigger times: 72\n",
      "Epoch: 097, Loss: 1.1906 \n",
      "trigger times: 73\n",
      "Epoch: 098, Loss: 1.2436 \n",
      "trigger times: 74\n",
      "Epoch: 099, Loss: 1.2788 \n",
      "trigger times: 75\n",
      "Epoch: 100, Loss: 1.2030 \n",
      "trigger times: 76\n",
      "Epoch: 101, Loss: 1.2358 \n",
      "trigger times: 77\n",
      "Epoch: 102, Loss: 1.2621 \n",
      "trigger times: 78\n",
      "Epoch: 103, Loss: 1.2828 \n",
      "trigger times: 79\n",
      "Epoch: 104, Loss: 1.2743 \n",
      "trigger times: 80\n",
      "Epoch: 105, Loss: 1.2890 \n",
      "trigger times: 81\n",
      "Epoch: 106, Loss: 1.2338 \n",
      "trigger times: 82\n",
      "Epoch: 107, Loss: 1.2301 \n",
      "trigger times: 83\n",
      "Epoch: 108, Loss: 1.1994 \n",
      "trigger times: 84\n",
      "Epoch: 109, Loss: 1.2011 \n",
      "trigger times: 85\n",
      "Epoch: 110, Loss: 1.2202 \n",
      "trigger times: 86\n",
      "Epoch: 111, Loss: 1.2377 \n",
      "trigger times: 87\n",
      "Epoch: 112, Loss: 1.3665 \n",
      "trigger times: 88\n",
      "Epoch: 113, Loss: 1.2939 \n",
      "trigger times: 89\n",
      "Epoch: 114, Loss: 1.2238 \n",
      "trigger times: 90\n",
      "Epoch: 115, Loss: 1.1821 \n",
      "trigger times: 91\n",
      "Epoch: 116, Loss: 1.2069 \n",
      "trigger times: 92\n",
      "Epoch: 117, Loss: 1.2461 \n",
      "trigger times: 93\n",
      "Epoch: 118, Loss: 1.2808 \n",
      "trigger times: 94\n",
      "Epoch: 119, Loss: 1.2088 \n",
      "trigger times: 95\n",
      "Epoch: 120, Loss: 1.2600 \n",
      "trigger times: 96\n",
      "Epoch: 121, Loss: 1.2204 \n",
      "trigger times: 97\n",
      "Epoch: 122, Loss: 1.2520 \n",
      "trigger times: 98\n",
      "Epoch: 123, Loss: 1.2275 \n",
      "trigger times: 99\n",
      "Epoch: 124, Loss: 1.1866 \n",
      "trigger times: 100\n",
      "Epoch: 125, Loss: 1.2893 \n",
      "trigger times: 101\n",
      "Epoch: 126, Loss: 1.2485 \n",
      "trigger times: 102\n",
      "Epoch: 127, Loss: 1.2343 \n",
      "trigger times: 103\n",
      "Epoch: 128, Loss: 1.2463 \n",
      "trigger times: 104\n",
      "Epoch: 129, Loss: 1.2710 \n",
      "trigger times: 105\n",
      "Epoch: 130, Loss: 1.1968 \n",
      "trigger times: 106\n",
      "Epoch: 131, Loss: 1.1757 \n",
      "trigger times: 107\n",
      "Epoch: 132, Loss: 1.2299 \n",
      "trigger times: 108\n",
      "Epoch: 133, Loss: 1.2371 \n",
      "trigger times: 109\n",
      "Epoch: 134, Loss: 1.2435 \n",
      "trigger times: 110\n",
      "Epoch: 135, Loss: 1.2191 \n",
      "trigger times: 111\n",
      "Epoch: 136, Loss: 1.3232 \n",
      "trigger times: 112\n",
      "Epoch: 137, Loss: 1.1682 \n",
      "trigger times: 113\n",
      "Epoch: 138, Loss: 1.2020 \n",
      "trigger times: 114\n",
      "Epoch: 139, Loss: 1.1952 \n",
      "trigger times: 115\n",
      "Epoch: 140, Loss: 1.3476 \n",
      "trigger times: 116\n",
      "Epoch: 141, Loss: 1.3310 \n",
      "trigger times: 117\n",
      "Epoch: 142, Loss: 1.1650 \n",
      "trigger times: 118\n",
      "Epoch: 143, Loss: 1.3356 \n",
      "trigger times: 119\n",
      "Epoch: 144, Loss: 1.1869 \n",
      "trigger times: 120\n",
      "Epoch: 145, Loss: 1.2081 \n",
      "trigger times: 121\n",
      "Epoch: 146, Loss: 1.2382 \n",
      "trigger times: 122\n",
      "Epoch: 147, Loss: 1.2158 \n",
      "trigger times: 123\n",
      "Epoch: 148, Loss: 1.2488 \n",
      "trigger times: 124\n",
      "Epoch: 149, Loss: 1.2963 \n",
      "trigger times: 125\n",
      "Epoch: 150, Loss: 1.2189 \n",
      "trigger times: 126\n",
      "Epoch: 151, Loss: 1.3321 \n",
      "trigger times: 127\n",
      "Epoch: 152, Loss: 1.2496 \n",
      "trigger times: 128\n",
      "Epoch: 153, Loss: 1.3163 \n",
      "trigger times: 129\n",
      "Epoch: 154, Loss: 1.2123 \n",
      "trigger times: 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155, Loss: 1.1776 \n",
      "trigger times: 131\n",
      "Epoch: 156, Loss: 1.1946 \n",
      "trigger times: 132\n",
      "Epoch: 157, Loss: 1.1737 \n",
      "trigger times: 133\n",
      "Epoch: 158, Loss: 1.2733 \n",
      "trigger times: 134\n",
      "Epoch: 159, Loss: 1.2289 \n",
      "trigger times: 135\n",
      "Epoch: 160, Loss: 1.1986 \n",
      "trigger times: 136\n",
      "Epoch: 161, Loss: 1.1995 \n",
      "trigger times: 137\n",
      "Epoch: 162, Loss: 1.2263 \n",
      "trigger times: 138\n",
      "Epoch: 163, Loss: 1.3196 \n",
      "trigger times: 139\n",
      "Epoch: 164, Loss: 1.2307 \n",
      "trigger times: 140\n",
      "Epoch: 165, Loss: 6.7957 \n",
      "trigger times: 141\n",
      "Epoch: 166, Loss: 1.1913 \n",
      "trigger times: 142\n",
      "Epoch: 167, Loss: 1.1439 \n",
      "trigger times: 143\n",
      "Epoch: 168, Loss: 1.1165 \n",
      "Epoch: 169, Loss: 1.1332 \n",
      "trigger times: 1\n",
      "Epoch: 170, Loss: 1.1266 \n",
      "trigger times: 2\n",
      "Epoch: 171, Loss: 1.1181 \n",
      "trigger times: 3\n",
      "Epoch: 172, Loss: 1.1130 \n",
      "Epoch: 173, Loss: 1.1064 \n",
      "Epoch: 174, Loss: 1.1056 \n",
      "Epoch: 175, Loss: 1.1150 \n",
      "trigger times: 1\n",
      "Epoch: 176, Loss: 1.1295 \n",
      "trigger times: 2\n",
      "Epoch: 177, Loss: 1.1121 \n",
      "trigger times: 3\n",
      "Epoch: 178, Loss: 1.1069 \n",
      "trigger times: 4\n",
      "Epoch: 179, Loss: 1.1249 \n",
      "trigger times: 5\n",
      "Epoch: 180, Loss: 1.1619 \n",
      "trigger times: 6\n",
      "Epoch: 181, Loss: 1.1177 \n",
      "trigger times: 7\n",
      "Epoch: 182, Loss: 1.1088 \n",
      "trigger times: 8\n",
      "Epoch: 183, Loss: 1.1534 \n",
      "trigger times: 9\n",
      "Epoch: 184, Loss: 1.1306 \n",
      "trigger times: 10\n",
      "Epoch: 185, Loss: 1.1019 \n",
      "Epoch: 186, Loss: 1.1189 \n",
      "trigger times: 1\n",
      "Epoch: 187, Loss: 1.1279 \n",
      "trigger times: 2\n",
      "Epoch: 188, Loss: 1.1506 \n",
      "trigger times: 3\n",
      "Epoch: 189, Loss: 1.1238 \n",
      "trigger times: 4\n",
      "Epoch: 190, Loss: 1.1303 \n",
      "trigger times: 5\n",
      "Epoch: 191, Loss: 1.1404 \n",
      "trigger times: 6\n",
      "Epoch: 192, Loss: 1.1206 \n",
      "trigger times: 7\n",
      "Epoch: 193, Loss: 1.1309 \n",
      "trigger times: 8\n",
      "Epoch: 194, Loss: 1.1389 \n",
      "trigger times: 9\n",
      "Epoch: 195, Loss: 1.1432 \n",
      "trigger times: 10\n",
      "Epoch: 196, Loss: 1.1392 \n",
      "trigger times: 11\n",
      "Epoch: 197, Loss: 1.1641 \n",
      "trigger times: 12\n",
      "Epoch: 198, Loss: 1.1474 \n",
      "trigger times: 13\n",
      "Epoch: 199, Loss: 1.1666 \n",
      "trigger times: 14\n",
      "Epoch: 000, Loss: 1.3994 \n",
      "Epoch: 001, Loss: 1.3099 \n",
      "Epoch: 002, Loss: 1.2742 \n",
      "Epoch: 003, Loss: 1.2523 \n",
      "Epoch: 004, Loss: 1.3021 \n",
      "trigger times: 1\n",
      "Epoch: 005, Loss: 1.2366 \n",
      "Epoch: 006, Loss: 1.3289 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.3865 \n",
      "trigger times: 2\n",
      "Epoch: 008, Loss: 1.2689 \n",
      "trigger times: 3\n",
      "Epoch: 009, Loss: 1.2402 \n",
      "trigger times: 4\n",
      "Epoch: 010, Loss: 1.2797 \n",
      "trigger times: 5\n",
      "Epoch: 011, Loss: 2.8000 \n",
      "trigger times: 6\n",
      "Epoch: 012, Loss: 1.1797 \n",
      "Epoch: 013, Loss: 1.1799 \n",
      "trigger times: 1\n",
      "Epoch: 014, Loss: 1.1505 \n",
      "Epoch: 015, Loss: 1.1582 \n",
      "trigger times: 1\n",
      "Epoch: 016, Loss: 1.1793 \n",
      "trigger times: 2\n",
      "Epoch: 017, Loss: 1.1584 \n",
      "trigger times: 3\n",
      "Epoch: 018, Loss: 1.1975 \n",
      "trigger times: 4\n",
      "Epoch: 019, Loss: 1.1978 \n",
      "trigger times: 5\n",
      "Epoch: 020, Loss: 1.1908 \n",
      "trigger times: 6\n",
      "Epoch: 021, Loss: 1.2236 \n",
      "trigger times: 7\n",
      "Epoch: 022, Loss: 1.1923 \n",
      "trigger times: 8\n",
      "Epoch: 023, Loss: 1.1726 \n",
      "trigger times: 9\n",
      "Epoch: 024, Loss: 1.1826 \n",
      "trigger times: 10\n",
      "Epoch: 025, Loss: 1.1836 \n",
      "trigger times: 11\n",
      "Epoch: 026, Loss: 1.2122 \n",
      "trigger times: 12\n",
      "Epoch: 027, Loss: 1.2161 \n",
      "trigger times: 13\n",
      "Epoch: 028, Loss: 1.1999 \n",
      "trigger times: 14\n",
      "Epoch: 029, Loss: 1.2573 \n",
      "trigger times: 15\n",
      "Epoch: 030, Loss: 1.2557 \n",
      "trigger times: 16\n",
      "Epoch: 031, Loss: 1.2610 \n",
      "trigger times: 17\n",
      "Epoch: 032, Loss: 1.2158 \n",
      "trigger times: 18\n",
      "Epoch: 033, Loss: 1.1982 \n",
      "trigger times: 19\n",
      "Epoch: 034, Loss: 1.2095 \n",
      "trigger times: 20\n",
      "Epoch: 035, Loss: 1.1849 \n",
      "trigger times: 21\n",
      "Epoch: 036, Loss: 1.3355 \n",
      "trigger times: 22\n",
      "Epoch: 037, Loss: 1.2790 \n",
      "trigger times: 23\n",
      "Epoch: 038, Loss: 1.3551 \n",
      "trigger times: 24\n",
      "Epoch: 039, Loss: 1.2943 \n",
      "trigger times: 25\n",
      "Epoch: 040, Loss: 1.2597 \n",
      "trigger times: 26\n",
      "Epoch: 041, Loss: 1.2066 \n",
      "trigger times: 27\n",
      "Epoch: 042, Loss: 1.2137 \n",
      "trigger times: 28\n",
      "Epoch: 043, Loss: 1.2239 \n",
      "trigger times: 29\n",
      "Epoch: 044, Loss: 1.2521 \n",
      "trigger times: 30\n",
      "Epoch: 045, Loss: 1.3035 \n",
      "trigger times: 31\n",
      "Epoch: 046, Loss: 1.2227 \n",
      "trigger times: 32\n",
      "Epoch: 047, Loss: 1.2547 \n",
      "trigger times: 33\n",
      "Epoch: 048, Loss: 1.3356 \n",
      "trigger times: 34\n",
      "Epoch: 049, Loss: 1.2332 \n",
      "trigger times: 35\n",
      "Epoch: 050, Loss: 1.2855 \n",
      "trigger times: 36\n",
      "Epoch: 051, Loss: 1.2587 \n",
      "trigger times: 37\n",
      "Epoch: 052, Loss: 1.2043 \n",
      "trigger times: 38\n",
      "Epoch: 053, Loss: 1.2419 \n",
      "trigger times: 39\n",
      "Epoch: 054, Loss: 1.2262 \n",
      "trigger times: 40\n",
      "Epoch: 055, Loss: 1.2222 \n",
      "trigger times: 41\n",
      "Epoch: 056, Loss: 1.2915 \n",
      "trigger times: 42\n",
      "Epoch: 057, Loss: 1.2823 \n",
      "trigger times: 43\n",
      "Epoch: 058, Loss: 1.3196 \n",
      "trigger times: 44\n",
      "Epoch: 059, Loss: 1.2341 \n",
      "trigger times: 45\n",
      "Epoch: 060, Loss: 1.3058 \n",
      "trigger times: 46\n",
      "Epoch: 061, Loss: 1.2184 \n",
      "trigger times: 47\n",
      "Epoch: 062, Loss: 1.2175 \n",
      "trigger times: 48\n",
      "Epoch: 063, Loss: 1.2719 \n",
      "trigger times: 49\n",
      "Epoch: 064, Loss: 1.2816 \n",
      "trigger times: 50\n",
      "Epoch: 065, Loss: 1.2639 \n",
      "trigger times: 51\n",
      "Epoch: 066, Loss: 1.2959 \n",
      "trigger times: 52\n",
      "Epoch: 067, Loss: 1.2103 \n",
      "trigger times: 53\n",
      "Epoch: 068, Loss: 1.1945 \n",
      "trigger times: 54\n",
      "Epoch: 069, Loss: 1.2218 \n",
      "trigger times: 55\n",
      "Epoch: 070, Loss: 1.5261 \n",
      "trigger times: 56\n",
      "Epoch: 071, Loss: 1.2838 \n",
      "trigger times: 57\n",
      "Epoch: 072, Loss: 1.1922 \n",
      "trigger times: 58\n",
      "Epoch: 073, Loss: 1.2528 \n",
      "trigger times: 59\n",
      "Epoch: 074, Loss: 1.2363 \n",
      "trigger times: 60\n",
      "Epoch: 075, Loss: 1.2143 \n",
      "trigger times: 61\n",
      "Epoch: 076, Loss: 1.1956 \n",
      "trigger times: 62\n",
      "Epoch: 077, Loss: 1.2360 \n",
      "trigger times: 63\n",
      "Epoch: 078, Loss: 1.1908 \n",
      "trigger times: 64\n",
      "Epoch: 079, Loss: 1.3027 \n",
      "trigger times: 65\n",
      "Epoch: 080, Loss: 1.5001 \n",
      "trigger times: 66\n",
      "Epoch: 081, Loss: 1.4188 \n",
      "trigger times: 67\n",
      "Epoch: 082, Loss: 1.2013 \n",
      "trigger times: 68\n",
      "Epoch: 083, Loss: 1.1715 \n",
      "trigger times: 69\n",
      "Epoch: 084, Loss: 1.2290 \n",
      "trigger times: 70\n",
      "Epoch: 085, Loss: 1.2002 \n",
      "trigger times: 71\n",
      "Epoch: 086, Loss: 1.3353 \n",
      "trigger times: 72\n",
      "Epoch: 087, Loss: 1.2262 \n",
      "trigger times: 73\n",
      "Epoch: 088, Loss: 1.1664 \n",
      "trigger times: 74\n",
      "Epoch: 089, Loss: 1.2224 \n",
      "trigger times: 75\n",
      "Epoch: 090, Loss: 1.1719 \n",
      "trigger times: 76\n",
      "Epoch: 091, Loss: 1.2528 \n",
      "trigger times: 77\n",
      "Epoch: 092, Loss: 1.3300 \n",
      "trigger times: 78\n",
      "Epoch: 093, Loss: 1.2253 \n",
      "trigger times: 79\n",
      "Epoch: 094, Loss: 1.3615 \n",
      "trigger times: 80\n",
      "Epoch: 095, Loss: 1.2251 \n",
      "trigger times: 81\n",
      "Epoch: 096, Loss: 1.2087 \n",
      "trigger times: 82\n",
      "Epoch: 097, Loss: 1.1982 \n",
      "trigger times: 83\n",
      "Epoch: 098, Loss: 1.3380 \n",
      "trigger times: 84\n",
      "Epoch: 099, Loss: 1.2693 \n",
      "trigger times: 85\n",
      "Epoch: 100, Loss: 1.2087 \n",
      "trigger times: 86\n",
      "Epoch: 101, Loss: 1.2310 \n",
      "trigger times: 87\n",
      "Epoch: 102, Loss: 1.2325 \n",
      "trigger times: 88\n",
      "Epoch: 103, Loss: 1.2339 \n",
      "trigger times: 89\n",
      "Epoch: 104, Loss: 1.2874 \n",
      "trigger times: 90\n",
      "Epoch: 105, Loss: 1.2525 \n",
      "trigger times: 91\n",
      "Epoch: 106, Loss: 1.2553 \n",
      "trigger times: 92\n",
      "Epoch: 107, Loss: 1.2327 \n",
      "trigger times: 93\n",
      "Epoch: 108, Loss: 1.2076 \n",
      "trigger times: 94\n",
      "Epoch: 109, Loss: 1.2326 \n",
      "trigger times: 95\n",
      "Epoch: 110, Loss: 1.2061 \n",
      "trigger times: 96\n",
      "Epoch: 111, Loss: 1.2366 \n",
      "trigger times: 97\n",
      "Epoch: 112, Loss: 1.2771 \n",
      "trigger times: 98\n",
      "Epoch: 113, Loss: 1.3186 \n",
      "trigger times: 99\n",
      "Epoch: 114, Loss: 1.2523 \n",
      "trigger times: 100\n",
      "Epoch: 115, Loss: 1.2921 \n",
      "trigger times: 101\n",
      "Epoch: 116, Loss: 1.3086 \n",
      "trigger times: 102\n",
      "Epoch: 117, Loss: 1.1832 \n",
      "trigger times: 103\n",
      "Epoch: 118, Loss: 1.2786 \n",
      "trigger times: 104\n",
      "Epoch: 119, Loss: 1.2404 \n",
      "trigger times: 105\n",
      "Epoch: 120, Loss: 1.1868 \n",
      "trigger times: 106\n",
      "Epoch: 121, Loss: 1.2430 \n",
      "trigger times: 107\n",
      "Epoch: 122, Loss: 1.2792 \n",
      "trigger times: 108\n",
      "Epoch: 123, Loss: 1.3255 \n",
      "trigger times: 109\n",
      "Epoch: 124, Loss: 1.2905 \n",
      "trigger times: 110\n",
      "Epoch: 125, Loss: 1.2222 \n",
      "trigger times: 111\n",
      "Epoch: 126, Loss: 1.2867 \n",
      "trigger times: 112\n",
      "Epoch: 127, Loss: 1.2469 \n",
      "trigger times: 113\n",
      "Epoch: 128, Loss: 1.2665 \n",
      "trigger times: 114\n",
      "Epoch: 129, Loss: 1.2464 \n",
      "trigger times: 115\n",
      "Epoch: 130, Loss: 1.2051 \n",
      "trigger times: 116\n",
      "Epoch: 131, Loss: 1.4020 \n",
      "trigger times: 117\n",
      "Epoch: 132, Loss: 1.5253 \n",
      "trigger times: 118\n",
      "Epoch: 133, Loss: 1.2948 \n",
      "trigger times: 119\n",
      "Epoch: 134, Loss: 1.2157 \n",
      "trigger times: 120\n",
      "Epoch: 135, Loss: 1.2200 \n",
      "trigger times: 121\n",
      "Epoch: 136, Loss: 1.2012 \n",
      "trigger times: 122\n",
      "Epoch: 137, Loss: 1.1845 \n",
      "trigger times: 123\n",
      "Epoch: 138, Loss: 1.2045 \n",
      "trigger times: 124\n",
      "Epoch: 139, Loss: 1.1960 \n",
      "trigger times: 125\n",
      "Epoch: 140, Loss: 1.2337 \n",
      "trigger times: 126\n",
      "Epoch: 141, Loss: 1.2287 \n",
      "trigger times: 127\n",
      "Epoch: 142, Loss: 1.2476 \n",
      "trigger times: 128\n",
      "Epoch: 143, Loss: 1.2329 \n",
      "trigger times: 129\n",
      "Epoch: 144, Loss: 1.2657 \n",
      "trigger times: 130\n",
      "Epoch: 145, Loss: 1.2780 \n",
      "trigger times: 131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, Loss: 1.4649 \n",
      "trigger times: 132\n",
      "Epoch: 147, Loss: 1.5341 \n",
      "trigger times: 133\n",
      "Epoch: 148, Loss: 1.1807 \n",
      "trigger times: 134\n",
      "Epoch: 149, Loss: 1.1697 \n",
      "trigger times: 135\n",
      "Epoch: 150, Loss: 1.3011 \n",
      "trigger times: 136\n",
      "Epoch: 151, Loss: 1.1732 \n",
      "trigger times: 137\n",
      "Epoch: 152, Loss: 1.1858 \n",
      "trigger times: 138\n",
      "Epoch: 153, Loss: 1.2214 \n",
      "trigger times: 139\n",
      "Epoch: 154, Loss: 1.2000 \n",
      "trigger times: 140\n",
      "Epoch: 155, Loss: 1.1857 \n",
      "trigger times: 141\n",
      "Epoch: 156, Loss: 1.2210 \n",
      "trigger times: 142\n",
      "Epoch: 157, Loss: 1.2486 \n",
      "trigger times: 143\n",
      "Epoch: 158, Loss: 1.2136 \n",
      "trigger times: 144\n",
      "Epoch: 159, Loss: 1.2142 \n",
      "trigger times: 145\n",
      "Epoch: 160, Loss: 1.1896 \n",
      "trigger times: 146\n",
      "Epoch: 161, Loss: 1.2111 \n",
      "trigger times: 147\n",
      "Epoch: 162, Loss: 1.2516 \n",
      "trigger times: 148\n",
      "Epoch: 163, Loss: 1.2878 \n",
      "trigger times: 149\n",
      "Epoch: 164, Loss: 1.1885 \n",
      "trigger times: 150\n",
      "Epoch: 165, Loss: 1.2693 \n",
      "trigger times: 151\n",
      "Epoch: 166, Loss: 1.2250 \n",
      "trigger times: 152\n",
      "Epoch: 167, Loss: 1.6722 \n",
      "trigger times: 153\n",
      "Epoch: 168, Loss: 1.2928 \n",
      "trigger times: 154\n",
      "Epoch: 169, Loss: 1.1801 \n",
      "trigger times: 155\n",
      "Epoch: 170, Loss: 1.1597 \n",
      "trigger times: 156\n",
      "Epoch: 171, Loss: 1.1822 \n",
      "trigger times: 157\n",
      "Epoch: 172, Loss: 1.1749 \n",
      "trigger times: 158\n",
      "Epoch: 173, Loss: 1.1825 \n",
      "trigger times: 159\n",
      "Epoch: 174, Loss: 1.2364 \n",
      "trigger times: 160\n",
      "Epoch: 175, Loss: 1.1900 \n",
      "trigger times: 161\n",
      "Epoch: 176, Loss: 1.1863 \n",
      "trigger times: 162\n",
      "Epoch: 177, Loss: 1.2488 \n",
      "trigger times: 163\n",
      "Epoch: 178, Loss: 1.3799 \n",
      "trigger times: 164\n",
      "Epoch: 179, Loss: 1.2244 \n",
      "trigger times: 165\n",
      "Epoch: 180, Loss: 1.1981 \n",
      "trigger times: 166\n",
      "Epoch: 181, Loss: 1.2413 \n",
      "trigger times: 167\n",
      "Epoch: 182, Loss: 1.2185 \n",
      "trigger times: 168\n",
      "Epoch: 183, Loss: 1.3249 \n",
      "trigger times: 169\n",
      "Epoch: 184, Loss: 1.2180 \n",
      "trigger times: 170\n",
      "Epoch: 185, Loss: 1.2322 \n",
      "trigger times: 171\n",
      "Epoch: 186, Loss: 1.2051 \n",
      "trigger times: 172\n",
      "Epoch: 187, Loss: 1.2064 \n",
      "trigger times: 173\n",
      "Epoch: 188, Loss: 1.2814 \n",
      "trigger times: 174\n",
      "Epoch: 189, Loss: 1.3046 \n",
      "trigger times: 175\n",
      "Epoch: 190, Loss: 1.2263 \n",
      "trigger times: 176\n",
      "Epoch: 191, Loss: 1.2202 \n",
      "trigger times: 177\n",
      "Epoch: 192, Loss: 1.2274 \n",
      "trigger times: 178\n",
      "Epoch: 193, Loss: 1.1958 \n",
      "trigger times: 179\n",
      "Epoch: 194, Loss: 1.2136 \n",
      "trigger times: 180\n",
      "Epoch: 195, Loss: 1.2861 \n",
      "trigger times: 181\n",
      "Epoch: 196, Loss: 1.1633 \n",
      "trigger times: 182\n",
      "Epoch: 197, Loss: 1.4275 \n",
      "trigger times: 183\n",
      "Epoch: 198, Loss: 1.3175 \n",
      "trigger times: 184\n",
      "Epoch: 199, Loss: 1.1922 \n",
      "trigger times: 185\n",
      "Epoch: 000, Loss: 1.5243 \n",
      "Epoch: 001, Loss: 1.2163 \n",
      "Epoch: 002, Loss: 1.1940 \n",
      "Epoch: 003, Loss: 1.3080 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2160 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.2034 \n",
      "trigger times: 3\n",
      "Epoch: 006, Loss: 1.2238 \n",
      "trigger times: 4\n",
      "Epoch: 007, Loss: 1.3266 \n",
      "trigger times: 5\n",
      "Epoch: 008, Loss: 1.3148 \n",
      "trigger times: 6\n",
      "Epoch: 009, Loss: 1.2392 \n",
      "trigger times: 7\n",
      "Epoch: 010, Loss: 1.2419 \n",
      "trigger times: 8\n",
      "Epoch: 011, Loss: 1.2531 \n",
      "trigger times: 9\n",
      "Epoch: 012, Loss: 1.2719 \n",
      "trigger times: 10\n",
      "Epoch: 013, Loss: 1.3203 \n",
      "trigger times: 11\n",
      "Epoch: 014, Loss: 1.2828 \n",
      "trigger times: 12\n",
      "Epoch: 015, Loss: 1.3130 \n",
      "trigger times: 13\n",
      "Epoch: 016, Loss: 1.2602 \n",
      "trigger times: 14\n",
      "Epoch: 017, Loss: 1.2327 \n",
      "trigger times: 15\n",
      "Epoch: 018, Loss: 1.2307 \n",
      "trigger times: 16\n",
      "Epoch: 019, Loss: 1.4234 \n",
      "trigger times: 17\n",
      "Epoch: 020, Loss: 1.2107 \n",
      "trigger times: 18\n",
      "Epoch: 021, Loss: 1.1905 \n",
      "Epoch: 022, Loss: 1.2298 \n",
      "trigger times: 1\n",
      "Epoch: 023, Loss: 1.3868 \n",
      "trigger times: 2\n",
      "Epoch: 024, Loss: 1.2552 \n",
      "trigger times: 3\n",
      "Epoch: 025, Loss: 1.2202 \n",
      "trigger times: 4\n",
      "Epoch: 026, Loss: 1.2070 \n",
      "trigger times: 5\n",
      "Epoch: 027, Loss: 1.2268 \n",
      "trigger times: 6\n",
      "Epoch: 028, Loss: 1.2340 \n",
      "trigger times: 7\n",
      "Epoch: 029, Loss: 1.2398 \n",
      "trigger times: 8\n",
      "Epoch: 030, Loss: 1.2083 \n",
      "trigger times: 9\n",
      "Epoch: 031, Loss: 1.3537 \n",
      "trigger times: 10\n",
      "Epoch: 032, Loss: 1.1855 \n",
      "Epoch: 033, Loss: 1.2848 \n",
      "trigger times: 1\n",
      "Epoch: 034, Loss: 1.2206 \n",
      "trigger times: 2\n",
      "Epoch: 035, Loss: 1.2289 \n",
      "trigger times: 3\n",
      "Epoch: 036, Loss: 1.2002 \n",
      "trigger times: 4\n",
      "Epoch: 037, Loss: 1.3119 \n",
      "trigger times: 5\n",
      "Epoch: 038, Loss: 1.2979 \n",
      "trigger times: 6\n",
      "Epoch: 039, Loss: 1.2172 \n",
      "trigger times: 7\n",
      "Epoch: 040, Loss: 1.2100 \n",
      "trigger times: 8\n",
      "Epoch: 041, Loss: 1.1931 \n",
      "trigger times: 9\n",
      "Epoch: 042, Loss: 1.2756 \n",
      "trigger times: 10\n",
      "Epoch: 043, Loss: 1.4075 \n",
      "trigger times: 11\n",
      "Epoch: 044, Loss: 1.3010 \n",
      "trigger times: 12\n",
      "Epoch: 045, Loss: 1.2170 \n",
      "trigger times: 13\n",
      "Epoch: 046, Loss: 1.2380 \n",
      "trigger times: 14\n",
      "Epoch: 047, Loss: 1.1874 \n",
      "trigger times: 15\n",
      "Epoch: 048, Loss: 1.2240 \n",
      "trigger times: 16\n",
      "Epoch: 049, Loss: 1.2143 \n",
      "trigger times: 17\n",
      "Epoch: 050, Loss: 1.2281 \n",
      "trigger times: 18\n",
      "Epoch: 051, Loss: 1.2879 \n",
      "trigger times: 19\n",
      "Epoch: 052, Loss: 1.2288 \n",
      "trigger times: 20\n",
      "Epoch: 053, Loss: 1.4861 \n",
      "trigger times: 21\n",
      "Epoch: 054, Loss: 1.2539 \n",
      "trigger times: 22\n",
      "Epoch: 055, Loss: 1.2116 \n",
      "trigger times: 23\n",
      "Epoch: 056, Loss: 1.2044 \n",
      "trigger times: 24\n",
      "Epoch: 057, Loss: 1.2216 \n",
      "trigger times: 25\n",
      "Epoch: 058, Loss: 1.1768 \n",
      "Epoch: 059, Loss: 1.2013 \n",
      "trigger times: 1\n",
      "Epoch: 060, Loss: 1.3285 \n",
      "trigger times: 2\n",
      "Epoch: 061, Loss: 1.2291 \n",
      "trigger times: 3\n",
      "Epoch: 062, Loss: 1.1902 \n",
      "trigger times: 4\n",
      "Epoch: 063, Loss: 1.3054 \n",
      "trigger times: 5\n",
      "Epoch: 064, Loss: 1.1988 \n",
      "trigger times: 6\n",
      "Epoch: 065, Loss: 1.2494 \n",
      "trigger times: 7\n",
      "Epoch: 066, Loss: 1.2198 \n",
      "trigger times: 8\n",
      "Epoch: 067, Loss: 1.2550 \n",
      "trigger times: 9\n",
      "Epoch: 068, Loss: 1.1768 \n",
      "Epoch: 069, Loss: 1.2026 \n",
      "trigger times: 1\n",
      "Epoch: 070, Loss: 1.2243 \n",
      "trigger times: 2\n",
      "Epoch: 071, Loss: 1.3036 \n",
      "trigger times: 3\n",
      "Epoch: 072, Loss: 1.3030 \n",
      "trigger times: 4\n",
      "Epoch: 073, Loss: 1.2355 \n",
      "trigger times: 5\n",
      "Epoch: 074, Loss: 1.2741 \n",
      "trigger times: 6\n",
      "Epoch: 075, Loss: 1.2551 \n",
      "trigger times: 7\n",
      "Epoch: 076, Loss: 1.2660 \n",
      "trigger times: 8\n",
      "Epoch: 077, Loss: 1.2394 \n",
      "trigger times: 9\n",
      "Epoch: 078, Loss: 1.2441 \n",
      "trigger times: 10\n",
      "Epoch: 079, Loss: 1.2932 \n",
      "trigger times: 11\n",
      "Epoch: 080, Loss: 1.2761 \n",
      "trigger times: 12\n",
      "Epoch: 081, Loss: 1.2242 \n",
      "trigger times: 13\n",
      "Epoch: 082, Loss: 1.2523 \n",
      "trigger times: 14\n",
      "Epoch: 083, Loss: 1.1977 \n",
      "trigger times: 15\n",
      "Epoch: 084, Loss: 1.2171 \n",
      "trigger times: 16\n",
      "Epoch: 085, Loss: 1.1908 \n",
      "trigger times: 17\n",
      "Epoch: 086, Loss: 1.2581 \n",
      "trigger times: 18\n",
      "Epoch: 087, Loss: 1.2367 \n",
      "trigger times: 19\n",
      "Epoch: 088, Loss: 1.1864 \n",
      "trigger times: 20\n",
      "Epoch: 089, Loss: 1.2334 \n",
      "trigger times: 21\n",
      "Epoch: 090, Loss: 1.2717 \n",
      "trigger times: 22\n",
      "Epoch: 091, Loss: 1.3040 \n",
      "trigger times: 23\n",
      "Epoch: 092, Loss: 1.2530 \n",
      "trigger times: 24\n",
      "Epoch: 093, Loss: 1.2482 \n",
      "trigger times: 25\n",
      "Epoch: 094, Loss: 1.2361 \n",
      "trigger times: 26\n",
      "Epoch: 095, Loss: 1.3318 \n",
      "trigger times: 27\n",
      "Epoch: 096, Loss: 1.1966 \n",
      "trigger times: 28\n",
      "Epoch: 097, Loss: 1.2345 \n",
      "trigger times: 29\n",
      "Epoch: 098, Loss: 1.3558 \n",
      "trigger times: 30\n",
      "Epoch: 099, Loss: 1.2330 \n",
      "trigger times: 31\n",
      "Epoch: 100, Loss: 1.1706 \n",
      "Epoch: 101, Loss: 1.2161 \n",
      "trigger times: 1\n",
      "Epoch: 102, Loss: 1.1829 \n",
      "trigger times: 2\n",
      "Epoch: 103, Loss: 1.1799 \n",
      "trigger times: 3\n",
      "Epoch: 104, Loss: 1.2911 \n",
      "trigger times: 4\n",
      "Epoch: 105, Loss: 1.2483 \n",
      "trigger times: 5\n",
      "Epoch: 106, Loss: 1.2809 \n",
      "trigger times: 6\n",
      "Epoch: 107, Loss: 1.2158 \n",
      "trigger times: 7\n",
      "Epoch: 108, Loss: 1.2794 \n",
      "trigger times: 8\n",
      "Epoch: 109, Loss: 1.2067 \n",
      "trigger times: 9\n",
      "Epoch: 110, Loss: 1.2867 \n",
      "trigger times: 10\n",
      "Epoch: 111, Loss: 1.2088 \n",
      "trigger times: 11\n",
      "Epoch: 112, Loss: 1.2534 \n",
      "trigger times: 12\n",
      "Epoch: 113, Loss: 1.2368 \n",
      "trigger times: 13\n",
      "Epoch: 114, Loss: 1.2597 \n",
      "trigger times: 14\n",
      "Epoch: 115, Loss: 1.2242 \n",
      "trigger times: 15\n",
      "Epoch: 116, Loss: 1.2746 \n",
      "trigger times: 16\n",
      "Epoch: 117, Loss: 1.2187 \n",
      "trigger times: 17\n",
      "Epoch: 118, Loss: 1.2753 \n",
      "trigger times: 18\n",
      "Epoch: 119, Loss: 1.2289 \n",
      "trigger times: 19\n",
      "Epoch: 120, Loss: 1.2462 \n",
      "trigger times: 20\n",
      "Epoch: 121, Loss: 1.2830 \n",
      "trigger times: 21\n",
      "Epoch: 122, Loss: 1.2197 \n",
      "trigger times: 22\n",
      "Epoch: 123, Loss: 1.2377 \n",
      "trigger times: 23\n",
      "Epoch: 124, Loss: 1.1909 \n",
      "trigger times: 24\n",
      "Epoch: 125, Loss: 1.1974 \n",
      "trigger times: 25\n",
      "Epoch: 126, Loss: 1.2456 \n",
      "trigger times: 26\n",
      "Epoch: 127, Loss: 1.2512 \n",
      "trigger times: 27\n",
      "Epoch: 128, Loss: 1.3281 \n",
      "trigger times: 28\n",
      "Epoch: 129, Loss: 1.2448 \n",
      "trigger times: 29\n",
      "Epoch: 130, Loss: 1.2417 \n",
      "trigger times: 30\n",
      "Epoch: 131, Loss: 1.2418 \n",
      "trigger times: 31\n",
      "Epoch: 132, Loss: 1.2398 \n",
      "trigger times: 32\n",
      "Epoch: 133, Loss: 1.2165 \n",
      "trigger times: 33\n",
      "Epoch: 134, Loss: 1.2567 \n",
      "trigger times: 34\n",
      "Epoch: 135, Loss: 1.2446 \n",
      "trigger times: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, Loss: 1.2013 \n",
      "trigger times: 36\n",
      "Epoch: 137, Loss: 1.3064 \n",
      "trigger times: 37\n",
      "Epoch: 138, Loss: 1.2259 \n",
      "trigger times: 38\n",
      "Epoch: 139, Loss: 1.2796 \n",
      "trigger times: 39\n",
      "Epoch: 140, Loss: 1.2557 \n",
      "trigger times: 40\n",
      "Epoch: 141, Loss: 1.2473 \n",
      "trigger times: 41\n",
      "Epoch: 142, Loss: 1.1844 \n",
      "trigger times: 42\n",
      "Epoch: 143, Loss: 1.2409 \n",
      "trigger times: 43\n",
      "Epoch: 144, Loss: 1.2151 \n",
      "trigger times: 44\n",
      "Epoch: 145, Loss: 1.1872 \n",
      "trigger times: 45\n",
      "Epoch: 146, Loss: 1.2779 \n",
      "trigger times: 46\n",
      "Epoch: 147, Loss: 1.1812 \n",
      "trigger times: 47\n",
      "Epoch: 148, Loss: 1.2199 \n",
      "trigger times: 48\n",
      "Epoch: 149, Loss: 1.2180 \n",
      "trigger times: 49\n",
      "Epoch: 150, Loss: 1.2293 \n",
      "trigger times: 50\n",
      "Epoch: 151, Loss: 1.2681 \n",
      "trigger times: 51\n",
      "Epoch: 152, Loss: 1.2309 \n",
      "trigger times: 52\n",
      "Epoch: 153, Loss: 1.2895 \n",
      "trigger times: 53\n",
      "Epoch: 154, Loss: 2.4648 \n",
      "trigger times: 54\n",
      "Epoch: 155, Loss: 1.4302 \n",
      "trigger times: 55\n",
      "Epoch: 156, Loss: 1.1562 \n",
      "Epoch: 157, Loss: 1.1545 \n",
      "Epoch: 158, Loss: 1.1204 \n",
      "Epoch: 159, Loss: 1.1248 \n",
      "trigger times: 1\n",
      "Epoch: 160, Loss: 1.1467 \n",
      "trigger times: 2\n",
      "Epoch: 161, Loss: 1.1259 \n",
      "trigger times: 3\n",
      "Epoch: 162, Loss: 1.1318 \n",
      "trigger times: 4\n",
      "Epoch: 163, Loss: 1.1321 \n",
      "trigger times: 5\n",
      "Epoch: 164, Loss: 1.1681 \n",
      "trigger times: 6\n",
      "Epoch: 165, Loss: 1.1277 \n",
      "trigger times: 7\n",
      "Epoch: 166, Loss: 1.1357 \n",
      "trigger times: 8\n",
      "Epoch: 167, Loss: 1.1817 \n",
      "trigger times: 9\n",
      "Epoch: 168, Loss: 1.1792 \n",
      "trigger times: 10\n",
      "Epoch: 169, Loss: 1.1223 \n",
      "trigger times: 11\n",
      "Epoch: 170, Loss: 1.1841 \n",
      "trigger times: 12\n",
      "Epoch: 171, Loss: 1.1444 \n",
      "trigger times: 13\n",
      "Epoch: 172, Loss: 1.1881 \n",
      "trigger times: 14\n",
      "Epoch: 173, Loss: 1.1945 \n",
      "trigger times: 15\n",
      "Epoch: 174, Loss: 1.1654 \n",
      "trigger times: 16\n",
      "Epoch: 175, Loss: 1.2073 \n",
      "trigger times: 17\n",
      "Epoch: 176, Loss: 1.2372 \n",
      "trigger times: 18\n",
      "Epoch: 177, Loss: 1.2427 \n",
      "trigger times: 19\n",
      "Epoch: 178, Loss: 1.2093 \n",
      "trigger times: 20\n",
      "Epoch: 179, Loss: 1.1975 \n",
      "trigger times: 21\n",
      "Epoch: 180, Loss: 1.2546 \n",
      "trigger times: 22\n",
      "Epoch: 181, Loss: 1.2204 \n",
      "trigger times: 23\n",
      "Epoch: 182, Loss: 1.2055 \n",
      "trigger times: 24\n",
      "Epoch: 183, Loss: 1.2483 \n",
      "trigger times: 25\n",
      "Epoch: 184, Loss: 1.4903 \n",
      "trigger times: 26\n",
      "Epoch: 185, Loss: 1.2136 \n",
      "trigger times: 27\n",
      "Epoch: 186, Loss: 1.2222 \n",
      "trigger times: 28\n",
      "Epoch: 187, Loss: 1.2319 \n",
      "trigger times: 29\n",
      "Epoch: 188, Loss: 1.3437 \n",
      "trigger times: 30\n",
      "Epoch: 189, Loss: 1.2474 \n",
      "trigger times: 31\n",
      "Epoch: 190, Loss: 1.1946 \n",
      "trigger times: 32\n",
      "Epoch: 191, Loss: 1.2138 \n",
      "trigger times: 33\n",
      "Epoch: 192, Loss: 1.1737 \n",
      "trigger times: 34\n",
      "Epoch: 193, Loss: 1.2298 \n",
      "trigger times: 35\n",
      "Epoch: 194, Loss: 1.2226 \n",
      "trigger times: 36\n",
      "Epoch: 195, Loss: 1.1693 \n",
      "trigger times: 37\n",
      "Epoch: 196, Loss: 1.1832 \n",
      "trigger times: 38\n",
      "Epoch: 197, Loss: 1.3496 \n",
      "trigger times: 39\n",
      "Epoch: 198, Loss: 1.4276 \n",
      "trigger times: 40\n",
      "Epoch: 199, Loss: 1.2498 \n",
      "trigger times: 41\n",
      "Epoch: 000, Loss: 1.4568 \n",
      "Epoch: 001, Loss: 1.3330 \n",
      "Epoch: 002, Loss: 1.3081 \n",
      "Epoch: 003, Loss: 1.3849 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2645 \n",
      "Epoch: 005, Loss: 1.2969 \n",
      "trigger times: 1\n",
      "Epoch: 006, Loss: 1.3010 \n",
      "trigger times: 2\n",
      "Epoch: 007, Loss: 1.3391 \n",
      "trigger times: 3\n",
      "Epoch: 008, Loss: 1.3485 \n",
      "trigger times: 4\n",
      "Epoch: 009, Loss: 1.2970 \n",
      "trigger times: 5\n",
      "Epoch: 010, Loss: 1.2827 \n",
      "trigger times: 6\n",
      "Epoch: 011, Loss: 1.2916 \n",
      "trigger times: 7\n",
      "Epoch: 012, Loss: 1.2930 \n",
      "trigger times: 8\n",
      "Epoch: 013, Loss: 1.2728 \n",
      "trigger times: 9\n",
      "Epoch: 014, Loss: 1.2734 \n",
      "trigger times: 10\n",
      "Epoch: 015, Loss: 1.3843 \n",
      "trigger times: 11\n",
      "Epoch: 016, Loss: 1.2877 \n",
      "trigger times: 12\n",
      "Epoch: 017, Loss: 1.3732 \n",
      "trigger times: 13\n",
      "Epoch: 018, Loss: 1.3279 \n",
      "trigger times: 14\n",
      "Epoch: 019, Loss: 1.2885 \n",
      "trigger times: 15\n",
      "Epoch: 020, Loss: 1.2233 \n",
      "Epoch: 021, Loss: 1.2664 \n",
      "trigger times: 1\n",
      "Epoch: 022, Loss: 1.3189 \n",
      "trigger times: 2\n",
      "Epoch: 023, Loss: 1.2600 \n",
      "trigger times: 3\n",
      "Epoch: 024, Loss: 1.2956 \n",
      "trigger times: 4\n",
      "Epoch: 025, Loss: 1.3269 \n",
      "trigger times: 5\n",
      "Epoch: 026, Loss: 1.2679 \n",
      "trigger times: 6\n",
      "Epoch: 027, Loss: 1.2409 \n",
      "trigger times: 7\n",
      "Epoch: 028, Loss: 1.2716 \n",
      "trigger times: 8\n",
      "Epoch: 029, Loss: 1.2679 \n",
      "trigger times: 9\n",
      "Epoch: 030, Loss: 1.2498 \n",
      "trigger times: 10\n",
      "Epoch: 031, Loss: 1.3174 \n",
      "trigger times: 11\n",
      "Epoch: 032, Loss: 1.4012 \n",
      "trigger times: 12\n",
      "Epoch: 033, Loss: 1.2175 \n",
      "Epoch: 034, Loss: 1.2871 \n",
      "trigger times: 1\n",
      "Epoch: 035, Loss: 1.2139 \n",
      "Epoch: 036, Loss: 1.2619 \n",
      "trigger times: 1\n",
      "Epoch: 037, Loss: 1.2659 \n",
      "trigger times: 2\n",
      "Epoch: 038, Loss: 1.2569 \n",
      "trigger times: 3\n",
      "Epoch: 039, Loss: 1.2260 \n",
      "trigger times: 4\n",
      "Epoch: 040, Loss: 1.2544 \n",
      "trigger times: 5\n",
      "Epoch: 041, Loss: 1.3315 \n",
      "trigger times: 6\n",
      "Epoch: 042, Loss: 1.2199 \n",
      "trigger times: 7\n",
      "Epoch: 043, Loss: 1.2588 \n",
      "trigger times: 8\n",
      "Epoch: 044, Loss: 1.2456 \n",
      "trigger times: 9\n",
      "Epoch: 045, Loss: 1.2531 \n",
      "trigger times: 10\n",
      "Epoch: 046, Loss: 1.2169 \n",
      "trigger times: 11\n",
      "Epoch: 047, Loss: 1.3117 \n",
      "trigger times: 12\n",
      "Epoch: 048, Loss: 1.2898 \n",
      "trigger times: 13\n",
      "Epoch: 049, Loss: 1.2752 \n",
      "trigger times: 14\n",
      "Epoch: 050, Loss: 1.2137 \n",
      "Epoch: 051, Loss: 1.3341 \n",
      "trigger times: 1\n",
      "Epoch: 052, Loss: 1.2182 \n",
      "trigger times: 2\n",
      "Epoch: 053, Loss: 1.2624 \n",
      "trigger times: 3\n",
      "Epoch: 054, Loss: 1.2659 \n",
      "trigger times: 4\n",
      "Epoch: 055, Loss: 1.3870 \n",
      "trigger times: 5\n",
      "Epoch: 056, Loss: 1.2728 \n",
      "trigger times: 6\n",
      "Epoch: 057, Loss: 1.3399 \n",
      "trigger times: 7\n",
      "Epoch: 058, Loss: 1.2819 \n",
      "trigger times: 8\n",
      "Epoch: 059, Loss: 1.3007 \n",
      "trigger times: 9\n",
      "Epoch: 060, Loss: 1.3058 \n",
      "trigger times: 10\n",
      "Epoch: 061, Loss: 1.2246 \n",
      "trigger times: 11\n",
      "Epoch: 062, Loss: 1.2311 \n",
      "trigger times: 12\n",
      "Epoch: 063, Loss: 1.3385 \n",
      "trigger times: 13\n",
      "Epoch: 064, Loss: 1.2135 \n",
      "Epoch: 065, Loss: 1.2466 \n",
      "trigger times: 1\n",
      "Epoch: 066, Loss: 1.2124 \n",
      "Epoch: 067, Loss: 1.2040 \n",
      "Epoch: 068, Loss: 1.3624 \n",
      "trigger times: 1\n",
      "Epoch: 069, Loss: 1.2746 \n",
      "trigger times: 2\n",
      "Epoch: 070, Loss: 1.2570 \n",
      "trigger times: 3\n",
      "Epoch: 071, Loss: 1.2652 \n",
      "trigger times: 4\n",
      "Epoch: 072, Loss: 1.2565 \n",
      "trigger times: 5\n",
      "Epoch: 073, Loss: 1.2818 \n",
      "trigger times: 6\n",
      "Epoch: 074, Loss: 1.2947 \n",
      "trigger times: 7\n",
      "Epoch: 075, Loss: 1.2217 \n",
      "trigger times: 8\n",
      "Epoch: 076, Loss: 1.2698 \n",
      "trigger times: 9\n",
      "Epoch: 077, Loss: 1.3578 \n",
      "trigger times: 10\n",
      "Epoch: 078, Loss: 1.3023 \n",
      "trigger times: 11\n",
      "Epoch: 079, Loss: 1.2337 \n",
      "trigger times: 12\n",
      "Epoch: 080, Loss: 1.1821 \n",
      "Epoch: 081, Loss: 1.2565 \n",
      "trigger times: 1\n",
      "Epoch: 082, Loss: 1.2525 \n",
      "trigger times: 2\n",
      "Epoch: 083, Loss: 1.2172 \n",
      "trigger times: 3\n",
      "Epoch: 084, Loss: 1.2930 \n",
      "trigger times: 4\n",
      "Epoch: 085, Loss: 1.2556 \n",
      "trigger times: 5\n",
      "Epoch: 086, Loss: 1.3051 \n",
      "trigger times: 6\n",
      "Epoch: 087, Loss: 1.2529 \n",
      "trigger times: 7\n",
      "Epoch: 088, Loss: 1.2540 \n",
      "trigger times: 8\n",
      "Epoch: 089, Loss: 1.1982 \n",
      "trigger times: 9\n",
      "Epoch: 090, Loss: 1.1766 \n",
      "Epoch: 091, Loss: 1.3033 \n",
      "trigger times: 1\n",
      "Epoch: 092, Loss: 1.2394 \n",
      "trigger times: 2\n",
      "Epoch: 093, Loss: 1.2704 \n",
      "trigger times: 3\n",
      "Epoch: 094, Loss: 1.2488 \n",
      "trigger times: 4\n",
      "Epoch: 095, Loss: 1.2566 \n",
      "trigger times: 5\n",
      "Epoch: 096, Loss: 1.3006 \n",
      "trigger times: 6\n",
      "Epoch: 097, Loss: 1.2542 \n",
      "trigger times: 7\n",
      "Epoch: 098, Loss: 1.2589 \n",
      "trigger times: 8\n",
      "Epoch: 099, Loss: 1.3359 \n",
      "trigger times: 9\n",
      "Epoch: 100, Loss: 1.2447 \n",
      "trigger times: 10\n",
      "Epoch: 101, Loss: 1.2669 \n",
      "trigger times: 11\n",
      "Epoch: 102, Loss: 1.2333 \n",
      "trigger times: 12\n",
      "Epoch: 103, Loss: 1.2295 \n",
      "trigger times: 13\n",
      "Epoch: 104, Loss: 1.1726 \n",
      "Epoch: 105, Loss: 1.2871 \n",
      "trigger times: 1\n",
      "Epoch: 106, Loss: 1.3480 \n",
      "trigger times: 2\n",
      "Epoch: 107, Loss: 1.2215 \n",
      "trigger times: 3\n",
      "Epoch: 108, Loss: 1.2610 \n",
      "trigger times: 4\n",
      "Epoch: 109, Loss: 1.2933 \n",
      "trigger times: 5\n",
      "Epoch: 110, Loss: 1.2749 \n",
      "trigger times: 6\n",
      "Epoch: 111, Loss: 1.2507 \n",
      "trigger times: 7\n",
      "Epoch: 112, Loss: 1.4242 \n",
      "trigger times: 8\n",
      "Epoch: 113, Loss: 1.1987 \n",
      "trigger times: 9\n",
      "Epoch: 114, Loss: 1.2426 \n",
      "trigger times: 10\n",
      "Epoch: 115, Loss: 1.2229 \n",
      "trigger times: 11\n",
      "Epoch: 116, Loss: 1.3573 \n",
      "trigger times: 12\n",
      "Epoch: 117, Loss: 1.2144 \n",
      "trigger times: 13\n",
      "Epoch: 118, Loss: 1.2033 \n",
      "trigger times: 14\n",
      "Epoch: 119, Loss: 1.2324 \n",
      "trigger times: 15\n",
      "Epoch: 120, Loss: 1.2126 \n",
      "trigger times: 16\n",
      "Epoch: 121, Loss: 1.1954 \n",
      "trigger times: 17\n",
      "Epoch: 122, Loss: 1.3045 \n",
      "trigger times: 18\n",
      "Epoch: 123, Loss: 1.2236 \n",
      "trigger times: 19\n",
      "Epoch: 124, Loss: 1.2803 \n",
      "trigger times: 20\n",
      "Epoch: 125, Loss: 1.2724 \n",
      "trigger times: 21\n",
      "Epoch: 126, Loss: 1.3544 \n",
      "trigger times: 22\n",
      "Epoch: 127, Loss: 1.2570 \n",
      "trigger times: 23\n",
      "Epoch: 128, Loss: 1.2217 \n",
      "trigger times: 24\n",
      "Epoch: 129, Loss: 1.1759 \n",
      "trigger times: 25\n",
      "Epoch: 130, Loss: 1.2342 \n",
      "trigger times: 26\n",
      "Epoch: 131, Loss: 1.2793 \n",
      "trigger times: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132, Loss: 1.2372 \n",
      "trigger times: 28\n",
      "Epoch: 133, Loss: 1.3076 \n",
      "trigger times: 29\n",
      "Epoch: 134, Loss: 1.2424 \n",
      "trigger times: 30\n",
      "Epoch: 135, Loss: 1.2000 \n",
      "trigger times: 31\n",
      "Epoch: 136, Loss: 1.2762 \n",
      "trigger times: 32\n",
      "Epoch: 137, Loss: 1.3156 \n",
      "trigger times: 33\n",
      "Epoch: 138, Loss: 1.1996 \n",
      "trigger times: 34\n",
      "Epoch: 139, Loss: 1.1940 \n",
      "trigger times: 35\n",
      "Epoch: 140, Loss: 1.2338 \n",
      "trigger times: 36\n",
      "Epoch: 141, Loss: 1.2646 \n",
      "trigger times: 37\n",
      "Epoch: 142, Loss: 1.3194 \n",
      "trigger times: 38\n",
      "Epoch: 143, Loss: 1.2281 \n",
      "trigger times: 39\n",
      "Epoch: 144, Loss: 1.1989 \n",
      "trigger times: 40\n",
      "Epoch: 145, Loss: 1.2948 \n",
      "trigger times: 41\n",
      "Epoch: 146, Loss: 1.2243 \n",
      "trigger times: 42\n",
      "Epoch: 147, Loss: 1.2441 \n",
      "trigger times: 43\n",
      "Epoch: 148, Loss: 1.2854 \n",
      "trigger times: 44\n",
      "Epoch: 149, Loss: 1.3608 \n",
      "trigger times: 45\n",
      "Epoch: 150, Loss: 1.2779 \n",
      "trigger times: 46\n",
      "Epoch: 151, Loss: 1.3022 \n",
      "trigger times: 47\n",
      "Epoch: 152, Loss: 1.1882 \n",
      "trigger times: 48\n",
      "Epoch: 153, Loss: 1.2175 \n",
      "trigger times: 49\n",
      "Epoch: 154, Loss: 1.1918 \n",
      "trigger times: 50\n",
      "Epoch: 155, Loss: 1.1922 \n",
      "trigger times: 51\n",
      "Epoch: 156, Loss: 1.2707 \n",
      "trigger times: 52\n",
      "Epoch: 157, Loss: 1.2487 \n",
      "trigger times: 53\n",
      "Epoch: 158, Loss: 1.1651 \n",
      "Epoch: 159, Loss: 1.2521 \n",
      "trigger times: 1\n",
      "Epoch: 160, Loss: 1.2700 \n",
      "trigger times: 2\n",
      "Epoch: 161, Loss: 1.2769 \n",
      "trigger times: 3\n",
      "Epoch: 162, Loss: 1.2656 \n",
      "trigger times: 4\n",
      "Epoch: 163, Loss: 1.2820 \n",
      "trigger times: 5\n",
      "Epoch: 164, Loss: 1.2129 \n",
      "trigger times: 6\n",
      "Epoch: 165, Loss: 1.2143 \n",
      "trigger times: 7\n",
      "Epoch: 166, Loss: 1.2470 \n",
      "trigger times: 8\n",
      "Epoch: 167, Loss: 1.3252 \n",
      "trigger times: 9\n",
      "Epoch: 168, Loss: 1.2705 \n",
      "trigger times: 10\n",
      "Epoch: 169, Loss: 1.2364 \n",
      "trigger times: 11\n",
      "Epoch: 170, Loss: 1.2070 \n",
      "trigger times: 12\n",
      "Epoch: 171, Loss: 1.2110 \n",
      "trigger times: 13\n",
      "Epoch: 172, Loss: 1.2167 \n",
      "trigger times: 14\n",
      "Epoch: 173, Loss: 1.2889 \n",
      "trigger times: 15\n",
      "Epoch: 174, Loss: 1.2427 \n",
      "trigger times: 16\n",
      "Epoch: 175, Loss: 1.9840 \n",
      "trigger times: 17\n",
      "Epoch: 176, Loss: 1.2345 \n",
      "trigger times: 18\n",
      "Epoch: 177, Loss: 1.1314 \n",
      "Epoch: 178, Loss: 1.1503 \n",
      "trigger times: 1\n",
      "Epoch: 179, Loss: 1.1433 \n",
      "trigger times: 2\n",
      "Epoch: 180, Loss: 1.1356 \n",
      "trigger times: 3\n",
      "Epoch: 181, Loss: 1.1588 \n",
      "trigger times: 4\n",
      "Epoch: 182, Loss: 1.1712 \n",
      "trigger times: 5\n",
      "Epoch: 183, Loss: 1.1747 \n",
      "trigger times: 6\n",
      "Epoch: 184, Loss: 1.1708 \n",
      "trigger times: 7\n",
      "Epoch: 185, Loss: 1.1817 \n",
      "trigger times: 8\n",
      "Epoch: 186, Loss: 1.1682 \n",
      "trigger times: 9\n",
      "Epoch: 187, Loss: 1.2149 \n",
      "trigger times: 10\n",
      "Epoch: 188, Loss: 1.1823 \n",
      "trigger times: 11\n",
      "Epoch: 189, Loss: 1.2205 \n",
      "trigger times: 12\n",
      "Epoch: 190, Loss: 1.2352 \n",
      "trigger times: 13\n",
      "Epoch: 191, Loss: 1.2637 \n",
      "trigger times: 14\n",
      "Epoch: 192, Loss: 1.2271 \n",
      "trigger times: 15\n",
      "Epoch: 193, Loss: 1.2413 \n",
      "trigger times: 16\n",
      "Epoch: 194, Loss: 1.2094 \n",
      "trigger times: 17\n",
      "Epoch: 195, Loss: 1.2804 \n",
      "trigger times: 18\n",
      "Epoch: 196, Loss: 1.2613 \n",
      "trigger times: 19\n",
      "Epoch: 197, Loss: 1.1993 \n",
      "trigger times: 20\n",
      "Epoch: 198, Loss: 1.2366 \n",
      "trigger times: 21\n",
      "Epoch: 199, Loss: 1.2780 \n",
      "trigger times: 22\n",
      "Epoch: 000, Loss: 1.4372 \n",
      "Epoch: 001, Loss: 1.2306 \n",
      "Epoch: 002, Loss: 1.2729 \n",
      "trigger times: 1\n",
      "Epoch: 003, Loss: 1.2711 \n",
      "trigger times: 2\n",
      "Epoch: 004, Loss: 1.2254 \n",
      "Epoch: 005, Loss: 1.2101 \n",
      "Epoch: 006, Loss: 1.2755 \n",
      "trigger times: 1\n",
      "Epoch: 007, Loss: 1.2776 \n",
      "trigger times: 2\n",
      "Epoch: 008, Loss: 1.2354 \n",
      "trigger times: 3\n",
      "Epoch: 009, Loss: 1.2642 \n",
      "trigger times: 4\n",
      "Epoch: 010, Loss: 1.2618 \n",
      "trigger times: 5\n",
      "Epoch: 011, Loss: 1.2465 \n",
      "trigger times: 6\n",
      "Epoch: 012, Loss: 1.3010 \n",
      "trigger times: 7\n",
      "Epoch: 013, Loss: 1.3165 \n",
      "trigger times: 8\n",
      "Epoch: 014, Loss: 1.2567 \n",
      "trigger times: 9\n",
      "Epoch: 015, Loss: 1.2770 \n",
      "trigger times: 10\n",
      "Epoch: 016, Loss: 1.2089 \n",
      "Epoch: 017, Loss: 1.2892 \n",
      "trigger times: 1\n",
      "Epoch: 018, Loss: 1.3019 \n",
      "trigger times: 2\n",
      "Epoch: 019, Loss: 1.2556 \n",
      "trigger times: 3\n",
      "Epoch: 020, Loss: 1.2011 \n",
      "Epoch: 021, Loss: 1.2506 \n",
      "trigger times: 1\n",
      "Epoch: 022, Loss: 1.2984 \n",
      "trigger times: 2\n",
      "Epoch: 023, Loss: 1.2256 \n",
      "trigger times: 3\n",
      "Epoch: 024, Loss: 1.2374 \n",
      "trigger times: 4\n",
      "Epoch: 025, Loss: 1.2739 \n",
      "trigger times: 5\n",
      "Epoch: 026, Loss: 1.2457 \n",
      "trigger times: 6\n",
      "Epoch: 027, Loss: 1.3711 \n",
      "trigger times: 7\n",
      "Epoch: 028, Loss: 1.2408 \n",
      "trigger times: 8\n",
      "Epoch: 029, Loss: 1.2577 \n",
      "trigger times: 9\n",
      "Epoch: 030, Loss: 1.2112 \n",
      "trigger times: 10\n",
      "Epoch: 031, Loss: 1.2608 \n",
      "trigger times: 11\n",
      "Epoch: 032, Loss: 1.2627 \n",
      "trigger times: 12\n",
      "Epoch: 033, Loss: 1.3217 \n",
      "trigger times: 13\n",
      "Epoch: 034, Loss: 1.2816 \n",
      "trigger times: 14\n",
      "Epoch: 035, Loss: 1.2829 \n",
      "trigger times: 15\n",
      "Epoch: 036, Loss: 1.2297 \n",
      "trigger times: 16\n",
      "Epoch: 037, Loss: 1.2170 \n",
      "trigger times: 17\n",
      "Epoch: 038, Loss: 1.3167 \n",
      "trigger times: 18\n",
      "Epoch: 039, Loss: 1.2234 \n",
      "trigger times: 19\n",
      "Epoch: 040, Loss: 1.1990 \n",
      "Epoch: 041, Loss: 1.2241 \n",
      "trigger times: 1\n",
      "Epoch: 042, Loss: 1.2929 \n",
      "trigger times: 2\n",
      "Epoch: 043, Loss: 1.2897 \n",
      "trigger times: 3\n",
      "Epoch: 044, Loss: 1.2307 \n",
      "trigger times: 4\n",
      "Epoch: 045, Loss: 1.2767 \n",
      "trigger times: 5\n",
      "Epoch: 046, Loss: 1.2698 \n",
      "trigger times: 6\n",
      "Epoch: 047, Loss: 1.2291 \n",
      "trigger times: 7\n",
      "Epoch: 048, Loss: 1.2247 \n",
      "trigger times: 8\n",
      "Epoch: 049, Loss: 1.2328 \n",
      "trigger times: 9\n",
      "Epoch: 050, Loss: 1.2431 \n",
      "trigger times: 10\n",
      "Epoch: 051, Loss: 2.3364 \n",
      "trigger times: 11\n",
      "Epoch: 052, Loss: 1.3166 \n",
      "trigger times: 12\n",
      "Epoch: 053, Loss: 1.1863 \n",
      "Epoch: 054, Loss: 1.1248 \n",
      "Epoch: 055, Loss: 1.1388 \n",
      "trigger times: 1\n",
      "Epoch: 056, Loss: 1.1492 \n",
      "trigger times: 2\n",
      "Epoch: 057, Loss: 1.1870 \n",
      "trigger times: 3\n",
      "Epoch: 058, Loss: 1.1340 \n",
      "trigger times: 4\n",
      "Epoch: 059, Loss: 1.1433 \n",
      "trigger times: 5\n",
      "Epoch: 060, Loss: 1.2060 \n",
      "trigger times: 6\n",
      "Epoch: 061, Loss: 1.1462 \n",
      "trigger times: 7\n",
      "Epoch: 062, Loss: 1.1599 \n",
      "trigger times: 8\n",
      "Epoch: 063, Loss: 1.1519 \n",
      "trigger times: 9\n",
      "Epoch: 064, Loss: 1.1632 \n",
      "trigger times: 10\n",
      "Epoch: 065, Loss: 1.1927 \n",
      "trigger times: 11\n",
      "Epoch: 066, Loss: 1.1912 \n",
      "trigger times: 12\n",
      "Epoch: 067, Loss: 1.1792 \n",
      "trigger times: 13\n",
      "Epoch: 068, Loss: 1.2051 \n",
      "trigger times: 14\n",
      "Epoch: 069, Loss: 1.2006 \n",
      "trigger times: 15\n",
      "Epoch: 070, Loss: 1.2155 \n",
      "trigger times: 16\n",
      "Epoch: 071, Loss: 1.2907 \n",
      "trigger times: 17\n",
      "Epoch: 072, Loss: 1.1703 \n",
      "trigger times: 18\n",
      "Epoch: 073, Loss: 1.1927 \n",
      "trigger times: 19\n",
      "Epoch: 074, Loss: 1.2551 \n",
      "trigger times: 20\n",
      "Epoch: 075, Loss: 1.3325 \n",
      "trigger times: 21\n",
      "Epoch: 076, Loss: 1.2353 \n",
      "trigger times: 22\n",
      "Epoch: 077, Loss: 1.2689 \n",
      "trigger times: 23\n",
      "Epoch: 078, Loss: 1.3441 \n",
      "trigger times: 24\n",
      "Epoch: 079, Loss: 1.2391 \n",
      "trigger times: 25\n",
      "Epoch: 080, Loss: 1.2463 \n",
      "trigger times: 26\n",
      "Epoch: 081, Loss: 1.2575 \n",
      "trigger times: 27\n",
      "Epoch: 082, Loss: 1.2503 \n",
      "trigger times: 28\n",
      "Epoch: 083, Loss: 1.1949 \n",
      "trigger times: 29\n",
      "Epoch: 084, Loss: 1.2598 \n",
      "trigger times: 30\n",
      "Epoch: 085, Loss: 1.2811 \n",
      "trigger times: 31\n",
      "Epoch: 086, Loss: 1.3168 \n",
      "trigger times: 32\n",
      "Epoch: 087, Loss: 1.2314 \n",
      "trigger times: 33\n",
      "Epoch: 088, Loss: 1.1937 \n",
      "trigger times: 34\n",
      "Epoch: 089, Loss: 1.2470 \n",
      "trigger times: 35\n",
      "Epoch: 090, Loss: 1.1814 \n",
      "trigger times: 36\n",
      "Epoch: 091, Loss: 1.3147 \n",
      "trigger times: 37\n",
      "Epoch: 092, Loss: 1.3708 \n",
      "trigger times: 38\n",
      "Epoch: 093, Loss: 1.2676 \n",
      "trigger times: 39\n",
      "Epoch: 094, Loss: 1.2134 \n",
      "trigger times: 40\n",
      "Epoch: 095, Loss: 1.2070 \n",
      "trigger times: 41\n",
      "Epoch: 096, Loss: 1.2393 \n",
      "trigger times: 42\n",
      "Epoch: 097, Loss: 1.2158 \n",
      "trigger times: 43\n",
      "Epoch: 098, Loss: 1.2249 \n",
      "trigger times: 44\n",
      "Epoch: 099, Loss: 1.2245 \n",
      "trigger times: 45\n",
      "Epoch: 100, Loss: 1.2898 \n",
      "trigger times: 46\n",
      "Epoch: 101, Loss: 1.2159 \n",
      "trigger times: 47\n",
      "Epoch: 102, Loss: 1.2705 \n",
      "trigger times: 48\n",
      "Epoch: 103, Loss: 1.2231 \n",
      "trigger times: 49\n",
      "Epoch: 104, Loss: 1.2338 \n",
      "trigger times: 50\n",
      "Epoch: 105, Loss: 1.2253 \n",
      "trigger times: 51\n",
      "Epoch: 106, Loss: 1.2293 \n",
      "trigger times: 52\n",
      "Epoch: 107, Loss: 1.2456 \n",
      "trigger times: 53\n",
      "Epoch: 108, Loss: 1.2803 \n",
      "trigger times: 54\n",
      "Epoch: 109, Loss: 1.2488 \n",
      "trigger times: 55\n",
      "Epoch: 110, Loss: 1.3243 \n",
      "trigger times: 56\n",
      "Epoch: 111, Loss: 1.4228 \n",
      "trigger times: 57\n",
      "Epoch: 112, Loss: 1.1980 \n",
      "trigger times: 58\n",
      "Epoch: 113, Loss: 1.2700 \n",
      "trigger times: 59\n",
      "Epoch: 114, Loss: 1.2196 \n",
      "trigger times: 60\n",
      "Epoch: 115, Loss: 1.2651 \n",
      "trigger times: 61\n",
      "Epoch: 116, Loss: 1.2389 \n",
      "trigger times: 62\n",
      "Epoch: 117, Loss: 1.1993 \n",
      "trigger times: 63\n",
      "Epoch: 118, Loss: 1.2511 \n",
      "trigger times: 64\n",
      "Epoch: 119, Loss: 1.2684 \n",
      "trigger times: 65\n",
      "Epoch: 120, Loss: 1.3581 \n",
      "trigger times: 66\n",
      "Epoch: 121, Loss: 1.3015 \n",
      "trigger times: 67\n",
      "Epoch: 122, Loss: 1.2778 \n",
      "trigger times: 68\n",
      "Epoch: 123, Loss: 1.2343 \n",
      "trigger times: 69\n",
      "Epoch: 124, Loss: 1.2292 \n",
      "trigger times: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 1.2227 \n",
      "trigger times: 71\n",
      "Epoch: 126, Loss: 1.2632 \n",
      "trigger times: 72\n",
      "Epoch: 127, Loss: 1.1817 \n",
      "trigger times: 73\n",
      "Epoch: 128, Loss: 1.2985 \n",
      "trigger times: 74\n",
      "Epoch: 129, Loss: 1.2762 \n",
      "trigger times: 75\n",
      "Epoch: 130, Loss: 1.2579 \n",
      "trigger times: 76\n",
      "Epoch: 131, Loss: 1.2276 \n",
      "trigger times: 77\n",
      "Epoch: 132, Loss: 1.2158 \n",
      "trigger times: 78\n",
      "Epoch: 133, Loss: 1.1972 \n",
      "trigger times: 79\n",
      "Epoch: 134, Loss: 1.2213 \n",
      "trigger times: 80\n",
      "Epoch: 135, Loss: 1.2257 \n",
      "trigger times: 81\n",
      "Epoch: 136, Loss: 1.2163 \n",
      "trigger times: 82\n",
      "Epoch: 137, Loss: 1.2718 \n",
      "trigger times: 83\n",
      "Epoch: 138, Loss: 1.3043 \n",
      "trigger times: 84\n",
      "Epoch: 139, Loss: 1.1930 \n",
      "trigger times: 85\n",
      "Epoch: 140, Loss: 1.2437 \n",
      "trigger times: 86\n",
      "Epoch: 141, Loss: 1.2413 \n",
      "trigger times: 87\n",
      "Epoch: 142, Loss: 1.1785 \n",
      "trigger times: 88\n",
      "Epoch: 143, Loss: 1.2020 \n",
      "trigger times: 89\n",
      "Epoch: 144, Loss: 1.2230 \n",
      "trigger times: 90\n",
      "Epoch: 145, Loss: 1.2459 \n",
      "trigger times: 91\n",
      "Epoch: 146, Loss: 1.2352 \n",
      "trigger times: 92\n",
      "Epoch: 147, Loss: 1.2398 \n",
      "trigger times: 93\n",
      "Epoch: 148, Loss: 1.2219 \n",
      "trigger times: 94\n",
      "Epoch: 149, Loss: 1.2375 \n",
      "trigger times: 95\n",
      "Epoch: 150, Loss: 1.3350 \n",
      "trigger times: 96\n",
      "Epoch: 151, Loss: 1.2917 \n",
      "trigger times: 97\n",
      "Epoch: 152, Loss: 1.3030 \n",
      "trigger times: 98\n",
      "Epoch: 153, Loss: 1.4155 \n",
      "trigger times: 99\n",
      "Epoch: 154, Loss: 1.2073 \n",
      "trigger times: 100\n",
      "Epoch: 155, Loss: 1.2173 \n",
      "trigger times: 101\n",
      "Epoch: 156, Loss: 1.2126 \n",
      "trigger times: 102\n",
      "Epoch: 157, Loss: 1.2032 \n",
      "trigger times: 103\n",
      "Epoch: 158, Loss: 1.2067 \n",
      "trigger times: 104\n",
      "Epoch: 159, Loss: 1.2382 \n",
      "trigger times: 105\n",
      "Epoch: 160, Loss: 1.2211 \n",
      "trigger times: 106\n",
      "Epoch: 161, Loss: 1.2079 \n",
      "trigger times: 107\n",
      "Epoch: 162, Loss: 1.2398 \n",
      "trigger times: 108\n",
      "Epoch: 163, Loss: 1.2543 \n",
      "trigger times: 109\n",
      "Epoch: 164, Loss: 1.1766 \n",
      "trigger times: 110\n",
      "Epoch: 165, Loss: 1.2649 \n",
      "trigger times: 111\n",
      "Epoch: 166, Loss: 1.2560 \n",
      "trigger times: 112\n",
      "Epoch: 167, Loss: 1.2319 \n",
      "trigger times: 113\n",
      "Epoch: 168, Loss: 1.3247 \n",
      "trigger times: 114\n",
      "Epoch: 169, Loss: 1.3411 \n",
      "trigger times: 115\n",
      "Epoch: 170, Loss: 1.2236 \n",
      "trigger times: 116\n",
      "Epoch: 171, Loss: 1.2044 \n",
      "trigger times: 117\n",
      "Epoch: 172, Loss: 1.2189 \n",
      "trigger times: 118\n",
      "Epoch: 173, Loss: 1.3095 \n",
      "trigger times: 119\n",
      "Epoch: 174, Loss: 1.2398 \n",
      "trigger times: 120\n",
      "Epoch: 175, Loss: 1.2297 \n",
      "trigger times: 121\n",
      "Epoch: 176, Loss: 1.2469 \n",
      "trigger times: 122\n",
      "Epoch: 177, Loss: 1.2147 \n",
      "trigger times: 123\n",
      "Epoch: 178, Loss: 1.2455 \n",
      "trigger times: 124\n",
      "Epoch: 179, Loss: 1.2689 \n",
      "trigger times: 125\n",
      "Epoch: 180, Loss: 1.2941 \n",
      "trigger times: 126\n",
      "Epoch: 181, Loss: 1.2553 \n",
      "trigger times: 127\n",
      "Epoch: 182, Loss: 1.2201 \n",
      "trigger times: 128\n",
      "Epoch: 183, Loss: 1.3317 \n",
      "trigger times: 129\n",
      "Epoch: 184, Loss: 1.2711 \n",
      "trigger times: 130\n",
      "Epoch: 185, Loss: 1.2043 \n",
      "trigger times: 131\n",
      "Epoch: 186, Loss: 1.2164 \n",
      "trigger times: 132\n",
      "Epoch: 187, Loss: 1.2375 \n",
      "trigger times: 133\n",
      "Epoch: 188, Loss: 1.1896 \n",
      "trigger times: 134\n",
      "Epoch: 189, Loss: 1.1982 \n",
      "trigger times: 135\n",
      "Epoch: 190, Loss: 1.2356 \n",
      "trigger times: 136\n",
      "Epoch: 191, Loss: 1.1919 \n",
      "trigger times: 137\n",
      "Epoch: 192, Loss: 1.2008 \n",
      "trigger times: 138\n",
      "Epoch: 193, Loss: 1.3246 \n",
      "trigger times: 139\n",
      "Epoch: 194, Loss: 1.2261 \n",
      "trigger times: 140\n",
      "Epoch: 195, Loss: 1.2236 \n",
      "trigger times: 141\n",
      "Epoch: 196, Loss: 1.2985 \n",
      "trigger times: 142\n",
      "Epoch: 197, Loss: 1.2765 \n",
      "trigger times: 143\n",
      "Epoch: 198, Loss: 1.2087 \n",
      "trigger times: 144\n",
      "Epoch: 199, Loss: 1.2926 \n",
      "trigger times: 145\n",
      "Epoch: 000, Loss: 1.4385 \n",
      "Epoch: 001, Loss: 1.2762 \n",
      "Epoch: 002, Loss: 1.2147 \n",
      "Epoch: 003, Loss: 1.2620 \n",
      "trigger times: 1\n",
      "Epoch: 004, Loss: 1.2723 \n",
      "trigger times: 2\n",
      "Epoch: 005, Loss: 1.3010 \n",
      "trigger times: 3\n",
      "Epoch: 006, Loss: 1.2407 \n",
      "trigger times: 4\n",
      "Epoch: 007, Loss: 1.2887 \n",
      "trigger times: 5\n",
      "Epoch: 008, Loss: 1.3140 \n",
      "trigger times: 6\n",
      "Epoch: 009, Loss: 1.2614 \n",
      "trigger times: 7\n",
      "Epoch: 010, Loss: 1.2574 \n",
      "trigger times: 8\n",
      "Epoch: 011, Loss: 1.2541 \n",
      "trigger times: 9\n",
      "Epoch: 012, Loss: 1.2753 \n",
      "trigger times: 10\n",
      "Epoch: 013, Loss: 1.2859 \n",
      "trigger times: 11\n",
      "Epoch: 014, Loss: 1.3450 \n",
      "trigger times: 12\n",
      "Epoch: 015, Loss: 1.2734 \n",
      "trigger times: 13\n",
      "Epoch: 016, Loss: 1.2323 \n",
      "trigger times: 14\n",
      "Epoch: 017, Loss: 1.2651 \n",
      "trigger times: 15\n",
      "Epoch: 018, Loss: 1.2550 \n",
      "trigger times: 16\n",
      "Epoch: 019, Loss: 1.2533 \n",
      "trigger times: 17\n",
      "Epoch: 020, Loss: 1.3241 \n",
      "trigger times: 18\n",
      "Epoch: 021, Loss: 1.2236 \n",
      "trigger times: 19\n",
      "Epoch: 022, Loss: 1.2374 \n",
      "trigger times: 20\n",
      "Epoch: 023, Loss: 1.2970 \n",
      "trigger times: 21\n",
      "Epoch: 024, Loss: 1.3249 \n",
      "trigger times: 22\n",
      "Epoch: 025, Loss: 1.2687 \n",
      "trigger times: 23\n",
      "Epoch: 026, Loss: 1.2398 \n",
      "trigger times: 24\n",
      "Epoch: 027, Loss: 1.2370 \n",
      "trigger times: 25\n",
      "Epoch: 028, Loss: 1.3248 \n",
      "trigger times: 26\n",
      "Epoch: 029, Loss: 1.2509 \n",
      "trigger times: 27\n",
      "Epoch: 030, Loss: 1.2207 \n",
      "trigger times: 28\n",
      "Epoch: 031, Loss: 1.2808 \n",
      "trigger times: 29\n",
      "Epoch: 032, Loss: 1.5130 \n",
      "trigger times: 30\n",
      "Epoch: 033, Loss: 1.2038 \n",
      "Epoch: 034, Loss: 1.2305 \n",
      "trigger times: 1\n",
      "Epoch: 035, Loss: 1.2346 \n",
      "trigger times: 2\n",
      "Epoch: 036, Loss: 1.2164 \n",
      "trigger times: 3\n",
      "Epoch: 037, Loss: 1.2457 \n",
      "trigger times: 4\n",
      "Epoch: 038, Loss: 1.2386 \n",
      "trigger times: 5\n",
      "Epoch: 039, Loss: 1.1876 \n",
      "Epoch: 040, Loss: 1.2494 \n",
      "trigger times: 1\n",
      "Epoch: 041, Loss: 1.2123 \n",
      "trigger times: 2\n",
      "Epoch: 042, Loss: 1.2110 \n",
      "trigger times: 3\n",
      "Epoch: 043, Loss: 1.2671 \n",
      "trigger times: 4\n",
      "Epoch: 044, Loss: 1.2222 \n",
      "trigger times: 5\n",
      "Epoch: 045, Loss: 1.2879 \n",
      "trigger times: 6\n",
      "Epoch: 046, Loss: 1.2691 \n",
      "trigger times: 7\n",
      "Epoch: 047, Loss: 1.2502 \n",
      "trigger times: 8\n",
      "Epoch: 048, Loss: 1.1849 \n",
      "Epoch: 049, Loss: 1.2992 \n",
      "trigger times: 1\n",
      "Epoch: 050, Loss: 1.3713 \n",
      "trigger times: 2\n",
      "Epoch: 051, Loss: 1.2269 \n",
      "trigger times: 3\n",
      "Epoch: 052, Loss: 1.2571 \n",
      "trigger times: 4\n",
      "Epoch: 053, Loss: 1.2092 \n",
      "trigger times: 5\n",
      "Epoch: 054, Loss: 1.2414 \n",
      "trigger times: 6\n",
      "Epoch: 055, Loss: 1.2274 \n",
      "trigger times: 7\n",
      "Epoch: 056, Loss: 1.2621 \n",
      "trigger times: 8\n",
      "Epoch: 057, Loss: 1.2743 \n",
      "trigger times: 9\n",
      "Epoch: 058, Loss: 1.2847 \n",
      "trigger times: 10\n",
      "Epoch: 059, Loss: 1.2594 \n",
      "trigger times: 11\n",
      "Epoch: 060, Loss: 1.2419 \n",
      "trigger times: 12\n",
      "Epoch: 061, Loss: 1.3064 \n",
      "trigger times: 13\n",
      "Epoch: 062, Loss: 1.1929 \n",
      "trigger times: 14\n",
      "Epoch: 063, Loss: 1.2122 \n",
      "trigger times: 15\n",
      "Epoch: 064, Loss: 1.2316 \n",
      "trigger times: 16\n",
      "Epoch: 065, Loss: 1.2267 \n",
      "trigger times: 17\n",
      "Epoch: 066, Loss: 1.2297 \n",
      "trigger times: 18\n",
      "Epoch: 067, Loss: 1.2484 \n",
      "trigger times: 19\n",
      "Epoch: 068, Loss: 1.2460 \n",
      "trigger times: 20\n",
      "Epoch: 069, Loss: 1.2201 \n",
      "trigger times: 21\n",
      "Epoch: 070, Loss: 1.3735 \n",
      "trigger times: 22\n",
      "Epoch: 071, Loss: 1.2793 \n",
      "trigger times: 23\n",
      "Epoch: 072, Loss: 1.2520 \n",
      "trigger times: 24\n",
      "Epoch: 073, Loss: 1.1960 \n",
      "trigger times: 25\n",
      "Epoch: 074, Loss: 1.2241 \n",
      "trigger times: 26\n",
      "Epoch: 075, Loss: 1.2605 \n",
      "trigger times: 27\n",
      "Epoch: 076, Loss: 1.2520 \n",
      "trigger times: 28\n",
      "Epoch: 077, Loss: 1.3279 \n",
      "trigger times: 29\n",
      "Epoch: 078, Loss: 1.2210 \n",
      "trigger times: 30\n",
      "Epoch: 079, Loss: 1.1947 \n",
      "trigger times: 31\n",
      "Epoch: 080, Loss: 1.2019 \n",
      "trigger times: 32\n",
      "Epoch: 081, Loss: 1.2303 \n",
      "trigger times: 33\n",
      "Epoch: 082, Loss: 1.2422 \n",
      "trigger times: 34\n",
      "Epoch: 083, Loss: 1.2465 \n",
      "trigger times: 35\n",
      "Epoch: 084, Loss: 1.2594 \n",
      "trigger times: 36\n",
      "Epoch: 085, Loss: 1.1901 \n",
      "trigger times: 37\n",
      "Epoch: 086, Loss: 1.2333 \n",
      "trigger times: 38\n",
      "Epoch: 087, Loss: 1.3192 \n",
      "trigger times: 39\n",
      "Epoch: 088, Loss: 1.2667 \n",
      "trigger times: 40\n",
      "Epoch: 089, Loss: 1.2157 \n",
      "trigger times: 41\n",
      "Epoch: 090, Loss: 1.2301 \n",
      "trigger times: 42\n",
      "Epoch: 091, Loss: 1.2307 \n",
      "trigger times: 43\n",
      "Epoch: 092, Loss: 1.2146 \n",
      "trigger times: 44\n",
      "Epoch: 093, Loss: 1.2606 \n",
      "trigger times: 45\n",
      "Epoch: 094, Loss: 1.2740 \n",
      "trigger times: 46\n",
      "Epoch: 095, Loss: 1.2585 \n",
      "trigger times: 47\n",
      "Epoch: 096, Loss: 1.2445 \n",
      "trigger times: 48\n",
      "Epoch: 097, Loss: 1.2300 \n",
      "trigger times: 49\n",
      "Epoch: 098, Loss: 1.2811 \n",
      "trigger times: 50\n",
      "Epoch: 099, Loss: 1.2100 \n",
      "trigger times: 51\n",
      "Epoch: 100, Loss: 1.2139 \n",
      "trigger times: 52\n",
      "Epoch: 101, Loss: 1.2246 \n",
      "trigger times: 53\n",
      "Epoch: 102, Loss: 1.3065 \n",
      "trigger times: 54\n",
      "Epoch: 103, Loss: 1.3552 \n",
      "trigger times: 55\n",
      "Epoch: 104, Loss: 1.2638 \n",
      "trigger times: 56\n",
      "Epoch: 105, Loss: 1.2381 \n",
      "trigger times: 57\n",
      "Epoch: 106, Loss: 1.1837 \n",
      "Epoch: 107, Loss: 1.2827 \n",
      "trigger times: 1\n",
      "Epoch: 108, Loss: 1.2287 \n",
      "trigger times: 2\n",
      "Epoch: 109, Loss: 1.2467 \n",
      "trigger times: 3\n",
      "Epoch: 110, Loss: 1.2615 \n",
      "trigger times: 4\n",
      "Epoch: 111, Loss: 1.2201 \n",
      "trigger times: 5\n",
      "Epoch: 112, Loss: 1.2710 \n",
      "trigger times: 6\n",
      "Epoch: 113, Loss: 1.2864 \n",
      "trigger times: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Loss: 1.2567 \n",
      "trigger times: 8\n",
      "Epoch: 115, Loss: 1.2407 \n",
      "trigger times: 9\n",
      "Epoch: 116, Loss: 1.3123 \n",
      "trigger times: 10\n",
      "Epoch: 117, Loss: 1.2391 \n",
      "trigger times: 11\n",
      "Epoch: 118, Loss: 1.2208 \n",
      "trigger times: 12\n",
      "Epoch: 119, Loss: 1.2893 \n",
      "trigger times: 13\n",
      "Epoch: 120, Loss: 1.3509 \n",
      "trigger times: 14\n",
      "Epoch: 121, Loss: 1.2198 \n",
      "trigger times: 15\n",
      "Epoch: 122, Loss: 1.1869 \n",
      "trigger times: 16\n",
      "Epoch: 123, Loss: 1.2188 \n",
      "trigger times: 17\n",
      "Epoch: 124, Loss: 1.2582 \n",
      "trigger times: 18\n",
      "Epoch: 125, Loss: 1.2827 \n",
      "trigger times: 19\n",
      "Epoch: 126, Loss: 1.2402 \n",
      "trigger times: 20\n",
      "Epoch: 127, Loss: 1.3486 \n",
      "trigger times: 21\n",
      "Epoch: 128, Loss: 1.1821 \n",
      "Epoch: 129, Loss: 1.2206 \n",
      "trigger times: 1\n",
      "Epoch: 130, Loss: 1.2695 \n",
      "trigger times: 2\n",
      "Epoch: 131, Loss: 1.1939 \n",
      "trigger times: 3\n",
      "Epoch: 132, Loss: 1.2534 \n",
      "trigger times: 4\n",
      "Epoch: 133, Loss: 1.3570 \n",
      "trigger times: 5\n",
      "Epoch: 134, Loss: 1.2325 \n",
      "trigger times: 6\n",
      "Epoch: 135, Loss: 1.2116 \n",
      "trigger times: 7\n",
      "Epoch: 136, Loss: 1.1551 \n",
      "Epoch: 137, Loss: 1.2367 \n",
      "trigger times: 1\n",
      "Epoch: 138, Loss: 1.3380 \n",
      "trigger times: 2\n",
      "Epoch: 139, Loss: 1.3034 \n",
      "trigger times: 3\n",
      "Epoch: 140, Loss: 1.2203 \n",
      "trigger times: 4\n",
      "Epoch: 141, Loss: 1.2276 \n",
      "trigger times: 5\n",
      "Epoch: 142, Loss: 1.2298 \n",
      "trigger times: 6\n",
      "Epoch: 143, Loss: 1.2355 \n",
      "trigger times: 7\n",
      "Epoch: 144, Loss: 1.1937 \n",
      "trigger times: 8\n",
      "Epoch: 145, Loss: 1.1661 \n",
      "trigger times: 9\n",
      "Epoch: 146, Loss: 1.2638 \n",
      "trigger times: 10\n",
      "Epoch: 147, Loss: 1.2819 \n",
      "trigger times: 11\n",
      "Epoch: 148, Loss: 1.2904 \n",
      "trigger times: 12\n",
      "Epoch: 149, Loss: 1.2311 \n",
      "trigger times: 13\n",
      "Epoch: 150, Loss: 1.3588 \n",
      "trigger times: 14\n",
      "Epoch: 151, Loss: 1.2816 \n",
      "trigger times: 15\n",
      "Epoch: 152, Loss: 1.2639 \n",
      "trigger times: 16\n",
      "Epoch: 153, Loss: 1.2262 \n",
      "trigger times: 17\n",
      "Epoch: 154, Loss: 1.2570 \n",
      "trigger times: 18\n",
      "Epoch: 155, Loss: 1.2205 \n",
      "trigger times: 19\n",
      "Epoch: 156, Loss: 1.2360 \n",
      "trigger times: 20\n",
      "Epoch: 157, Loss: 1.1562 \n",
      "trigger times: 21\n",
      "Epoch: 158, Loss: 1.2412 \n",
      "trigger times: 22\n",
      "Epoch: 159, Loss: 1.1961 \n",
      "trigger times: 23\n",
      "Epoch: 160, Loss: 1.2989 \n",
      "trigger times: 24\n",
      "Epoch: 161, Loss: 1.2862 \n",
      "trigger times: 25\n",
      "Epoch: 162, Loss: 1.2210 \n",
      "trigger times: 26\n",
      "Epoch: 163, Loss: 1.2106 \n",
      "trigger times: 27\n",
      "Epoch: 164, Loss: 1.3042 \n",
      "trigger times: 28\n",
      "Epoch: 165, Loss: 1.1887 \n",
      "trigger times: 29\n",
      "Epoch: 166, Loss: 1.2088 \n",
      "trigger times: 30\n",
      "Epoch: 167, Loss: 1.2427 \n",
      "trigger times: 31\n",
      "Epoch: 168, Loss: 1.2126 \n",
      "trigger times: 32\n",
      "Epoch: 169, Loss: 1.1839 \n",
      "trigger times: 33\n",
      "Epoch: 170, Loss: 1.2292 \n",
      "trigger times: 34\n",
      "Epoch: 171, Loss: 1.2683 \n",
      "trigger times: 35\n",
      "Epoch: 172, Loss: 1.2526 \n",
      "trigger times: 36\n",
      "Epoch: 173, Loss: 1.2379 \n",
      "trigger times: 37\n",
      "Epoch: 174, Loss: 1.2601 \n",
      "trigger times: 38\n",
      "Epoch: 175, Loss: 1.2142 \n",
      "trigger times: 39\n",
      "Epoch: 176, Loss: 1.2388 \n",
      "trigger times: 40\n",
      "Epoch: 177, Loss: 1.3209 \n",
      "trigger times: 41\n",
      "Epoch: 178, Loss: 1.2284 \n",
      "trigger times: 42\n",
      "Epoch: 179, Loss: 1.2715 \n",
      "trigger times: 43\n",
      "Epoch: 180, Loss: 1.2598 \n",
      "trigger times: 44\n",
      "Epoch: 181, Loss: 1.2844 \n",
      "trigger times: 45\n",
      "Epoch: 182, Loss: 1.3054 \n",
      "trigger times: 46\n",
      "Epoch: 183, Loss: 1.2539 \n",
      "trigger times: 47\n",
      "Epoch: 184, Loss: 1.2034 \n",
      "trigger times: 48\n",
      "Epoch: 185, Loss: 1.2487 \n",
      "trigger times: 49\n",
      "Epoch: 186, Loss: 1.2049 \n",
      "trigger times: 50\n",
      "Epoch: 187, Loss: 1.2011 \n",
      "trigger times: 51\n",
      "Epoch: 188, Loss: 1.2776 \n",
      "trigger times: 52\n",
      "Epoch: 189, Loss: 1.2783 \n",
      "trigger times: 53\n",
      "Epoch: 190, Loss: 1.2409 \n",
      "trigger times: 54\n",
      "Epoch: 191, Loss: 1.2336 \n",
      "trigger times: 55\n",
      "Epoch: 192, Loss: 1.1971 \n",
      "trigger times: 56\n",
      "Epoch: 193, Loss: 1.3935 \n",
      "trigger times: 57\n",
      "Epoch: 194, Loss: 1.2392 \n",
      "trigger times: 58\n",
      "Epoch: 195, Loss: 1.2853 \n",
      "trigger times: 59\n",
      "Epoch: 196, Loss: 1.2293 \n",
      "trigger times: 60\n",
      "Epoch: 197, Loss: 1.1725 \n",
      "trigger times: 61\n",
      "Epoch: 198, Loss: 1.1998 \n",
      "trigger times: 62\n",
      "Epoch: 199, Loss: 1.1607 \n",
      "trigger times: 63\n"
     ]
    }
   ],
   "source": [
    "# training whole dataset. first loading each fold model and train on whole dataset\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "wholetrain_data = createTestData('Data_Prep','solubility_1.csv','solubility_1')\n",
    "for fold in tqdm(range(folds)):\n",
    "    wholetrain_loader  = DataLoader(wholetrain_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    # model = define_model(trial).to(device)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=.0023467,\n",
    "    #                              weight_decay=.00095)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.007531829,\n",
    "#                                    weight_decay=0.000252036)\n",
    "#     model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "#                     edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "#                     dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    load_model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "    model_file_name = 'saved_models/wholetrainmodel_' +  str(fold) +  '.model'\n",
    "    result_file_name = 'wholetrainresult_' + str(fold) +  '.csv'\n",
    "    checkpoint = torch.load(load_model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "#     test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    \n",
    "#     TRAIN_BATCH_SIZE = 64\n",
    "    \n",
    "#     train_loader   = DataLoader(train_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     test_loader  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     model = AttentiveFP(in_channels=112, hidden_channels=279, out_channels=1,\n",
    "#                     num_layers=3, num_timesteps=2,\n",
    "#                     dropout=0.047352327938708194).to(device)\n",
    "    best_ret = []\n",
    "    \n",
    "#     model = model.cuda(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "#     best_mae = 0.00\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss,train_rmse=train(model, optimizer,train_loader)\n",
    "#         test_loss,test_rmse = test(test_loader, model)\n",
    "#         score = metrics.r2_score(true, prediction)\n",
    "#         , true, prediction\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} '\n",
    "          ) #f'Test: {test_rmse:.4f} 'f'score: {score:.4f} '   \n",
    "        \n",
    "        ret = [epoch,train_rmse]\n",
    "        \n",
    "        train_losses.append(train_rmse)\n",
    "#         val_losses.append(test_rmse)\n",
    "#         scores.append(score)\n",
    "        # Early Stopping\n",
    "        the_current_loss = train_rmse   #.item()\n",
    "        best_ret.append(ret)\n",
    "        if the_current_loss > the_last_loss:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "    \n",
    "            if trigger_times >= 200:   #patience\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                break\n",
    "        else:\n",
    "            ret = [epoch,train_rmse] #, ,test_rmse, score\n",
    "            trigger_times = 0\n",
    "            the_last_loss = the_current_loss\n",
    "            best_rmse = the_current_loss\n",
    "            \n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "        # Early stopping\n",
    "#         the_current_loss = test_loss.item()\n",
    "        \n",
    "#         best_ret.append(ret)\n",
    "        \n",
    "#         if the_current_loss > the_last_loss:\n",
    "#             trigger_times += 1\n",
    "#             print('trigger times:', trigger_times)\n",
    "            \n",
    "#             if trigger_times >= patience:\n",
    "#                 print('Early stopping!\\nStart to test process.')\n",
    "#                 break\n",
    "#         else:\n",
    "#             ret = [epoch,train_loss,test_loss.item()]\n",
    "#             trigger_times = 0\n",
    "#             best_mae = the_current_loss\n",
    "#             the_last_loss = the_current_loss\n",
    "            \n",
    "#             torch.save(model.state_dict(), model_file_name)\n",
    "\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "noveltest_data = createTestData('New_fold','testset_novel.csv','testset_novel')\n",
    "# noveltest_data = Molecule_data(root='data', dataset='testset_novel',y=None,smile_graph=None,smiles=None)\n",
    "# noveltest_loader  = DataLoader(noveltest_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "#     model = AttentiveFP(in_channels=112, hidden_channels=279, out_channels=1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce85d112e92b487cb111cf871d47f3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2:  0.2441388813170452\n",
      "Test RMSE: 0.7720296743377252\n",
      "Test R2:  0.2066908162048513\n",
      "Test RMSE: 0.7909230495958045\n",
      "Test R2:  0.28779230190016036\n",
      "Test RMSE: 0.7494045914127789\n",
      "Test R2:  0.31317453902074344\n",
      "Test RMSE: 0.7359294267280441\n",
      "Test R2:  0.050747526463346904\n",
      "Test RMSE: 0.8651748188954784\n",
      "Test R2:  0.38613311740911693\n",
      "Test RMSE: 0.6957450121564744\n",
      "Test R2:  0.264583100733769\n",
      "Test RMSE: 0.7615173464832588\n",
      "Test R2:  -0.31521026387077855\n",
      "Test RMSE: 1.0183817496290868\n",
      "Test R2:  0.2970653041295014\n",
      "Test RMSE: 0.7445099482336007\n",
      "Test R2:  -1.9089318419911594\n",
      "Test RMSE: 1.5145365159023763\n"
     ]
    }
   ],
   "source": [
    "# test novel dataset on whole trained model.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "# fig = plt.figure()\n",
    "# for fold in tqdm(range(folds)):\n",
    "# val_losses = []\n",
    "# train_losses = []\n",
    "# mae_arr = []\n",
    "# patience = 30\n",
    "# trigger_times = 0\n",
    "# the_last_loss = 100\n",
    "for fold in tqdm(range(folds)):\n",
    "    noveltest_loader  = DataLoader(noveltest_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    # model = define_model(trial).to(device)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=.0023467,\n",
    "    #                              weight_decay=.00095)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.007531829,\n",
    "#                                    weight_decay=0.000252036)\n",
    "#     model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "#                     edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "#                     dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    model_file_name = 'saved_models/wholetrainmodel_' +  str(fold) +  '.model'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    test_loss,test_rmse, true, prediction = predicting(noveltest_loader, model)\n",
    "\n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('Test R2: ', score)\n",
    "    print('Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cb9b8e01ca4d268b1588352231f548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2:  -2.213837852121167\n",
      "Test RMSE: 1.5919336927158065\n",
      "Test R2:  -1.1998890269391462\n",
      "Test RMSE: 1.3170837560594881\n",
      "Test R2:  0.2443747764397144\n",
      "Test RMSE: 0.771909147735033\n",
      "Test R2:  -1.2368285572387094\n",
      "Test RMSE: 1.328095604066343\n",
      "Test R2:  -1.9071088578044404\n",
      "Test RMSE: 1.5140619772013577\n",
      "Test R2:  0.3000484569395985\n",
      "Test RMSE: 0.7429284663465879\n",
      "Test R2:  -0.7771527538869438\n",
      "Test RMSE: 1.1837919465863647\n",
      "Test R2:  0.4070820612865619\n",
      "Test RMSE: 0.6837704021541257\n",
      "Test R2:  -0.9272699348565427\n",
      "Test RMSE: 1.2327763309177067\n",
      "Test R2:  -0.4539832569231388\n",
      "Test RMSE: 1.0707613706837804\n"
     ]
    }
   ],
   "source": [
    "# test novel dataset on whole trained model.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "for fold in tqdm(range(folds)):\n",
    "    noveltest_loader  = DataLoader(noveltest_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    test_loss,test_rmse, true, prediction = predicting(noveltest_loader, model)\n",
    "\n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('Test R2: ', score)\n",
    "    print('Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Explainer test novel dataset on whole trained model.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "# fig = plt.figure()\n",
    "# for fold in tqdm(range(folds)):\n",
    "# val_losses = []\n",
    "# train_losses = []\n",
    "# mae_arr = []\n",
    "# patience = 30\n",
    "# trigger_times = 0\n",
    "# the_last_loss = 100\n",
    "for fold in tqdm(range(folds)):\n",
    "    noveltest_loader  = DataLoader(noveltest_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    # model = define_model(trial).to(device)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=.0023467,\n",
    "    #                              weight_decay=.00095)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.007531829,\n",
    "#                                    weight_decay=0.000252036)\n",
    "#     model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "#                     edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "#                     dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    model_file_name = 'saved_models/wholetrainmodel_' +  str(fold) +  '.model'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    test_loss,test_rmse, true, prediction = predicting(noveltest_loader, model)\n",
    "\n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('Test R2: ', score)\n",
    "    print('Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed data data/processed/DLS100.pt not found, doing pre-processing...\n",
      "Converting SMILES to graph: 1/100\n",
      "Converting SMILES to graph: 2/100\n",
      "Converting SMILES to graph: 3/100\n",
      "Converting SMILES to graph: 4/100\n",
      "Converting SMILES to graph: 5/100\n",
      "Converting SMILES to graph: 6/100\n",
      "Converting SMILES to graph: 7/100\n",
      "Converting SMILES to graph: 8/100\n",
      "Converting SMILES to graph: 9/100\n",
      "Converting SMILES to graph: 10/100\n",
      "Converting SMILES to graph: 11/100\n",
      "Converting SMILES to graph: 12/100\n",
      "Converting SMILES to graph: 13/100\n",
      "Converting SMILES to graph: 14/100\n",
      "Converting SMILES to graph: 15/100\n",
      "Converting SMILES to graph: 16/100\n",
      "Converting SMILES to graph: 17/100\n",
      "Converting SMILES to graph: 18/100\n",
      "Converting SMILES to graph: 19/100\n",
      "Converting SMILES to graph: 20/100\n",
      "Converting SMILES to graph: 21/100\n",
      "Converting SMILES to graph: 22/100\n",
      "Converting SMILES to graph: 23/100\n",
      "Converting SMILES to graph: 24/100\n",
      "Converting SMILES to graph: 25/100\n",
      "Converting SMILES to graph: 26/100\n",
      "Converting SMILES to graph: 27/100\n",
      "Converting SMILES to graph: 28/100\n",
      "Converting SMILES to graph: 29/100\n",
      "Converting SMILES to graph: 30/100\n",
      "Converting SMILES to graph: 31/100\n",
      "Converting SMILES to graph: 32/100\n",
      "Converting SMILES to graph: 33/100\n",
      "Converting SMILES to graph: 34/100\n",
      "Converting SMILES to graph: 35/100\n",
      "Converting SMILES to graph: 36/100\n",
      "Converting SMILES to graph: 37/100\n",
      "Converting SMILES to graph: 38/100\n",
      "Converting SMILES to graph: 39/100\n",
      "Converting SMILES to graph: 40/100\n",
      "Converting SMILES to graph: 41/100\n",
      "Converting SMILES to graph: 42/100\n",
      "Converting SMILES to graph: 43/100\n",
      "Converting SMILES to graph: 44/100\n",
      "Converting SMILES to graph: 45/100\n",
      "Converting SMILES to graph: 46/100\n",
      "Converting SMILES to graph: 47/100\n",
      "Converting SMILES to graph: 48/100\n",
      "Converting SMILES to graph: 49/100\n",
      "Converting SMILES to graph: 50/100\n",
      "Converting SMILES to graph: 51/100\n",
      "Converting SMILES to graph: 52/100\n",
      "Converting SMILES to graph: 53/100\n",
      "Converting SMILES to graph: 54/100\n",
      "Converting SMILES to graph: 55/100\n",
      "Converting SMILES to graph: 56/100\n",
      "Converting SMILES to graph: 57/100\n",
      "Converting SMILES to graph: 58/100\n",
      "Converting SMILES to graph: 59/100\n",
      "Converting SMILES to graph: 60/100\n",
      "Converting SMILES to graph: 61/100\n",
      "Converting SMILES to graph: 62/100\n",
      "Converting SMILES to graph: 63/100\n",
      "Converting SMILES to graph: 64/100\n",
      "Converting SMILES to graph: 65/100\n",
      "Converting SMILES to graph: 66/100\n",
      "Converting SMILES to graph: 67/100\n",
      "Converting SMILES to graph: 68/100\n",
      "Converting SMILES to graph: 69/100\n",
      "Converting SMILES to graph: 70/100\n",
      "Converting SMILES to graph: 71/100\n",
      "Converting SMILES to graph: 72/100\n",
      "Converting SMILES to graph: 73/100\n",
      "Converting SMILES to graph: 74/100\n",
      "Converting SMILES to graph: 75/100\n",
      "Converting SMILES to graph: 76/100\n",
      "Converting SMILES to graph: 77/100\n",
      "Converting SMILES to graph: 78/100\n",
      "Converting SMILES to graph: 79/100\n",
      "Converting SMILES to graph: 80/100\n",
      "Converting SMILES to graph: 81/100\n",
      "Converting SMILES to graph: 82/100\n",
      "Converting SMILES to graph: 83/100\n",
      "Converting SMILES to graph: 84/100\n",
      "Converting SMILES to graph: 85/100\n",
      "Converting SMILES to graph: 86/100\n",
      "Converting SMILES to graph: 87/100\n",
      "Converting SMILES to graph: 88/100\n",
      "Converting SMILES to graph: 89/100\n",
      "Converting SMILES to graph: 90/100\n",
      "Converting SMILES to graph: 91/100\n",
      "Converting SMILES to graph: 92/100\n",
      "Converting SMILES to graph: 93/100\n",
      "Converting SMILES to graph: 94/100\n",
      "Converting SMILES to graph: 95/100\n",
      "Converting SMILES to graph: 96/100\n",
      "Converting SMILES to graph: 97/100\n",
      "Converting SMILES to graph: 98/100\n",
      "Converting SMILES to graph: 99/100\n",
      "Converting SMILES to graph: 100/100\n",
      "Graph construction done. Saving to file.\n"
     ]
    }
   ],
   "source": [
    "DLS100_data = createTestData('New_fold','DLS100.csv','DLS100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c7abb4f2d94f7488b5e85282a013ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLS100 Test R2:  0.6392118967183169\n",
      "DLS100 Test RMSE: 1.0240424987311114\n",
      "DLS100 Test R2:  0.4428526469382511\n",
      "DLS100 Test RMSE: 1.2725560138867402\n",
      "DLS100 Test R2:  0.6300548759186226\n",
      "DLS100 Test RMSE: 1.0369564539926988\n",
      "DLS100 Test R2:  0.5125976712366211\n",
      "DLS100 Test RMSE: 1.1902430925591485\n",
      "DLS100 Test R2:  0.6107955384835786\n",
      "DLS100 Test RMSE: 1.063606023133415\n",
      "DLS100 Test R2:  0.5747910743160598\n",
      "DLS100 Test RMSE: 1.1117139761226449\n",
      "DLS100 Test R2:  0.4825179561162708\n",
      "DLS100 Test RMSE: 1.2264208669346681\n",
      "DLS100 Test R2:  0.6331663765364391\n",
      "DLS100 Test RMSE: 1.0325864812918775\n",
      "DLS100 Test R2:  0.5358126529418485\n",
      "DLS100 Test RMSE: 1.1615516260195895\n",
      "DLS100 Test R2:  0.6005907671592549\n",
      "DLS100 Test RMSE: 1.0774594051918736\n"
     ]
    }
   ],
   "source": [
    "# test novel dataset on whole trained model.\n",
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "folds = 10\n",
    "results = []\n",
    "best_rmse_arr = []\n",
    "bestrmsesum = 0\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "# fig = plt.figure()\n",
    "# for fold in tqdm(range(folds)):\n",
    "# val_losses = []\n",
    "# train_losses = []\n",
    "# mae_arr = []\n",
    "# patience = 30\n",
    "# trigger_times = 0\n",
    "# the_last_loss = 100\n",
    "for fold in tqdm(range(folds)):\n",
    "    DLS100_loader  = DataLoader(DLS100_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    model = GAT().to(device)\n",
    "    # model = define_model(trial).to(device)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=.0023467,\n",
    "    #                              weight_decay=.00095)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.007531829,\n",
    "#                                    weight_decay=0.000252036)\n",
    "#     model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "#                     edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "#                     dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    mae_arr = []\n",
    "    patience = 30\n",
    "    trigger_times = 0\n",
    "    the_last_loss = 100\n",
    "    \n",
    "    model_file_name = 'saved_models/wholetrainmodel_' +  str(fold) +  '.model'\n",
    "    checkpoint = torch.load(model_file_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "#     train_data = Molecule_data(root='data', dataset='train_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    test_loss,test_rmse, true, prediction = predicting(DLS100_loader, model)\n",
    "\n",
    "    best_ret = []\n",
    "    bestrmsesum = bestrmsesum + test_rmse\n",
    "    results.append(best_ret)\n",
    "    #best_rmse_arr.append(best_rmse)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    print('DLS100 Test R2: ', score)\n",
    "    print('DLS100 Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
